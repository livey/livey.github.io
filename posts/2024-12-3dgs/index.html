<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>3D Gaussian Splatting | Fuwei's Tech Notes</title>
<meta name=keywords content="3D Gaussian Splatting,3D Reconstruction,NeRF"><meta name=description content="Notes on 3D Gaussian Splatting."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2024-12-3dgs/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="Notes on 3D Gaussian Splatting."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2024-12-3dgs/"><meta property="og:title" content="3D Gaussian Splatting"><meta property="og:description" content="Notes on 3D Gaussian Splatting."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="3D Gaussian Splatting"><meta name=twitter:description content="Notes on 3D Gaussian Splatting."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2024-12-3dgs/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="3D Gaussian Splatting"><meta property="og:description" content="Notes on 3D Gaussian Splatting."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-28T00:00:00+00:00"><meta property="article:tag" content="3D Gaussian Splatting"><meta property="article:tag" content="3D Reconstruction"><meta property="article:tag" content="NeRF"><meta property="og:image" content="https://livey.github.io/posts/2024-12-3dgs/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2024-12-3dgs/%3Cimage%20path/url%3E"><meta name=twitter:title content="3D Gaussian Splatting"><meta name=twitter:description content="Notes on 3D Gaussian Splatting."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"3D Gaussian Splatting","item":"https://livey.github.io/posts/2024-12-3dgs/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"3D Gaussian Splatting","name":"3D Gaussian Splatting","description":"Notes on 3D Gaussian Splatting.","keywords":["3D Gaussian Splatting","3D Reconstruction","NeRF"],"articleBody":"I highly recommend reading the two papers: [5] and [3]. [5] provides a comprehensive description of the splatting process. [3] provides an efficient implementation of the splatting process.\nProblem Formulation Volume Rendering with Radiance Fields [1, 2] figure from https://www.cs.cornell.edu/courses/cs5670/2022sp/lectures/lec22_nerf_for_web.pdf. The volume density $\\sigma(\\mathbf{x})$ can be interpreted as the differential probability of a ray terminating (being absorbed or scattered) at an infinitesimal particle at location $\\mathbf{x}$.\nThe expected color $\\mathcal{C}(\\mathbf{r})$ of camera ray $\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}$ with near and far bounds $t_n$ and $t_f$ is, where the ray starts from the observer and extends to the far field:\n$$\\begin{equation} \\mathcal{C}(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\sigma (\\mathbf{r}(t)) c(\\mathbf{r}(t), \\mathbf{d}) \\, dt \\end{equation}$$where $T(t) = \\exp \\left( -\\int_{t_n}^{t} \\sigma (\\mathbf{r}(s)) \\, ds \\right)$, and it denotes the accumulated transmittance along the ray from $t_n$ to $t$. (see Appendix A) The color, $c(\\mathbf{r}(t), \\mathbf{d})$, not only depends on its location but also the viewing direction, $\\mathbf{d}$.\nFrom the rendering function, (1), we have two hidden variables, $\\sigma(\\mathbf{r}(t))$ and $c(\\mathbf{r}(t),\\mathbf{d})$, that are functions of the location and viewing direction.\nDiscretized form We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation’s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition $[t_n, t_f]$ into $N$ evenly-spaced bins and then draw one sample uniformly at random from within each bin:\n$$t_i \\sim \\mathcal{U} \\left[ t_n + \\frac{i - 1}{N} (t_f - t_n), t_n + \\frac{i}{N} (t_f - t_n) \\right]$$Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate $\\hat{C}(\\mathbf{r})$ with the quadrature rule discussed in the volume rendering review by Max [25]:\n$$\\begin{equation} \\hat{C}(\\mathbf{r}) = \\sum_{i=1}^N T_i \\left(1 - \\exp(-\\sigma_i \\delta_i) \\right) c_i, \\quad \\text{where} \\quad T_i = \\exp \\left( - \\sum_{j=1}^{i-1} \\sigma_j \\delta_j \\right) \\end{equation}$$where $\\delta_i = t_{i+1} - t_i$ is the distance between adjacent samples. This function for calculating $\\hat{C}(\\mathbf{r})$ from the set of $(c_i, \\sigma_i)$ values is trivially differentiable and reduces to traditional alpha compositing with alpha values $\\alpha_i = 1 - \\exp(-\\sigma_i \\delta_i)$.\nWhy $1-\\exp(-\\sigma_i\\delta_i)$? where From Appendix A, we know $T(t_{i+1}) = \\exp(-\\int^{t_i}\\sigma(t)dt-\\int_{t_i}^{t_{i+1}}\\sigma(t)dt)\\approx T(t_i)\\exp(-\\sigma_i\\delta_i)$. Considering the meaning of $T(t_i)$, the term $\\exp(-\\sigma_i\\delta_i)$ can be treated as the probability of not observed. So, $(1-\\exp(-\\delta_i\\sigma_i))$ is interpreted as the probability density of absorbing in the interval of $t_{i}$ and $t_{i+1}$.\nEq. (2) can be re-written as\n$C = \\sum_{i=1}^{N} T_i \\alpha_i c_i,$ $\\alpha_i = (1 - \\exp(-\\sigma_i \\delta_i)) \\quad \\text{and} \\quad T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j)$\nThe point-based method First, it is assumed that the emission coefficient is approximately constant in the support of each footprint function, hence $\\alpha_k(\\mathbf{r}) = \\alpha_k$, for all $\\mathbf{r}$ in the support area.\" [5]. A typical neural point-based approach (e.g., [Kopanas et al. 2022, 2021]) computes the color $C$ of a pixel by blending $\\mathcal{N}$ ordered points overlapping the pixel:\n$$\\begin{equation} C = \\sum_{i \\in \\mathcal{N}} c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) \\end{equation}$$where $c_i$ is the color of each point and $\\alpha_i$ is given by evaluating a 2D Gaussian with covariance $\\Sigma$ [Yifan et al. 2019] multiplied with a learned per-point opacity. In [3], the color parameterized by spherical harmonics (see Appendix B).\nConsidering the weight and per-point opacity of Eq. (3), we get\n$$\\begin{equation} C = \\sum_{i \\in \\mathcal{N}} c_i \\alpha_i w_i \\prod_{j=1}^{i-1} (1 - \\alpha_jw_j) \\end{equation}$$where $w_i = \\mathcal{G}_i(r_i;\\mu_i, \\Sigma_i)$ and\n$$\\mathcal{G}(\\mathbf{x} - \\mu) = \\frac{1}{\\sqrt{2 \\pi \\Sigma(\\mathbf{x})}} \\exp \\left( -\\frac{1}{2} (\\mathbf{x} - \\mu)^T \\Sigma^{-1} (\\mathbf{x} - \\mu) \\right)$$Note: there is mis-alignment between the paper and the implementation of the Gaussian form, in the paper, it discards the normalization term as:\n$$G(x) = e^{-\\frac{1}{2}(x)^T \\Sigma^{-1} (x)}$$There are some detailed comparisons between the two in github.\nSplatting and sampling (See Appendix D)\nWhen project 3D Gaussian onto 2D, the covariance matrix is approximately as $\\Sigma^\\prime = JW\\Sigma W^\\top J^\\top$, where $W$ is the viewing transformation (world to camera rotation) and $J$ is the Jacobian of affine projective transformation (camera to image affine transformation).\nSince $\\Sigma$ is symmetric and positive definite, it can be represented as\n$$\\boldsymbol{\\Sigma}=\\mathbf{R}\\mathbf{S}\\mathbf{S}^\\top\\mathbf{R}^\\top$$where $\\mathbf{R}$ is a rotation matrix and $\\mathbf{S}$ is a positive diagonal matrix, i.e., $\\mathbf{S}=\\text{diag}(s_1, s_2, s_3)$. The rotation matrix $\\mathbf{R}$ can be represented by a Quaternion.\nOptimization The loss function is $\\mathcal{L}_1$ combined with a D-SSIM term:\n$$\\mathcal{L} = (1 - \\lambda) \\mathcal{L}_1 + \\lambda \\mathcal{L}_{\\text{D-SSIM}}$$The parameters include the position $\\boldsymbol{\\mu}_i$, covariance matrix $\\boldsymbol{\\Sigma}$, alpha-blending $\\alpha$, color, and the spherical harmonics coefficients.\nMethod figure from [3]. Adaptive Control of Gaussians:\nClone 3D gaussians: Under-reconstruction (missing geometric features)\nSplit 3D gaussians: Over-reconstruction (covering large area)\nRemove 3D gaussians: Opacity is lower than threshold\n3D Smoothing Given the maximal sampling rate $\\hat{\\nu}_k$ for a primitive, we aim to constrain the maximal frequency of the 3D representation. This is achieved by applying a Gaussian low-pass filter $G_{\\text{low}}$ to each 3D Gaussian primitive $G_k$ before projecting it onto screen space [6]\n$$\\begin{aligned} G_k(\\mathbf{x})_{\\text{reg}} = (G_k \\otimes G_{\\text{low}})(\\mathbf{x}) \\end{aligned}$$This operation is efficient as convolving two Gaussians with covariance matrices $\\Sigma_1$ and $\\Sigma_2$ results in another Gaussian with variance $\\Sigma_1 + \\Sigma_2$. Hence,\n$$\\begin{aligned} G_k(\\mathbf{x})_{\\text{reg}} = \\sqrt{\\frac{|\\Sigma_k|}{|\\Sigma_k + \\frac{s}{\\hat{\\nu}_k^2} \\cdot \\mathbf{I}|}} e^{-\\frac{1}{2} (\\mathbf{x} - \\mathbf{p}_k)^T \\left( \\Sigma_k + \\frac{s}{\\hat{\\nu}_k^2} \\cdot \\mathbf{I} \\right)^{-1} (\\mathbf{x} - \\mathbf{p}_k)} \\end{aligned}$$Here, $s$ is a scalar hyperparameter to control the size of the filter. Note that the scale $\\frac{s}{\\hat{\\nu}_k^2}$ of the 3D filters for each primitive are different as they depend on the training views in which they are visible. By employing 3D Gaussian smoothing, we ensure that the highest frequency component of any Gaussian does not exceed half of its maximal sampling rate for at least one camera. Note that $G_{\\text{low}}$ becomes an intrinsic part of the 3D representation, remaining constant post-training.\n4D Gaussian Splatting The 4D Gaussian splatting involves adding another dimension time, $t$, to represent the objects’ dynamics [7]. The rendering formula is\n$$\\mathcal{I}(u,v,t) = \\sum_{i=1}^{N} p_i(u,v,t) \\alpha_i c_i(d) \\prod_{j=1}^{i-1} (1 - p_j(u,v,t) \\alpha_j)$$Note that $p_i(u,v,t)$ can be further factorized as a product of a conditional probability $p_i(u,v|t)$ and a marginal probability $p_i(t)$ at time $t$, yielding:\n$$\\mathcal{I}(u,v,t) = \\sum_{i=1}^{N} p_i(t) p_i(u,v|t) \\alpha_i c_i(d) \\prod_{j=1}^{i-1} (1 - p_j(t) p_j(u,v|t) \\alpha_j)$$We parameterize its covariance matrix $\\Sigma$ as the configuration of a 4D ellipsoid for easing model optimization:\n$$\\Sigma = R S S^{T} R^{T}$$where $S$ is a scaling matrix and $R$ is a 4D rotation matrix. Since $S$ is diagonal, it can be completely inscribed by its diagonal elements as $S = \\operatorname{diag}(s_x, s_y, s_z, s_t)$. On the other hand, a rotation in 4D Euclidean space can be decomposed into a pair of isotropic rotations, each of which can be represented by a quaternion. wiki\nSpecifically, given $q_l = (a, b, c, d)$ and $q_r = (p, q, r, s)$ denoting the left and right isotropic rotations respectively, $R$ can be constructed by:\n$$R = L(q_l)R(q_r) = \\begin{pmatrix} a \u0026 -b \u0026 -c \u0026 -d \\\\ b \u0026 a \u0026 -d \u0026 c \\\\ c \u0026 d \u0026 a \u0026 -b \\\\ d \u0026 -c \u0026 b \u0026 a \\end{pmatrix} \\begin{pmatrix} p \u0026 -q \u0026 -r \u0026 -s \\\\ q \u0026 p \u0026 s \u0026 -r \\\\ r \u0026 -s \u0026 p \u0026 q \\\\ s \u0026 r \u0026 -q \u0026 p \\end{pmatrix}$$The mean of a 4D Gaussian can be represented by four scalars as $\\mu = (\\mu_x, \\mu_y, \\mu_z, \\mu_t)$. Thus far we arrive at a complete representation of the general 4D Gaussian.\nSubsequently, the conditional 3D Gaussian can be derived from the properties of the multivariate Gaussian with:\n$$\\mu_{xyz|t} = \\mu_{1:3} + \\Sigma_{1:3,4}\\Sigma_{4,4}^{-1}(t - \\mu_t),$$$$\\Sigma_{xyz|t} = \\Sigma_{1:3,1:3} - \\Sigma_{1:3,4}\\Sigma_{4,4}^{-1}\\Sigma_{4,1:3}$$Moreover, the marginal $p_i(t)$ is also a Gaussian in one-dimension:\n$$p(t) = \\mathcal{N}(t; \\mu_4, \\Sigma_{4,4})$$4D Spherindrical Harmonics Inspired by studies on head-related transfer function, we propose to represent $c_i(d, t)$ as the combination of a series of 4D spherindrical harmonics (4DSH) which are constructed by merging SH with different 1D-basis functions. For computational convenience, we use the Fourier series as the adopted 1D-basis functions. Consequently, 4DSH can be expressed as:\n$$Z_{nl}^{m}(t, \\theta, \\phi) = \\cos\\left(\\frac{2\\pi n}{T} t\\right) Y_{l}^{m}(\\theta, \\phi)$$where $Y_{l}^{m}$ is the 3D spherical harmonics. The index $l \\geq 0$ denotes its degree, and $m$ is the order satisfying $-l \\leq m \\leq l$. The index $n$ is the order of the Fourier series. The 4D spherindrical harmonics form an orthonormal basis in the spherindrical coordinate system.\nAppendix Appendix A: Understanding the Accumulated Transmittance Volume Density $\\sigma(\\mathbf{x})$:\nThe volume density $\\sigma(\\mathbf{x})$ at a point $\\mathbf{x}$ represents the differential probability per unit length that a ray will be absorbed or scattered at that point. Survival Probability $T(t)$: $T(t)$ represents the probability that a ray traveling along a path $\\mathbf{r}(t)$ from $t_n$ to $t_n$ has not been absorbed or scattered.\nIt can be thought of as the transmittance or the cumulative probability of the ray surviving up to point $t_n$.\nDifferential Equation for Transmittance To derive the transmittance function, we consider how the survival probability changes over an infinitesimal segment of the ray path.\nInfinitesimal Segment:\nConsider a small segment of the ray’s path from $t$ to $t + dt$.\nThe probability that the ray is not absorbed or scattered in this small segment is $1 - \\sigma(\\mathbf{r}(t)) dt$.\nSurvival Probability Over the Small Segment:\nIf the ray has survived up to $t$ with probability $T(t)$, the probability that it also survives the next small segment $dt$ is: $$T(t + dt) = T(t) \\cdot (1 - \\sigma(\\mathbf{r}(t)) dt)$$ Taking the Limit:\nAs $dt$ approaches zero, this becomes:\n$$\\frac{T(t + dt) - T(t)}{dt} \\approx -\\sigma(\\mathbf{r}(t)) T(t)$$$$\\frac{dT(t)}{dt} = -\\sigma(\\mathbf{r}(t)) T(t)$$ Solving the Differential Equation Separation of Variables:\nRewrite the differential equation to separate the variables $T$ and $t$: $$\\frac{dT(t)}{T(t)} = -\\sigma(\\mathbf{r}(t)) dt$$ Integrating Both Sides:\nIntegrate both sides with respect to their respective variables: $$ \\int \\frac{dT(t)}{T(t)} = - \\int \\sigma(\\mathbf{r}(t)) dt$$$$ \\ln|T(t)| = - \\int_{t_n}^t \\sigma(\\mathbf{r}(s)) ds + C$$where $C$ is the integration constant.\nExponentiating Both Sides:\nSolve for $T(t)$ by exponentiating both sides: $$T(t) = e^{-\\int_{t_n}^t \\sigma(\\mathbf{r}(s)) ds + C}$$$$ T(t) = e^{-\\int_{t_n}^t \\sigma(\\mathbf{r}(s)) ds}$$ In neural radiance fields (NeRF) and other volumetric rendering techniques, spherical harmonics are used to efficiently represent the directional dependency of emitted radiance (color) at each point in the volume. Given that color is a three-dimensional vector (representing the RGB color channels), spherical harmonics can be employed to represent each of these color channels separately for each direction.\nAppendix B: Spherical Harmonics in Neural Radiance Fields Radiance Representation:\nIn a neural radiance field, the color $c(\\mathbf{x}, \\mathbf{d})$ emitted from a point $\\mathbf{x}$ in the direction $\\mathbf{d}$ can vary depending on the direction $\\mathbf{d}$.\nInstead of storing the color for every possible direction, which is computationally infeasible, spherical harmonics allow for a compact representation.\nSpherical Harmonics Expansion:\nThe color at each point can be expanded into spherical harmonics: $$c(\\mathbf{x}, \\mathbf{d}) = \\sum_{\\ell=0}^{L} \\sum_{m=-\\ell}^{\\ell} c_{\\ell m}(\\mathbf{x}) Y_{\\ell m}(\\mathbf{d})$$ RGB Color Channels:\nSince color is a three-dimensional vector (RGB), each color channel can be represented using its own set of spherical harmonics coefficients: $$c_r(\\mathbf{x}, \\mathbf{d}) = \\sum_{\\ell=0}^{L} \\sum_{m=-\\ell}^{\\ell} c_{\\ell m}^r(\\mathbf{x}) Y_{\\ell m}(\\mathbf{d})$$$$ c_g(\\mathbf{x}, \\mathbf{d}) = \\sum_{\\ell=0}^{L} \\sum_{m=-\\ell}^{\\ell} c_{\\ell m}^g(\\mathbf{x}) Y_{\\ell m}(\\mathbf{d})$$$$ c_b(\\mathbf{x}, \\mathbf{d}) = \\sum_{\\ell=0}^{L} \\sum_{m=-\\ell}^{\\ell} c_{\\ell m}^b(\\mathbf{x}) Y_{\\ell m}(\\mathbf{d})$$ Here, $c_{\\ell m}^r(\\mathbf{x})$, $c_{\\ell m}^g(\\mathbf{x})$, and $c_{\\ell m}^b(\\mathbf{x})$ are the spherical harmonics coefficients for the red, green, and blue color channels, respectively.\nWrite it in a more compact form\n$$\\mathbf{c}(\\mathbf{x}, \\mathbf{d}) = \\sum_{\\ell=0}^{L} \\sum_{m=-\\ell}^{\\ell} \\mathbf{c}_{\\ell m}(\\mathbf{x}) Y_{\\ell m}(\\mathbf{d})$$where $\\mathbf{c}_{lm}=[c^r, c^g, c^b]^\\top$.\nAdvantages:\nCompactness: Using a small number of spherical harmonics coefficients can effectively capture the variation in color with direction, reducing the memory and computational requirements.\nEfficiency: Once the coefficients are computed, evaluating the color for any direction $\\mathbf{d}$ is efficient, involving only a sum of products.\nSmoothness: Spherical harmonics provide a smooth interpolation of the radiance function, which is particularly useful for rendering soft lighting and shadows.\nNeural Network Training:\nDuring the training of a neural radiance field, the network learns to predict the spherical harmonics coefficients $c_{\\ell m}(\\mathbf{x})$ for each point $\\mathbf{x}$.\nThe input to the network typically includes the spatial location $\\mathbf{x}$ and sometimes additional features like view direction $\\mathbf{d}$ to capture more complex lighting effects.\nAppendix C: From Surface to Point Splatting Surface splatting [4] Texture functions on point-based objects figure from [4]. The points $\\mathbf{Q}$ and $\\mathbf{P_k}$ have local coordinates $\\mathbf{u}$ and $\\mathbf{u_k}$, respectively. We define the continuous surface function $f_c(\\mathbf{u})$ as the weighted sum:\n$$f_c(\\mathbf{u}) = \\sum_{k \\in \\mathbb{N}} w_k r_k (\\mathbf{u} - \\mathbf{u_k})$$where the basis functions $r_k$ that have local support or that are appropriately truncated.\nRendering figure from [4]. Warp $f_c(\\mathbf{u})$ to screen space, yielding the warped, continuous screen space signal $g_c(\\mathbf{x})$: $$g_c(\\mathbf{x}) = (f_c \\circ \\mathbf{m}^{-1})(\\mathbf{x}) = f_c(\\mathbf{m}^{-1}(\\mathbf{x}))$$where $\\circ$ denotes function concatenation.\nBand-limit the screen space signal using a prefilter $h$, resulting in the continuous output function $g'_c(\\mathbf{x})$: $$ g'_c(\\mathbf{x}) = g_c(\\mathbf{x}) \\otimes h(\\mathbf{x}) = \\int_{\\mathbb{R}^2} g_c(\\boldsymbol{\\xi}) h(\\mathbf{x} - \\boldsymbol{\\xi}) d\\boldsymbol{\\xi}$$where $\\otimes$ denotes convolution.\nSample the continuous output function by multiplying it with an $\\textit{impulse train}$ $i$ to produce the discrete output $g(\\mathbf{x})$: $$ g(\\mathbf{x}) = g'_c(\\mathbf{x}) i(\\mathbf{x})$$An explicit expression for the warped continuous output function can be derived by expanding the above relations in reverse order:\n$$\\begin{equation} g'_c(\\mathbf{x}) = \\int_{\\mathbb{R}^2} h(\\mathbf{x} - \\boldsymbol{\\xi}) \\sum_{k \\in \\mathbb{N}} w_k r_k(\\mathbf{m}^{-1}(\\boldsymbol{\\xi}) - \\mathbf{u_k}) d\\boldsymbol{\\xi} = \\sum_{k \\in \\mathbb{N}} w_k \\rho_k(\\mathbf{x}) \\end{equation}$$where\n$$\\rho_k(\\mathbf{x}) = \\int_{\\mathbb{R}^2} h(\\mathbf{x} - \\boldsymbol{\\xi}) r_k(\\mathbf{m}^{-1}(\\boldsymbol{\\xi}) - \\mathbf{u_k}) d\\boldsymbol{\\xi}$$We call a warped and filtered basis function $\\rho_k(\\mathbf{x})$ a $\\textit{resampling kernel}$$, which is expressed here as a screen space integral. Equation (5) states that we can first warp and filter each basis function $r_k$ individually to construct these resampling kernels $\\rho_k$, and then sum up the contributions of these kernels in screen space. We call this approach $\\textit{surface splatting}$.\nfigure from [4]. In order to simplify the integral for $\\rho_k(\\mathbf{x})$ in (5), we replace a general mapping $\\mathbf{m}(\\mathbf{u})$ by its local affine approximation $\\mathbf{m_{u_k}}$ at a point $\\mathbf{u_k}$,\n$$\\mathbf{m_{u_k}}(\\mathbf{u}) = \\mathbf{x_k} + \\mathbf{J_{u_k}} \\cdot (\\mathbf{u} - \\mathbf{u_k})$$where $\\mathbf{x_k} = \\mathbf{m}(\\mathbf{u_k})$ and the Jacobian $\\mathbf{J_{u_k}} = \\frac{\\partial \\mathbf{m}}{\\partial \\mathbf{u}} (\\mathbf{u_k})$.\nSince the basis functions $r_k$ have local support, $\\mathbf{m_{u_k}}$ is used only in a small neighborhood around $\\mathbf{u_k}$ in (5). Moreover, the approximation is most accurate in the neighborhood of $\\mathbf{u_k}$ and so it does not cause visual artifacts. We use it to rearrange Equation (5), and after a few steps we find:\n$$\\rho_k(\\mathbf{x}) = \\int_{\\mathbb{R}^2} h(\\mathbf{x} - \\mathbf{m_{u_k}}(\\mathbf{u_k}) - \\boldsymbol{\\xi}) r'_k(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi} = (r'_k \\otimes h)(\\mathbf{x} - \\mathbf{m_{u_k}}(\\mathbf{u_k}))$$where $r'_k(\\mathbf{x}) = r_k(\\mathbf{J_{u_k}^{-1}} \\mathbf{x})$ denotes a warped basis function. Thus, although the texture function is defined on an irregular grid, Equation (5) states that the resampling kernel in screen space, $\\rho_k(\\mathbf{x})$, can be written as a $\\textit{convolution}$ of a warped basis function $r'_k$ and the low-pass filter kernel $h$. This is essential for the derivation of screen space EWA in the next section. Note that from now on we are omitting the subscript $u_k$ for $\\mathbf{m}$ and $\\mathbf{J}$.\nScreen Space EWA We choose elliptical Gaussians both for the basis functions and the low-pass filter, since they are closed under affine mappings and convolution. An elliptical Gaussian $\\mathcal{G}_{\\mathbf{V}}(\\mathbf{x})$ with variance matrix $\\mathbf{V}$ is defined as, for $\\mathbf{x}\\in\\mathbb{R}^2$,\n$$\\mathcal{G}_{\\mathbf{V}}(\\mathbf{x}) = \\frac{1}{2\\pi \\sqrt{|\\mathbf{V}|}} e^{-\\frac{1}{2} \\mathbf{x}^T \\mathbf{V}^{-1} \\mathbf{x}}$$where $|\\mathbf{V}|$ is the determinant of $\\mathbf{V}$. We denote the variance matrices of the basis functions $r_k$ and the low-pass filter $h$ with $\\mathbf{V}^r_k$ and $\\mathbf{V}^h$, respectively. The warped basis function and the low-pass filter are:\n$$r'_k(\\mathbf{x}) = r(\\mathbf{J}^{-1}\\mathbf{x}) = \\mathcal{G}_{\\mathbf{V}^r_k}(\\mathbf{J}^{-1}\\mathbf{x}) = \\frac{1}{|\\mathbf{J}|^{-1}} \\mathcal{G}_{\\mathbf{J}\\mathbf{V}^r_k \\mathbf{J}^T}(\\mathbf{x})\\\\ h(\\mathbf{x}) = \\mathcal{G}_{\\mathbf{V}^h}(\\mathbf{x}).$$The resampling kernel $\\rho_k$ of (5) can be written as a single Gaussian with a variance matrix that combines the warped basis function and the low-pass filter. Typically $\\mathbf{V}^h = \\mathbf{I}$, yielding:\n$$ \\begin{aligned} \\rho_k(\\mathbf{x}) \u0026= (r'_k \\otimes h)(\\mathbf{x} - \\mathbf{m}(\\mathbf{u_k})) \\\\ \u0026= \\frac{1}{|\\mathbf{J}|^{-1}} (\\mathcal{G}_{\\mathbf{J}\\mathbf{V}^r_k \\mathbf{J}^T} \\otimes \\mathcal{G}_{\\mathbf{I}})(\\mathbf{x} - \\mathbf{m}(\\mathbf{u_k})) \\\\ \u0026= \\frac{1}{|\\mathbf{J}|^{-1}} \\mathcal{G}_{\\mathbf{J}\\mathbf{V}^r_k \\mathbf{J}^T + \\mathbf{I}}(\\mathbf{x} - \\mathbf{m}(\\mathbf{u_k})) \\end{aligned} $$We will show how to determine $\\mathbf{J}^{-1}$ in Section 4, and how to compute $\\mathbf{V}^r_k$ in Section 5. Substituting the Gaussian resampling kernel (6) into (5) we get the continuous output function is the weighted sum:\n$$g'_c(\\mathbf{x}) = \\sum_{k \\in \\mathbb{N}} w_k \\frac{1}{|\\mathbf{J}|^{-1}} \\mathcal{G}_{\\mathbf{J}\\mathbf{V}^r_k \\mathbf{J}^T + \\mathbf{I}} (\\mathbf{x} - \\mathbf{m}(\\mathbf{u_k}))$$We call this novel formulation $\\textit{screen space EWA}$.\nObject with per point color Many of today’s imaging systems, such as laser range scanners or passive vision systems, acquire range and color information. In such cases, the acquisition process provides a color sample $c_k$ with each point. We have to compute a continuous approximation $f_c(\\mathbf{u})$ of the unknown original texture function from the irregular set of samples $c_k$.\nA computationally reasonable approximation is to normalize the basis functions $r_k$ to form a partition of unity, i.e., to sum up to one everywhere. Then we use the samples as coefficients, hence $w_k = c_k$, and build a weighted sum of the samples $c_k$:\n$$f_c(\\mathbf{u}) = \\sum_{k \\in \\mathbb{N}} c_k \\hat{r}_k (\\mathbf{u} - \\mathbf{u}_k) = \\sum_{k \\in \\mathbb{N}} c_k \\frac{r_k (\\mathbf{u} - \\mathbf{u}_k)}{\\sum_{j \\in \\mathbb{N}} r_j (\\mathbf{u} - \\mathbf{u}_j)}$$Appendix D: Perspective Transformation These statements may be unclear to readers. From reference [5], we know the author is discussing the transformation from ray space to image space. Converting the camera space, $[t_0, t_1, t_2]^\\top$, to the ray space, $[x_0, x_1, x_2]^\\top$ follows\n$$\\begin{aligned} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\phi(\\mathbf{t}) = \\begin{bmatrix} t_0 / t_2 \\\\ t_1 / t_2 \\\\ \\| (t_0, t_1, t_2) \\|^T \\end{bmatrix} \\end{aligned}$$Then, from ray space to camera space is simply integrating out $x_2$.\nActually, we can go directly from camera space to image space by using the perspective transformation\n$$t_2 \\begin{bmatrix} x_0\\\\ x_1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} f_x\u00260\u0026c_x\\\\ 0\u0026f_y\u0026c_y\\\\ 0\u00260\u00261 \\end{bmatrix} \\begin{bmatrix} t_0\\\\ t_1\\\\ t_2 \\end{bmatrix}$$Then, we do linear approximation.\nReferences [1] J. T. Kajiya and B. P. V. Herren, “RAY TRACING VOLUME DENSITIES”.\n[2] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis”.\n[3] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” arXiv, Aug. 08, 2023. doi: 10.48550/arXiv.2308.04079.\n[4] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface splatting,” in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM, Aug. 2001, pp. 371–378. doi: 10.1145/383259.383300.\n[5] M. Zwicker, H. Pfister, J. van Baar and M. Gross, “EWA splatting,” in IEEE Transactions on Visualization and Computer Graphics, vol. 8, no. 3, pp. 223-238, July-Sept. 2002, doi: 10.1109/TVCG.2002.1021576.\n[6] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, “Mip-Splatting: Alias-free 3D Gaussian Splatting”.\n[7] Z. Yang, H. Yang, Z. Pan, and L. Zhang, “Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting,” Feb. 22, 2024, arXiv: arXiv:2310.10642. Accessed: Jul. 11, 2024. [Online]. Available: http://arxiv.org/abs/2310.10642\n","wordCount":"3131","inLanguage":"en","image":"https://livey.github.io/posts/2024-12-3dgs/%3Cimage%20path/url%3E","datePublished":"2024-12-28T00:00:00Z","dateModified":"2024-12-28T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2024-12-3dgs/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">3D Gaussian Splatting</h1><div class=post-description>Notes on 3D Gaussian Splatting.</div><div class=post-meta><span title='2024-12-28 00:00:00 +0000 UTC'>December 28, 2024</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3131 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem-formulation aria-label="Problem Formulation">Problem Formulation</a><ul><li><a href=#volume-rendering-with-radiance-fields-1-2 aria-label="Volume Rendering with Radiance Fields [1, 2]">Volume Rendering with Radiance Fields [1, 2]</a><ul><li><a href=#discretized-form aria-label="Discretized form">Discretized form</a></li><li><a href=#why-1-exp-sigma_idelta_i aria-label="Why $1-\exp(-\sigma_i\delta_i)$?">Why $1-\exp(-\sigma_i\delta_i)$?</a></li><li><a href=#the-point-based-method aria-label="The point-based method">The point-based method</a></li></ul></li><li><a href=#splatting-and-sampling aria-label="Splatting and sampling">Splatting and sampling</a></li><li><a href=#optimization aria-label=Optimization>Optimization</a></li><li><a href=#method aria-label=Method>Method</a></li></ul></li><li><a href=#3d-smoothing aria-label="3D Smoothing">3D Smoothing</a></li><li><a href=#4d-gaussian-splatting aria-label="4D Gaussian Splatting">4D Gaussian Splatting</a><ul><li><a href=#4d-spherindrical-harmonics aria-label="4D Spherindrical Harmonics">4D Spherindrical Harmonics</a></li></ul></li><li><a href=#appendix aria-label=Appendix>Appendix</a><ul><li><a href=#appendix-a-understanding-the-accumulated-transmittance aria-label="Appendix A: Understanding the Accumulated Transmittance">Appendix A: Understanding the Accumulated Transmittance</a><ul><li><a href=#differential-equation-for-transmittance aria-label="Differential Equation for Transmittance">Differential Equation for Transmittance</a></li><li><a href=#solving-the-differential-equation aria-label="Solving the Differential Equation">Solving the Differential Equation</a></li></ul></li><li><a href=#appendix-b-spherical-harmonics-in-neural-radiance-fields aria-label="Appendix B: Spherical Harmonics in Neural Radiance Fields">Appendix B: Spherical Harmonics in Neural Radiance Fields</a></li><li><a href=#appendix-c-from-surface-to-point-splatting aria-label="Appendix C: From Surface to Point Splatting">Appendix C: From Surface to Point Splatting</a><ul><li><a href=#surface-splatting-4 aria-label="Surface splatting [4]">Surface splatting [4]</a><ul><li><a href=#texture-functions-on-point-based-objects aria-label="Texture functions on point-based objects">Texture functions on point-based objects</a></li><li><a href=#rendering aria-label=Rendering>Rendering</a></li><li><a href=#screen-space-ewa aria-label="Screen Space EWA">Screen Space EWA</a></li><li><a href=#object-with-per-point-color aria-label="Object with per point color">Object with per point color</a></li></ul></li></ul></li><li><a href=#appendix-d-perspective-transformation aria-label="Appendix D: Perspective Transformation">Appendix D: Perspective Transformation</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>I highly recommend reading the two papers: [5] and [3]. [5] provides a comprehensive description of the splatting process. [3] provides an efficient implementation of the splatting process.</p><h1 id=problem-formulation>Problem Formulation<a hidden class=anchor aria-hidden=true href=#problem-formulation>#</a></h1><h2 id=volume-rendering-with-radiance-fields-1-2>Volume Rendering with Radiance Fields [1, 2]<a hidden class=anchor aria-hidden=true href=#volume-rendering-with-radiance-fields-1-2>#</a></h2><figure style=text-align:center><img src=./resources/volume-rendering.png alt="volumne rendering" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>figure from https://www.cs.cornell.edu/courses/cs5670/2022sp/lectures/lec22_nerf_for_web.pdf.</figcaption></figure><p>The volume density $\sigma(\mathbf{x})$ can be interpreted as the differential probability of a ray terminating (being absorbed or scattered) at an infinitesimal particle at location $\mathbf{x}$.</p><p>The expected color $\mathcal{C}(\mathbf{r})$ of camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ with near and far bounds $t_n$ and $t_f$ is, where the ray starts from the observer and extends to the far field:</p>$$\begin{equation}
\mathcal{C}(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma (\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) \, dt
\end{equation}$$<p>where $T(t) = \exp \left( -\int_{t_n}^{t} \sigma (\mathbf{r}(s)) \, ds \right)$, and it denotes the accumulated transmittance along the ray from $t_n$ to $t$. (see Appendix A) The color, $c(\mathbf{r}(t), \mathbf{d})$, not only depends on its location but also the viewing direction, $\mathbf{d}$.</p><p>From the rendering function, (1), we have two hidden variables, $\sigma(\mathbf{r}(t))$ and $c(\mathbf{r}(t),\mathbf{d})$, that are functions of the location and viewing direction.</p><h3 id=discretized-form>Discretized form<a hidden class=anchor aria-hidden=true href=#discretized-form>#</a></h3><p>We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation&rsquo;s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition $[t_n, t_f]$ into $N$ evenly-spaced bins and then draw one sample uniformly at random from within each bin:</p>$$t_i \sim \mathcal{U} \left[ t_n + \frac{i - 1}{N} (t_f - t_n), t_n + \frac{i}{N} (t_f - t_n) \right]$$<p>Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate $\hat{C}(\mathbf{r})$ with the quadrature rule discussed in the volume rendering review by Max [25]:</p>$$\begin{equation}
\hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i) \right) c_i, \quad \text{where} \quad T_i = \exp \left( - \sum_{j=1}^{i-1} \sigma_j \delta_j \right)
\end{equation}$$<p>where $\delta_i = t_{i+1} - t_i$ is the distance between adjacent samples. This function for calculating $\hat{C}(\mathbf{r})$ from the set of $(c_i, \sigma_i)$ values is trivially differentiable and reduces to traditional alpha compositing with alpha values $\alpha_i = 1 - \exp(-\sigma_i \delta_i)$.</p><h3 id=why-1-exp-sigma_idelta_i>Why $1-\exp(-\sigma_i\delta_i)$?<a hidden class=anchor aria-hidden=true href=#why-1-exp-sigma_idelta_i>#</a></h3><p>where From Appendix A, we know $T(t_{i+1}) = \exp(-\int^{t_i}\sigma(t)dt-\int_{t_i}^{t_{i+1}}\sigma(t)dt)\approx T(t_i)\exp(-\sigma_i\delta_i)$. Considering the meaning of $T(t_i)$, the term $\exp(-\sigma_i\delta_i)$ can be treated as the probability of not observed. So, $(1-\exp(-\delta_i\sigma_i))$ is interpreted as the probability density of absorbing in the interval of $t_{i}$ and $t_{i+1}$.</p><p>Eq. (2) can be re-written as</p><p>$C = \sum_{i=1}^{N} T_i \alpha_i c_i,$ $\alpha_i = (1 - \exp(-\sigma_i \delta_i)) \quad \text{and} \quad T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)$</p><h3 id=the-point-based-method>The point-based method<a hidden class=anchor aria-hidden=true href=#the-point-based-method>#</a></h3><p>First, it is assumed that the emission coefficient is approximately constant in the support of each footprint function, hence $\alpha_k(\mathbf{r}) = \alpha_k$, for all $\mathbf{r}$ in the support area." [5]. A typical neural point-based approach (e.g., [Kopanas et al. 2022, 2021]) computes the color $C$ of a pixel by blending $\mathcal{N}$ ordered points overlapping the pixel:</p>$$\begin{equation}
C = \sum_{i \in \mathcal{N}} c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)
\end{equation}$$<p>where $c_i$ is the color of each point and $\alpha_i$ is given by evaluating a 2D Gaussian with covariance $\Sigma$ [Yifan et al. 2019] multiplied with a learned per-point opacity. In [3], the color parameterized by spherical harmonics (see Appendix B).</p><p>Considering the weight and per-point opacity of Eq. (3), we get</p>$$\begin{equation}
C = \sum_{i \in \mathcal{N}} c_i \alpha_i w_i \prod_{j=1}^{i-1} (1 - \alpha_jw_j)
\end{equation}$$<p>where $w_i = \mathcal{G}_i(r_i;\mu_i, \Sigma_i)$ and</p>$$\mathcal{G}(\mathbf{x} - \mu) = \frac{1}{\sqrt{2 \pi \Sigma(\mathbf{x})}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right)$$<p>Note: there is mis-alignment between the paper and the implementation of the Gaussian form, in the paper, it discards the normalization term as:</p>$$G(x) = e^{-\frac{1}{2}(x)^T \Sigma^{-1} (x)}$$<p>There are some detailed comparisons between the two in <a href=https://github.com/graphdeco-inria/gaussian-splatting/issues/294>github</a>.</p><h2 id=splatting-and-sampling>Splatting and sampling<a hidden class=anchor aria-hidden=true href=#splatting-and-sampling>#</a></h2><p>(See Appendix D)</p><p>When project 3D Gaussian onto 2D, the covariance matrix is approximately as $\Sigma^\prime = JW\Sigma W^\top J^\top$, where $W$ is the viewing transformation (world to camera rotation) and $J$ is the Jacobian of affine projective transformation (camera to image affine transformation).</p><p>Since $\Sigma$ is symmetric and positive definite, it can be represented as</p>$$\boldsymbol{\Sigma}=\mathbf{R}\mathbf{S}\mathbf{S}^\top\mathbf{R}^\top$$<p>where $\mathbf{R}$ is a rotation matrix and $\mathbf{S}$ is a positive diagonal matrix, i.e., $\mathbf{S}=\text{diag}(s_1, s_2, s_3)$. The rotation matrix $\mathbf{R}$ can be represented by a Quaternion.</p><h2 id=optimization>Optimization<a hidden class=anchor aria-hidden=true href=#optimization>#</a></h2><p>The loss function is $\mathcal{L}_1$ combined with a D-SSIM term:</p>$$\mathcal{L} = (1 - \lambda) \mathcal{L}_1 + \lambda \mathcal{L}_{\text{D-SSIM}}$$<p>The parameters include the position $\boldsymbol{\mu}_i$, covariance matrix $\boldsymbol{\Sigma}$, alpha-blending $\alpha$, color, and the spherical harmonics coefficients.</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><figure style=text-align:center><img src=./resources/3dgs-system-diagram.png alt="3DGS system diagram" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>figure from [3].</figcaption></figure><p>Adaptive Control of Gaussians:</p><ol><li><p>Clone 3D gaussians: Under-reconstruction (missing geometric features)</p></li><li><p>Split 3D gaussians: Over-reconstruction (covering large area)</p></li><li><p>Remove 3D gaussians: Opacity is lower than threshold</p></li></ol><h1 id=3d-smoothing>3D Smoothing<a hidden class=anchor aria-hidden=true href=#3d-smoothing>#</a></h1><p>Given the maximal sampling rate $\hat{\nu}_k$ for a primitive, we aim to constrain the maximal frequency of the 3D representation. This is achieved by applying a Gaussian low-pass filter $G_{\text{low}}$ to each 3D Gaussian primitive $G_k$ before projecting it onto screen space [6]</p>$$\begin{aligned} G_k(\mathbf{x})_{\text{reg}} = (G_k \otimes G_{\text{low}})(\mathbf{x}) \end{aligned}$$<p>This operation is efficient as convolving two Gaussians with covariance matrices $\Sigma_1$ and $\Sigma_2$ results in another Gaussian with variance $\Sigma_1 + \Sigma_2$. Hence,</p>$$\begin{aligned} G_k(\mathbf{x})_{\text{reg}} = \sqrt{\frac{|\Sigma_k|}{|\Sigma_k + \frac{s}{\hat{\nu}_k^2} \cdot \mathbf{I}|}} e^{-\frac{1}{2} (\mathbf{x} - \mathbf{p}_k)^T \left( \Sigma_k + \frac{s}{\hat{\nu}_k^2} \cdot \mathbf{I} \right)^{-1} (\mathbf{x} - \mathbf{p}_k)}
\end{aligned}$$<p>Here, $s$ is a scalar hyperparameter to control the size of the filter. Note that the scale $\frac{s}{\hat{\nu}_k^2}$ of the 3D filters for each primitive are different as they depend on the training views in which they are visible. By employing 3D Gaussian smoothing, we ensure that the highest frequency component of any Gaussian does not exceed half of its maximal sampling rate for at least one camera. Note that $G_{\text{low}}$ becomes an intrinsic part of the 3D representation, remaining constant post-training.</p><h1 id=4d-gaussian-splatting>4D Gaussian Splatting<a hidden class=anchor aria-hidden=true href=#4d-gaussian-splatting>#</a></h1><p>The 4D Gaussian splatting involves adding another dimension time, $t$, to represent the objects&rsquo; dynamics [7]. The rendering formula is</p>$$\mathcal{I}(u,v,t) = \sum_{i=1}^{N} p_i(u,v,t) \alpha_i c_i(d) \prod_{j=1}^{i-1} (1 - p_j(u,v,t) \alpha_j)$$<p>Note that $p_i(u,v,t)$ can be further factorized as a product of a conditional probability $p_i(u,v|t)$ and a marginal probability $p_i(t)$ at time $t$, yielding:</p>$$\mathcal{I}(u,v,t) = \sum_{i=1}^{N} p_i(t) p_i(u,v|t) \alpha_i c_i(d) \prod_{j=1}^{i-1} (1 - p_j(t) p_j(u,v|t) \alpha_j)$$<p>We parameterize its covariance matrix $\Sigma$ as the configuration of a 4D ellipsoid for easing model optimization:</p>$$\Sigma = R S S^{T} R^{T}$$<p>where $S$ is a scaling matrix and $R$ is a 4D rotation matrix. Since $S$ is diagonal, it can be completely inscribed by its diagonal elements as $S = \operatorname{diag}(s_x, s_y, s_z, s_t)$. On the other hand, a rotation in 4D Euclidean space can be decomposed into a pair of isotropic rotations, each of which can be represented by a quaternion. <a href=https://en.wikipedia.org/wiki/Rotations_in_4-dimensional_Euclidean_space>wiki</a></p><p>Specifically, given $q_l = (a, b, c, d)$ and $q_r = (p, q, r, s)$ denoting the left and right isotropic rotations respectively, $R$ can be constructed by:</p>$$R = L(q_l)R(q_r) =
\begin{pmatrix}
a & -b & -c & -d \\
b & a & -d & c \\
c & d & a & -b \\
d & -c & b & a 
\end{pmatrix}
\begin{pmatrix}
p & -q & -r & -s \\
q & p & s & -r \\
r & -s & p & q \\
s & r & -q & p
\end{pmatrix}$$<p>The mean of a 4D Gaussian can be represented by four scalars as $\mu = (\mu_x, \mu_y, \mu_z, \mu_t)$. Thus far we arrive at a complete representation of the general 4D Gaussian.</p><p>Subsequently, the conditional 3D Gaussian can be derived from the properties of the multivariate Gaussian with:</p>$$\mu_{xyz|t} = \mu_{1:3} + \Sigma_{1:3,4}\Sigma_{4,4}^{-1}(t - \mu_t),$$$$\Sigma_{xyz|t} = \Sigma_{1:3,1:3} - \Sigma_{1:3,4}\Sigma_{4,4}^{-1}\Sigma_{4,1:3}$$<p>Moreover, the marginal $p_i(t)$ is also a Gaussian in one-dimension:</p>$$p(t) = \mathcal{N}(t; \mu_4, \Sigma_{4,4})$$<h2 id=4d-spherindrical-harmonics>4D Spherindrical Harmonics<a hidden class=anchor aria-hidden=true href=#4d-spherindrical-harmonics>#</a></h2><p>Inspired by studies on head-related transfer function, we propose to represent $c_i(d, t)$ as the combination of a series of 4D spherindrical harmonics (4DSH) which are constructed by merging SH with different 1D-basis functions. For computational convenience, we use the Fourier series as the adopted 1D-basis functions. Consequently, 4DSH can be expressed as:</p>$$Z_{nl}^{m}(t, \theta, \phi) = \cos\left(\frac{2\pi n}{T} t\right) Y_{l}^{m}(\theta, \phi)$$<p>where $Y_{l}^{m}$ is the 3D spherical harmonics. The index $l \geq 0$ denotes its degree, and $m$ is the order satisfying $-l \leq m \leq l$. The index $n$ is the order of the Fourier series. The 4D spherindrical harmonics form an orthonormal basis in the spherindrical coordinate system.</p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><h2 id=appendix-a-understanding-the-accumulated-transmittance>Appendix A: Understanding the Accumulated Transmittance<a hidden class=anchor aria-hidden=true href=#appendix-a-understanding-the-accumulated-transmittance>#</a></h2><p><strong>Volume Density</strong> $\sigma(\mathbf{x})$:</p><ul><li>The volume density $\sigma(\mathbf{x})$ at a point $\mathbf{x}$ represents the differential probability per unit length that a ray will be absorbed or scattered at that point.</li></ul><ul><li><strong>Survival Probability</strong> $T(t)$:</li></ul><ul><li><p>$T(t)$ represents the probability that a ray traveling along a path $\mathbf{r}(t)$ from $t_n$ to $t_n$ has not been absorbed or scattered.</p></li><li><p>It can be thought of as the transmittance or the cumulative probability of the ray surviving up to point $t_n$.</p></li></ul><h3 id=differential-equation-for-transmittance>Differential Equation for Transmittance<a hidden class=anchor aria-hidden=true href=#differential-equation-for-transmittance>#</a></h3><p>To derive the transmittance function, we consider how the survival probability changes over an infinitesimal segment of the ray path.</p><ol><li><p><strong>Infinitesimal Segment</strong>:</p><ul><li><p>Consider a small segment of the ray&rsquo;s path from $t$ to $t + dt$.</p></li><li><p>The probability that the ray is not absorbed or scattered in this small segment is $1 - \sigma(\mathbf{r}(t)) dt$.</p></li></ul></li><li><p><strong>Survival Probability Over the Small Segment</strong>:</p><ul><li>If the ray has survived up to $t$ with probability $T(t)$, the probability that it also survives the next small segment $dt$ is:</li></ul>$$T(t + dt) = T(t) \cdot (1 - \sigma(\mathbf{r}(t)) dt)$$</li></ol><ul><li><p><strong>Taking the Limit</strong>:</p><ul><li><p>As $dt$ approaches zero, this becomes:</p>$$\frac{T(t + dt) - T(t)}{dt} \approx -\sigma(\mathbf{r}(t)) T(t)$$$$\frac{dT(t)}{dt} = -\sigma(\mathbf{r}(t)) T(t)$$</li></ul></li></ul><h3 id=solving-the-differential-equation>Solving the Differential Equation<a hidden class=anchor aria-hidden=true href=#solving-the-differential-equation>#</a></h3><ol><li><p><strong>Separation of Variables</strong>:</p><ul><li>Rewrite the differential equation to separate the variables $T$ and $t$:</li></ul>$$\frac{dT(t)}{T(t)} = -\sigma(\mathbf{r}(t)) dt$$</li><li><p><strong>Integrating Both Sides</strong>:</p><ul><li>Integrate both sides with respect to their respective variables:</li></ul>$$ \int \frac{dT(t)}{T(t)} = - \int \sigma(\mathbf{r}(t)) dt$$$$ \ln|T(t)| = - \int_{t_n}^t \sigma(\mathbf{r}(s)) ds + C$$<p>where $C$ is the integration constant.</p></li><li><p><strong>Exponentiating Both Sides</strong>:</p><ul><li>Solve for $T(t)$ by exponentiating both sides:</li></ul>$$T(t) = e^{-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds + C}$$$$ T(t) = e^{-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds}$$</li></ol><p>In neural radiance fields (NeRF) and other volumetric rendering techniques, spherical harmonics are used to efficiently represent the directional dependency of emitted radiance (color) at each point in the volume. Given that color is a three-dimensional vector (representing the RGB color channels), spherical harmonics can be employed to represent each of these color channels separately for each direction.</p><h2 id=appendix-b-spherical-harmonics-in-neural-radiance-fields>Appendix B: Spherical Harmonics in Neural Radiance Fields<a hidden class=anchor aria-hidden=true href=#appendix-b-spherical-harmonics-in-neural-radiance-fields>#</a></h2><figure style=text-align:center><img src=./resources/3d-hs.png alt="3D spherical harmonics" style="width:100%;margin:0 auto;display:block"></figure><ol><li><p><strong>Radiance Representation</strong>:</p><ul><li><p>In a neural radiance field, the color $c(\mathbf{x}, \mathbf{d})$ emitted from a point $\mathbf{x}$ in the direction $\mathbf{d}$ can vary depending on the direction $\mathbf{d}$.</p></li><li><p>Instead of storing the color for every possible direction, which is computationally infeasible, spherical harmonics allow for a compact representation.</p></li></ul></li><li><p><strong>Spherical Harmonics Expansion</strong>:</p><ul><li>The color at each point can be expanded into spherical harmonics:</li></ul>$$c(\mathbf{x}, \mathbf{d}) = \sum_{\ell=0}^{L} \sum_{m=-\ell}^{\ell} c_{\ell m}(\mathbf{x}) Y_{\ell m}(\mathbf{d})$$</li><li><p><strong>RGB Color Channels</strong>:</p><ul><li>Since color is a three-dimensional vector (RGB), each color channel can be represented using its own set of spherical harmonics coefficients:</li></ul>$$c_r(\mathbf{x}, \mathbf{d}) = \sum_{\ell=0}^{L} \sum_{m=-\ell}^{\ell} c_{\ell m}^r(\mathbf{x}) Y_{\ell m}(\mathbf{d})$$$$ c_g(\mathbf{x}, \mathbf{d}) = \sum_{\ell=0}^{L} \sum_{m=-\ell}^{\ell} c_{\ell m}^g(\mathbf{x}) Y_{\ell m}(\mathbf{d})$$$$ c_b(\mathbf{x}, \mathbf{d}) = \sum_{\ell=0}^{L} \sum_{m=-\ell}^{\ell} c_{\ell m}^b(\mathbf{x}) Y_{\ell m}(\mathbf{d})$$</li></ol><ul><li><p>Here, $c_{\ell m}^r(\mathbf{x})$, $c_{\ell m}^g(\mathbf{x})$, and $c_{\ell m}^b(\mathbf{x})$ are the spherical harmonics coefficients for the red, green, and blue color channels, respectively.</p></li><li><p>Write it in a more compact form</p></li></ul>$$\mathbf{c}(\mathbf{x}, \mathbf{d}) = \sum_{\ell=0}^{L} \sum_{m=-\ell}^{\ell} \mathbf{c}_{\ell m}(\mathbf{x}) Y_{\ell m}(\mathbf{d})$$<p>where $\mathbf{c}_{lm}=[c^r, c^g, c^b]^\top$.</p><ol start=4><li><p><strong>Advantages</strong>:</p><ul><li><p><strong>Compactness</strong>: Using a small number of spherical harmonics coefficients can effectively capture the variation in color with direction, reducing the memory and computational requirements.</p></li><li><p><strong>Efficiency</strong>: Once the coefficients are computed, evaluating the color for any direction $\mathbf{d}$ is efficient, involving only a sum of products.</p></li><li><p><strong>Smoothness</strong>: Spherical harmonics provide a smooth interpolation of the radiance function, which is particularly useful for rendering soft lighting and shadows.</p></li></ul></li><li><p><strong>Neural Network Training</strong>:</p><ul><li><p>During the training of a neural radiance field, the network learns to predict the spherical harmonics coefficients $c_{\ell m}(\mathbf{x})$ for each point $\mathbf{x}$.</p></li><li><p>The input to the network typically includes the spatial location $\mathbf{x}$ and sometimes additional features like view direction $\mathbf{d}$ to capture more complex lighting effects.</p></li></ul></li></ol><h2 id=appendix-c-from-surface-to-point-splatting>Appendix C: From Surface to Point Splatting<a hidden class=anchor aria-hidden=true href=#appendix-c-from-surface-to-point-splatting>#</a></h2><h3 id=surface-splatting-4>Surface splatting [4]<a hidden class=anchor aria-hidden=true href=#surface-splatting-4>#</a></h3><h4 id=texture-functions-on-point-based-objects>Texture functions on point-based objects<a hidden class=anchor aria-hidden=true href=#texture-functions-on-point-based-objects>#</a></h4><figure style=text-align:center><img src=./resources/image-1.png alt="surface splatting" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>figure from [4].</figcaption></figure><p>The points $\mathbf{Q}$ and $\mathbf{P_k}$ have local coordinates $\mathbf{u}$ and $\mathbf{u_k}$, respectively. We define the continuous surface function $f_c(\mathbf{u})$ as the weighted sum:</p>$$f_c(\mathbf{u}) = \sum_{k \in \mathbb{N}} w_k r_k (\mathbf{u} - \mathbf{u_k})$$<p>where the basis functions $r_k$ that have local support or that are appropriately truncated.</p><h4 id=rendering>Rendering<a hidden class=anchor aria-hidden=true href=#rendering>#</a></h4><figure style=text-align:center><img src=./resources/image-2.png alt="rendering via filtering" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>figure from [4].</figcaption></figure><ol><li>Warp $f_c(\mathbf{u})$ to screen space, yielding the warped, continuous screen space signal $g_c(\mathbf{x})$:</li></ol>$$g_c(\mathbf{x}) = (f_c \circ \mathbf{m}^{-1})(\mathbf{x}) = f_c(\mathbf{m}^{-1}(\mathbf{x}))$$<p>where $\circ$ denotes function concatenation.</p><ul><li>Band-limit the screen space signal using a prefilter $h$, resulting in the continuous output function $g'_c(\mathbf{x})$:</li></ul>$$ g'_c(\mathbf{x}) = g_c(\mathbf{x}) \otimes h(\mathbf{x}) = \int_{\mathbb{R}^2} g_c(\boldsymbol{\xi}) h(\mathbf{x} - \boldsymbol{\xi}) d\boldsymbol{\xi}$$<p>where $\otimes$ denotes convolution.</p><ul><li>Sample the continuous output function by multiplying it with an $\textit{impulse train}$ $i$ to produce the discrete output $g(\mathbf{x})$:</li></ul>$$ g(\mathbf{x}) = g'_c(\mathbf{x}) i(\mathbf{x})$$<p>An explicit expression for the warped continuous output function can be derived by expanding the above relations in reverse order:</p>$$\begin{equation}
g'_c(\mathbf{x}) = \int_{\mathbb{R}^2} h(\mathbf{x} - \boldsymbol{\xi}) \sum_{k \in \mathbb{N}} w_k r_k(\mathbf{m}^{-1}(\boldsymbol{\xi}) - \mathbf{u_k}) d\boldsymbol{\xi} = \sum_{k \in \mathbb{N}} w_k \rho_k(\mathbf{x})
\end{equation}$$<p>where</p>$$\rho_k(\mathbf{x}) = \int_{\mathbb{R}^2} h(\mathbf{x} - \boldsymbol{\xi}) r_k(\mathbf{m}^{-1}(\boldsymbol{\xi}) - \mathbf{u_k}) d\boldsymbol{\xi}$$<p>We call a warped and filtered basis function $\rho_k(\mathbf{x})$ a $\textit{resampling kernel}$$, which is expressed here as a screen space integral. Equation (5) states that we can first warp and filter each basis function $r_k$ individually to construct these resampling kernels $\rho_k$, and then sum up the contributions of these kernels in screen space. We call this approach $\textit{surface splatting}$.</p><figure style=text-align:center><img src=./resources/image-3.png alt="surface splatting illustration" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>figure from [4].</figcaption></figure><p>In order to simplify the integral for $\rho_k(\mathbf{x})$ in (5), we replace a general mapping $\mathbf{m}(\mathbf{u})$ by its local affine approximation $\mathbf{m_{u_k}}$ at a point $\mathbf{u_k}$,</p>$$\mathbf{m_{u_k}}(\mathbf{u}) = \mathbf{x_k} + \mathbf{J_{u_k}} \cdot (\mathbf{u} - \mathbf{u_k})$$<p>where $\mathbf{x_k} = \mathbf{m}(\mathbf{u_k})$ and the Jacobian $\mathbf{J_{u_k}} = \frac{\partial \mathbf{m}}{\partial \mathbf{u}} (\mathbf{u_k})$.</p><p>Since the basis functions $r_k$ have local support, $\mathbf{m_{u_k}}$ is used only in a small neighborhood around $\mathbf{u_k}$ in (5). Moreover, the approximation is most accurate in the neighborhood of $\mathbf{u_k}$ and so it does not cause visual artifacts. We use it to rearrange Equation (5), and after a few steps we find:</p>$$\rho_k(\mathbf{x}) = \int_{\mathbb{R}^2} h(\mathbf{x} - \mathbf{m_{u_k}}(\mathbf{u_k}) - \boldsymbol{\xi}) r'_k(\boldsymbol{\xi}) d\boldsymbol{\xi} = (r'_k \otimes h)(\mathbf{x} - \mathbf{m_{u_k}}(\mathbf{u_k}))$$<p>where $r'_k(\mathbf{x}) = r_k(\mathbf{J_{u_k}^{-1}} \mathbf{x})$ denotes a warped basis function. Thus, although the texture function is defined on an irregular grid, Equation (5) states that the resampling kernel in screen space, $\rho_k(\mathbf{x})$, can be written as a $\textit{convolution}$ of a warped basis function $r'_k$ and the low-pass filter kernel $h$. This is essential for the derivation of screen space EWA in the next section. Note that from now on we are omitting the subscript $u_k$ for $\mathbf{m}$ and $\mathbf{J}$.</p><h4 id=screen-space-ewa>Screen Space EWA<a hidden class=anchor aria-hidden=true href=#screen-space-ewa>#</a></h4><p>We choose elliptical Gaussians both for the basis functions and the low-pass filter, since they are closed under affine mappings and convolution. An elliptical Gaussian $\mathcal{G}_{\mathbf{V}}(\mathbf{x})$ with variance matrix $\mathbf{V}$ is defined as, for $\mathbf{x}\in\mathbb{R}^2$,</p>$$\mathcal{G}_{\mathbf{V}}(\mathbf{x}) = \frac{1}{2\pi \sqrt{|\mathbf{V}|}} e^{-\frac{1}{2} \mathbf{x}^T \mathbf{V}^{-1} \mathbf{x}}$$<p>where $|\mathbf{V}|$ is the determinant of $\mathbf{V}$. We denote the variance matrices of the basis functions $r_k$ and the low-pass filter $h$ with $\mathbf{V}^r_k$ and $\mathbf{V}^h$, respectively. The warped basis function and the low-pass filter are:</p>$$r'_k(\mathbf{x}) = r(\mathbf{J}^{-1}\mathbf{x}) = \mathcal{G}_{\mathbf{V}^r_k}(\mathbf{J}^{-1}\mathbf{x}) = \frac{1}{|\mathbf{J}|^{-1}} \mathcal{G}_{\mathbf{J}\mathbf{V}^r_k \mathbf{J}^T}(\mathbf{x})\\
h(\mathbf{x}) = \mathcal{G}_{\mathbf{V}^h}(\mathbf{x}).$$<p>The resampling kernel $\rho_k$ of (5) can be written as a single Gaussian with a variance matrix that combines the warped basis function and the low-pass filter. Typically $\mathbf{V}^h = \mathbf{I}$, yielding:</p>$$
\begin{aligned}
\rho_k(\mathbf{x}) &= (r'_k \otimes h)(\mathbf{x} - \mathbf{m}(\mathbf{u_k})) \\
&= \frac{1}{|\mathbf{J}|^{-1}} (\mathcal{G}_{\mathbf{J}\mathbf{V}^r_k \mathbf{J}^T} \otimes \mathcal{G}_{\mathbf{I}})(\mathbf{x} - \mathbf{m}(\mathbf{u_k})) \\
&= \frac{1}{|\mathbf{J}|^{-1}} \mathcal{G}_{\mathbf{J}\mathbf{V}^r_k \mathbf{J}^T + \mathbf{I}}(\mathbf{x} - \mathbf{m}(\mathbf{u_k}))
\end{aligned}
$$<p>We will show how to determine $\mathbf{J}^{-1}$ in Section 4, and how to compute $\mathbf{V}^r_k$ in Section 5. Substituting the Gaussian resampling kernel (6) into (5) we get the continuous output function is the weighted sum:</p>$$g'_c(\mathbf{x}) = \sum_{k \in \mathbb{N}} w_k \frac{1}{|\mathbf{J}|^{-1}} \mathcal{G}_{\mathbf{J}\mathbf{V}^r_k \mathbf{J}^T + \mathbf{I}} (\mathbf{x} - \mathbf{m}(\mathbf{u_k}))$$<p>We call this novel formulation $\textit{screen space EWA}$.</p><h4 id=object-with-per-point-color>Object with per point color<a hidden class=anchor aria-hidden=true href=#object-with-per-point-color>#</a></h4><p>Many of today&rsquo;s imaging systems, such as laser range scanners or passive vision systems, acquire range and color information. In such cases, the acquisition process provides a color sample $c_k$ with each point. We have to compute a continuous approximation $f_c(\mathbf{u})$ of the unknown original texture function from the irregular set of samples $c_k$.</p><p>A computationally reasonable approximation is to normalize the basis functions $r_k$ to form a partition of unity, i.e., to sum up to one everywhere. Then we use the samples as coefficients, hence $w_k = c_k$, and build a weighted sum of the samples $c_k$:</p>$$f_c(\mathbf{u}) = \sum_{k \in \mathbb{N}} c_k \hat{r}_k (\mathbf{u} - \mathbf{u}_k) = \sum_{k \in \mathbb{N}} c_k \frac{r_k (\mathbf{u} - \mathbf{u}_k)}{\sum_{j \in \mathbb{N}} r_j (\mathbf{u} - \mathbf{u}_j)}$$<h2 id=appendix-d-perspective-transformation>Appendix D: Perspective Transformation<a hidden class=anchor aria-hidden=true href=#appendix-d-perspective-transformation>#</a></h2><p>These statements may be unclear to readers. From reference [5], we know the author is discussing the transformation from ray space to image space. Converting the camera space, $[t_0, t_1, t_2]^\top$, to the ray space, $[x_0, x_1, x_2]^\top$ follows</p>$$\begin{aligned}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2
\end{bmatrix} =
\phi(\mathbf{t}) =
\begin{bmatrix}
t_0 / t_2 \\
t_1 / t_2 \\
\| (t_0, t_1, t_2) \|^T
\end{bmatrix}
\end{aligned}$$<p>Then, from ray space to camera space is simply integrating out $x_2$.</p><p>Actually, we can go directly from camera space to image space by using the perspective transformation</p>$$t_2
\begin{bmatrix}
x_0\\
x_1 \\
1
\end{bmatrix} =
\begin{bmatrix}
f_x&0&c_x\\
0&f_y&c_y\\
0&0&1
\end{bmatrix}
\begin{bmatrix}
t_0\\
t_1\\
t_2
\end{bmatrix}$$<p>Then, we do linear approximation.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] J. T. Kajiya and B. P. V. Herren, &ldquo;RAY TRACING VOLUME DENSITIES&rdquo;.</p><p>[2] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, &ldquo;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&rdquo;.</p><p>[3] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, &ldquo;3D Gaussian Splatting for Real-Time Radiance Field Rendering.&rdquo; arXiv, Aug. 08, 2023. doi: 10.48550/arXiv.2308.04079.</p><p>[4] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, &ldquo;Surface splatting,&rdquo; in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM, Aug. 2001, pp. 371–378. doi: 10.1145/383259.383300.</p><p>[5] M. Zwicker, H. Pfister, J. van Baar and M. Gross, &ldquo;EWA splatting,&rdquo; in IEEE Transactions on Visualization and Computer Graphics, vol. 8, no. 3, pp. 223-238, July-Sept. 2002, doi: 10.1109/TVCG.2002.1021576.</p><p>[6] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, &ldquo;Mip-Splatting: Alias-free 3D Gaussian Splatting&rdquo;.</p><p>[7] Z. Yang, H. Yang, Z. Pan, and L. Zhang, &ldquo;Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting,&rdquo; Feb. 22, 2024, arXiv: arXiv:2310.10642. Accessed: Jul. 11, 2024. [Online]. Available: <a href=http://arxiv.org/abs/2310.10642>http://arxiv.org/abs/2310.10642</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/3d-gaussian-splatting/>3D Gaussian Splatting</a></li><li><a href=https://livey.github.io/tags/3d-reconstruction/>3D Reconstruction</a></li><li><a href=https://livey.github.io/tags/nerf/>NeRF</a></li></ul><nav class=paginav><a class=prev href=https://livey.github.io/posts/2025-02-eol-calib/><span class=title>« Prev</span><br><span>End of Line Camera Calibration</span>
</a><a class=next href=https://livey.github.io/posts/2024-12-fast-lio/><span class=title>Next »</span><br><span>Fast LIO Paper Reading</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on x" href="https://x.com/intent/tweet/?text=3D%20Gaussian%20Splatting&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f&amp;hashtags=3DGaussianSplatting%2c3DReconstruction%2cNeRF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f&amp;title=3D%20Gaussian%20Splatting&amp;summary=3D%20Gaussian%20Splatting&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f&title=3D%20Gaussian%20Splatting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on whatsapp" href="https://api.whatsapp.com/send?text=3D%20Gaussian%20Splatting%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on telegram" href="https://telegram.me/share/url?text=3D%20Gaussian%20Splatting&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 3D Gaussian Splatting on ycombinator" href="https://news.ycombinator.com/submitlink?t=3D%20Gaussian%20Splatting&u=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-3dgs%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Iterative Closest Point Uncovered: Mathematical Foundations and Applications | Fuwei's Tech Notes</title>
<meta name=keywords content="Iterative Closest Point,ICP,Signal Processing,Optimization,SLAM"><meta name=description content="A detailed derivation of the Iterative Closest Point (ICP) problem."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2024-12-icp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="A detailed derivation of the Iterative Closest Point (ICP) problem."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2024-12-icp/"><meta property="og:title" content="Iterative Closest Point Uncovered: Mathematical Foundations and Applications"><meta property="og:description" content="A detailed derivation of the Iterative Closest Point (ICP) problem."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Iterative Closest Point Uncovered: Mathematical Foundations and Applications"><meta name=twitter:description content="A detailed derivation of the Iterative Closest Point (ICP) problem."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2024-12-icp/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="Iterative Closest Point Uncovered: Mathematical Foundations and Applications"><meta property="og:description" content="A detailed derivation of the Iterative Closest Point (ICP) problem."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-26T00:00:00+00:00"><meta property="article:tag" content="Iterative Closest Point"><meta property="article:tag" content="ICP"><meta property="article:tag" content="Signal Processing"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="SLAM"><meta property="og:image" content="https://livey.github.io/posts/2024-12-icp/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2024-12-icp/%3Cimage%20path/url%3E"><meta name=twitter:title content="Iterative Closest Point Uncovered: Mathematical Foundations and Applications"><meta name=twitter:description content="A detailed derivation of the Iterative Closest Point (ICP) problem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Iterative Closest Point Uncovered: Mathematical Foundations and Applications","item":"https://livey.github.io/posts/2024-12-icp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Iterative Closest Point Uncovered: Mathematical Foundations and Applications","name":"Iterative Closest Point Uncovered: Mathematical Foundations and Applications","description":"A detailed derivation of the Iterative Closest Point (ICP) problem.","keywords":["Iterative Closest Point","ICP","Signal Processing","Optimization","SLAM"],"articleBody":"In this post, we will discuss the Iterative Closest Point (ICP) problem: from point-to-point and point-to-plane ICP to generalized ICP.\nProblem Formulation Let two 3D point-sets $\\mathcal{X} = \\{\\mathbf{x}_i\\}, i = 1, \\ldots, N$ and $\\mathcal{Y} = \\{\\mathbf{y}_j\\}, j = 1, \\ldots, M$, where $\\mathbf{x}_i, \\mathbf{y}_j \\in \\mathbb{R}^3$ are point coordinates, be the data point-set and the model point-set respectively. The goal is to estimate a rigid motion with rotation $\\mathbf{R} \\in SO(3)$ and translation $\\mathbf{t} \\in \\mathbb{R}^3$ that minimizes the following $L_2$-error $E$:\n$$\\underset{\\mathbf{R}, \\mathbf{t}}{\\arg\\min E(\\mathbf{R}}, \\mathbf{t}) = \\sum_{i=1}^N e_i(\\mathbf{R}, \\mathbf{t})^2 = \\sum_{i=1}^N \\left\\| \\mathbf{R} \\mathbf{x}_i + \\mathbf{t} - \\mathbf{y}_{j^*} \\right\\|^2 \\tag{1}$$where $e_i(\\mathbf{R}, \\mathbf{t})$ is the per-point residual error for $x_i$. Given $\\mathbf{R}$ and $\\mathbf{t}$, the point $y_{j^*} \\in \\mathcal{Y}$ is denoted as the optimal correspondence of $x_i$, which is the closest point to the transformed $x_i$ in $\\mathcal{Y}$, i.e.,\n$$j^* = \\underset{j \\in \\{1, \\ldots, M\\}}{\\arg \\min} \\left\\| \\mathbf{R} x_i + \\mathbf{t} - \\mathbf{y}_j \\right\\|\\tag{2}$$Note the short-hand notation used here: $j^*$ varies as a function of $(\\mathbf{R}, \\mathbf{t})$ and also depends on $x_i$.\nIterative closest point algorithm solves problem (1) by iteratively solving problem (1) and (2).\nIterative Solution Solving the Problem with Fixed Pose With fixed pose, problem (2) can be solved with a computational complexity of $\\mathcal{O}(NM)$. However, we can build an octree to accelerate the closest point search. Further, if the data points are received sequentially, we can use an incremental kd-tree to update the tree [4].\nSolving the Problem with Known Point Correspondence With known point correspondence, problem (1) can be solved analytically.\n$$\\min_{\\mathbf{R}\\in SO(3), \\mathbf{t}}\\|\\mathbf{R}\\mathbf{x}_i+t-\\mathbf{y}_i\\|_2^2$$For any $\\mathbf{R}$, taking the derivative of the objective with respect to $\\mathbf{t}$ and letting it equal to zero we have\n$$\\mathbf{t} = \\frac{1}{N}\\sum_i(\\mathbf{y}_i - \\mathbf{R}\\mathbf{x}_i) = \\bar{\\mathbf{y}} - \\mathbf{R}\\bar{\\mathbf{x}}$$where we have $\\bar{\\mathbf{y}}\\triangleq\\frac{1}{N}\\sum_i \\mathbf{y}_i$ and $\\bar{\\mathbf{x}}\\triangleq\\frac{1}{N}\\sum_i \\mathbf{x}_i$. So, plug them into the objective function and let $\\mathbf{x}_i\\triangleq \\mathbf{x}_i-\\bar{\\mathbf{x}}$ and $\\mathbf{y}_i\\triangleq \\mathbf{y}_i - \\bar{\\mathbf{y}}$, we have\n$$\\min_{\\mathbf{R}\\in SO(3)} \\|\\mathbf{R}\\mathbf{x}_i - \\mathbf{y}_i\\|_2^2$$It is equivalent to\n$$\\max_{\\mathbf{R}\\in SO(3)}\\,\\mathbf{y}_i^\\top\\mathbf{R}\\mathbf{x}_i$$and further we have\n$$\\max_{\\mathbf{R}\\in SO(3)}\\text{tr}(\\mathbf{R}\\sum_i \\mathbf{x}_i\\mathbf{y}_i^\\top)$$Let $\\mathbf{M} = \\sum_i \\mathbf{x}_i\\mathbf{y}_i^\\top$. We are thus solving:\n$$\\max_{\\mathbf{R}\\in SO(3)}\\text{tr}(\\mathbf{R}\\mathbf{M})$$Let the SVD of $\\mathbf{M} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. Then $\\text{tr}(\\mathbf{R}\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top) = \\text{tr}(\\mathbf{V}^\\top\\mathbf{R}\\mathbf{U}\\mathbf{\\Sigma}) \\triangleq \\text{tr}(\\mathbf{Q}\\mathbf{\\Sigma})$, where $\\mathbf{Q}=\\mathbf{V}^\\top\\mathbf{R}\\mathbf{U}$ is an orthonormal matrix.\n$$\\text{tr}(\\mathbf{Q}\\mathbf{\\Sigma}) = \\sum_i q_{i,i}\\sigma_{i}\\leq \\sum_i \\sigma_{i}$$The last equality holds when $q_{i,i}=1$ for all $i$ and $\\sigma_{i,i}\u003e0$, which means $\\mathbf{Q}=\\mathbf{I}$. Therefore, we have $\\mathbf{R}=\\mathbf{V}\\mathbf{U}^\\top$.\nSince $SO(3)$ puts a determinant constraint on the orthonormal matrix with $\\text{det}(R)=1$, we have two cases:\nIf $\\text{det}(\\mathbf{R})=1$, the optimal solution is $\\mathbf{R}=\\mathbf{V}\\mathbf{U}^\\top$. If $\\text{det}(\\mathbf{R})=-1$, the algorithm fails. In the case where $\\text{det}(\\mathbf{R})=-1$ and there exists at least one $\\sigma_i$ that is zero, we can multiply $-1$ to its corresponding column of $\\mathbf{U}$ or $\\mathbf{V}$.\nIn the case where $\\text{det}(\\mathbf{R})=-1$ and all $\\sigma_i$’s are positive, “This can happen only when the noises are very large. In that case, the least-squares solution is probably useless anyway. A better approach would be to use a RANSAC-like technique (using 3 points at a time) to combat against outliers” [1].\nOther Variants of ICP Let us first write the cost of problem (1) in a more general form:\n$$ \\underset{\\mathbf{T}}{\\text{argmin}} \\sum_{i=1}^N \\ell( \\mathbf{T}\\mathbf{x}_i, \\mathbf{y}_{j^*}) $$where $\\mathbf{T} \\in SE(3)$ and we have written the points in its homogeneous form.\nRobust Point-to-point ICP When there are outliers in the dataset or not every point has a corresponding correspondence, we can use a robust loss function $\\ell$ to handle them. Typical choices are\n$$ \\ell(\\mathbf{T}\\mathbf{x}, \\mathbf{y}) = w\\|\\mathbf{T}\\mathbf{x}-\\mathbf{y}\\|_2^2 $$where\n$$ w = \\begin{cases} 1, \u0026 \\text{if } \\|\\mathbf{T}\\mathbf{x}-\\mathbf{y}\\|_2 \u003c d_{max} \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$which means when the distance is larger than $d_{max}$, the point is considered an outlier and ignored.\nPoint to Plane ICP The point-to-plane ICP is used as a more robust and accurate variant of the standard ICP. It minimizes the distance between the point and the plane [5]. The corresponding cost function is\n$$ \\ell(\\mathbf{T}\\mathbf{x}, \\mathbf{y}) = \\|w\\cdot\\mathbf{n}^\\top(\\mathbf{T}\\mathbf{x}-\\mathbf{y})\\|_2^2 $$where $\\mathbf{n}$ is the normal vector of the plane where the point $\\mathbf{y}$ is located.\nPoint-to-Line ICP This variant is used when we know the target feature is a line. So, the cost function is the distance between the point and the line.\n$$ \\ell(\\mathbf{T}\\mathbf{x}, \\mathbf{y}) = \\|w\\cdot(\\mathbf{I} - \\mathbf{n}\\mathbf{n}^\\top)(\\mathbf{T}\\mathbf{x}-\\mathbf{y})\\|_2^2 $$where $\\mathbf{n}$ is the unit direction vector of the line in which the point $\\mathbf{y}$ is located.\nGeneralized ICP Assume points $\\{\\mathbf{x}_i\\}_{i=1}^N$ and $\\{\\mathbf{y}_i\\}_{i=1}^N$ are already aligned, $\\mathbf{x}_i\\sim \\mathcal{N}(\\bar{\\mathbf{x}}_i, \\mathbf{C}^x_i)$ and $\\mathbf{y}_i\\sim \\mathcal{N}(\\bar{\\mathbf{y}}_i, \\mathbf{C}^y_i)$. Further, assume $\\mathbf{y}_i = \\mathbf{T}\\mathbf{x}_i$. Thus, the residual, $\\mathbf{d}^T_i = \\mathbf{y}_i - \\mathbf{T}\\mathbf{x}_i$, is distributed as $\\mathcal{N}(0, \\mathbf{C}^y_i + \\mathbf{T}\\mathbf{C}^x_i\\mathbf{T}^\\top)$. Using the maximum log likelihood estimation, we have [6]\n$$ \\underset{\\mathbf{T}}{\\text{argmin}} \\sum_{i=1}^N (\\mathbf{d}_i^T)^\\top\\left(\\mathbf{C}^y_i + \\mathbf{T}\\mathbf{C}^x_i\\mathbf{T}^\\top\\right)^{-1}\\mathbf{d}_i^T. \\tag{3} $$If we set $\\mathbf{C}_i^x = \\mathbf{0}$ and $\\mathbf{C}_i^y =\\mathbf{I}$, problem (3) is equivalent to the standard point-to-point ICP.\nIf we set $\\mathbf{C}_i^x =\\mathbf{0}$ and $\\mathbf{C}_i^y = \\mathbf{P}_i^{-1}$, where $\\mathbf{P}_i$ is the projection onto the space spanned by the plane normal. Then, problem (3) is equivalent to the point-to-plane ICP. Also, we can construct the point-to-line ICP from the generalized ICP problem formulation.\nPlane to Plane ICP If the points are generated from a plane, we can assume that the points are generated from the distribution $\\mathcal{N}(\\bar{\\mathbf{x}}, \\mathbf{\\Sigma})$, where\n$$ \\mathbf{\\Sigma} = \\mathbf{U} \\begin{bmatrix} \\mathbf{s_1} \u0026 \\mathbf{0} \u0026 \\mathbf{0} \\\\ \\mathbf{0} \u0026 \\mathbf{s_2} \u0026 \\mathbf{0} \\\\ \\mathbf{0} \u0026 \\mathbf{0} \u0026 \\mathbf{\\epsilon} \\end{bmatrix} \\mathbf{U}^\\top, $$where $\\epsilon$ should be a positive number near zero. By applying PCA on the point and along with its near by points, we can find such covariance matrix. Then, with the estimated covariance matrix, we can plug it into problem (3) to solve the generalized ICP problem, which is also called as the plane-to-plane ICP in [6]. In real practice, we should force $\\mathbf{\\Sigma}=\\text{diag}(1, 1, \\epsilon)$ to mitigate the effect of non uniformly distributed LiDAR points.\nNormal Distributions Transform The Normal Distributions Transform (NDT) method [7], published before Generalized ICP [6], offers an alternative approach to scan matching.\nThe NDT models the distribution of all reconstructed 2D-Points of one laser scan by a collection of local normal distributions. First, the 2D space around the robot is subdivided regularly into cells with constant size. Then for each cell, that contains at least three points, the following is done:\nCollect all 2D-Points $\\mathbf{x}_i$, $i = 1..n$, contained in this box. Calculate the mean $\\mathbf{q} = \\frac{1}{n} \\sum_i \\mathbf{x}_i$. Calculate the covariance matrix $$ \\Sigma = \\frac{1}{n} \\sum_i (\\mathbf{x}_i - \\mathbf{q})(\\mathbf{x}_i - \\mathbf{q})^t. $$The probability of measuring a sample at 2D-point $\\mathbf{x}$ contained in this cell is now modeled by the normal distribution $N(\\mathbf{q}, \\Sigma)$:\n$$ p(\\mathbf{x}) \\sim \\exp\\left( -\\frac{(\\mathbf{x} - \\mathbf{q})^t \\Sigma^{-1} (\\mathbf{x} - \\mathbf{q})}{2} \\right). $$The outline of the proposed approach, given two scans (the first one and the second one), is as follows:\nBuild the NDT of the first scan. Initialize the estimate for the parameters (by zero or by using odometry data). For each sample of the second scan: Map the reconstructed 2D point into the coordinate frame of the first scan according to the parameters. Determine the corresponding normal distributions for each mapped point. The score for the parameters is determined by evaluating the distribution for each mapped point and summing the result. Calculate a new parameter estimate by trying to optimize the score. This is done by performing one step of Newton’s Algorithm. Go to step 3 until a convergence criterion is met. The first four steps are straightforward: Building the NDT was described in the last section. As noted above, odometry data could be used to initialize the estimate. Mapping the second scan is done using $T$ and finding the corresponding normal distribution is a simple lookup in the grid of the NDT.\nThe rest is now described in detail using the following notation:\n$\\mathbf{p} = (p_i)_{i=1..3} = (t_x, t_y, \\phi)^t$: The vector of the parameters to estimate. $\\mathbf{x}_i$: The reconstructed 2D point of laser scan sample $i$ of the second scan in the coordinate frame of the second scan. $\\mathbf{x}_i'$: The point $\\mathbf{x}_i$ mapped into the coordinate frame of the first scan according to the parameters $\\mathbf{p}$, that is $\\mathbf{x}_i' = T(\\mathbf{x}_i, \\mathbf{p})$. $\\Sigma_i$, $\\mathbf{q}_i$: The covariance matrix and the mean of the corresponding normal distribution to point $\\mathbf{x}_i'$, looked up in the NDT of the first scan. The mapping according to $\\mathbf{p}$ could be considered optimal if the sum evaluating the normal distributions of all points $\\mathbf{x}_i'$ with parameters $\\Sigma_i$ and $\\mathbf{q}_i$ is a maximum. We call this sum the score of $\\mathbf{p}$. It is defined as:\n$$ \\text{score}(\\mathbf{p}) = \\sum_i \\exp\\left( -\\frac{(\\mathbf{x}_i' - \\mathbf{q}_i)^t \\Sigma_i^{-1} (\\mathbf{x}_i' - \\mathbf{q}_i)}{2} \\right). $$Finally, Newton’s method is applied iteratively to find the optimal parameters.\nVoxelized GICP Voxelized GICP [8] is built upon GICP [6]. It employs a point to multi-voxel mapping so that it eliminates the nearest neighbor search in the GICP. Then, it can efficiently compute the loss function by parallelizing the computation.\nThe following notation is similar to the one used in Eq. (3).\nTo derive the voxelized GICP algorithm, we first extend $\\tilde{\\mathbf{d}}_i$ so that it calculates the distances between $\\mathbf{a}_i$ and its neighbor points $\\{\\mathbf{b}_j \\mid \\|\\mathbf{a}_i - \\mathbf{b}_j\\| \u003c r\\}$ as follows:\n$$ \\tilde{\\mathbf{d}}_i = \\sum_j \\left( \\hat{\\mathbf{b}}_j - \\mathbf{T} \\hat{\\mathbf{a}}_i \\right). $$This equation can be interpreted as smoothing the target point distributions. Then, the distribution of $\\tilde{\\mathbf{d}}_i$ is given by\n$$ \\tilde{\\mathbf{d}}_i \\sim \\left( \\boldsymbol{\\mu}^{\\tilde{d}_i}, \\mathbf{C}^{\\tilde{d}_i} \\right), $$$$ \\mathbf{\\mu}^{\\tilde{d}_i} = \\sum_j \\left( \\hat{\\mathbf{b}}_j - \\mathbf{T} \\hat{\\mathbf{a}}_i \\right) = \\mathbf{0}, $$$$ \\mathbf{C}^{\\tilde{d}_i} = \\sum_j \\left( \\mathbf{C}^B_j + \\mathbf{T} \\mathbf{C}^A_i \\mathbf{T}^T \\right). $$We estimate the transformation $\\mathbf{T}$ that maximizes the log-likelihood of as follows:\n$$ \\mathbf{T} = \\arg\\min_\\mathbf{T} \\sum_i \\left( \\tilde{\\mathbf{d}}_i^T \\tilde{\\mathbf{C}}_i^{-1} \\tilde{\\mathbf{d}}_i \\right), $$where\n$$ \\tilde{\\mathbf{d}}_i = \\sum_j \\left( \\mathbf{b}_j - \\mathbf{T} \\mathbf{a}_i \\right), $$$$ \\tilde{\\mathbf{C}}_i = \\sum_j \\left( \\mathbf{C}^B_j + \\mathbf{T} \\mathbf{C}^A_i \\mathbf{T}^T \\right). $$To efficiently calculate the above equation, we modify it to:\n$$ \\mathbf{T} = \\arg\\min_\\mathbf{T} \\sum_i \\left( N_i \\tilde{\\mathbf{d}}_i^T \\tilde{\\mathbf{C}}_i^{-1} \\tilde{\\mathbf{d}}_i \\right), $$where:\n$$ \\tilde{\\mathbf{d}}_i = \\frac{\\sum_j \\mathbf{b}_j}{N_i} - \\mathbf{T} \\mathbf{a}_i, $$$$ \\tilde{\\mathbf{C}}_i = \\frac{\\sum_j \\mathbf{C}^B_j}{N_i} + \\mathbf{T} \\mathbf{C}^A_i \\mathbf{T}^T. $$where $N_i$ is the number of neighbor points. It suggests that we can efficiently compute the objective function by substituting the mean of the distributions of the points ($\\mathbf{b}_j$ and $\\mathbf{C}^B_j$) around $\\mathbf{a}_i$ for $\\mathbf{b}_i$ and $\\mathbf{C}^B_i$ and weighting the function by $N_i$.\nWe can naturally adapt this equation to voxel-based calculation by storing $\\mathbf{b}'_i = \\frac{\\sum \\mathbf{b}_j}{N_i}$ and $\\mathbf{C}'_i = \\frac{\\sum \\mathbf{C}^B_j}{N_i}$ in each voxel.\nFollowing the log-likelihood function, it uses Gauss-Newton method to optimize the objective function.\nPerformance Metrics Information Matrix The information matrix in the context of ICP is typically the Hessian matrix of the ICP cost function, which quantifies the curvature of the cost function at its minimum. This matrix, often a 6x6 matrix in 3D (corresponding to three translational and three rotational degrees of freedom), represents the precision or reliability of the estimated transformation parameters. In optimization, the Hessian is related to the Fisher information matrix, and its inverse provides the covariance matrix, indicating the uncertainty in the transformation estimate.\nThe information matrix is used to assess the reliability of the ICP alignment by providing an estimate of the uncertainty in the transformation parameters. A well-conditioned information matrix (with large diagonal elements) indicates high confidence in the transformation, suggesting that the point clouds are well-aligned with sufficient geometric constraints. Conversely, a poorly conditioned matrix (e.g., with small eigenvalues) suggests high uncertainty, possibly due to insufficient overlap, flat surfaces, or noisy data.\nFitness Score The fitness score measures the overlap between the source and target point clouds, defined as the ratio of inlier correspondences (points within a maximum correspondence distance) to the total number of points in the target cloud. A higher fitness score suggests better alignment coverage.\nInlier RMSE Inlier RMSE is the RMSE calculated only for inlier correspondences, providing a measure of alignment precision for the overlapping regions. It is expressed as:\n$$ \\text{Inlier RMSE} = \\sqrt{\\frac{1}{N_{\\text{inliers}}} \\sum_{i \\in \\text{inliers}} \\|\\mathbf{p}_i - \\mathbf{q}_i\\|^2} $$where $N_{\\text{inliers}}$ is the number of inliers. Lower values indicate higher precision.\nTransformation Errors When ground truth is available, transformation errors directly measure the accuracy of the estimated rotation and translation. These include:\nTranslation Error $e_t$: The Euclidean norm of the difference between estimated and true translation vectors: $$ e_t = \\sqrt{\\Delta x^2 + \\Delta y^2 + \\Delta z^2} $$ Rotation Error $e_r$: The geodesic distance between estimated and true rotation matrices: $$ e_r = \\arccos\\left(\\frac{\\text{trace}(\\Delta \\mathbf{R}) - 1}{2}\\right) $$Apppendix Rotation Error Derivation Below is a compact derivation that shows: why the shortest‐path (geodesic) rotation error between two rotation matrices is captured by\n$$ e_r=\\arccos\\!\\Bigl(\\tfrac{\\operatorname{tr}(\\Delta \\mathbf{R})-1}{2}\\Bigr), \\qquad \\Delta \\mathbf{R}:=\\mathbf{R}_{\\text{gt}}^{\\top}\\mathbf{R}_{\\text{est}}\\in\\mathrm{SO}(3). $$Distance on SO(3) $SO(3)$ is the Lie group of $3\\times3$ rotation matrices. A natural, bi-invariant Riemannian metric gives a geodesic distance $$ \\begin{aligned} d(\\mathbf{R}_1,\\mathbf{R}_2) \u0026=\\|\\log(\\mathbf{R}_1^{\\top}\\mathbf{R}_2)\\|_F/\\sqrt{2}\\\\ \u0026=\\|\\text{Log}(\\mathbf{R}_1^{\\top}\\mathbf{R}_2)\\|_2 \\end{aligned} $$where “$\\log$” is the matrix logarithm sending a rotation to its axis–angle element of the Lie algebra $\\mathfrak{so}(3)$ and “$\\text{Log}$” is the matrix logarithm sending a rotation to its vector space.\nFor $\\Delta \\mathbf{R}\\in\\mathrm{SO}(3)$ there exists an axis $\\mathbf{u}\\in\\mathbb{S}^2$ and angle $\\theta\\in[-\\pi,\\pi]$ such that\n$$ \\Delta \\mathbf{R}=\\exp\\bigl(\\,[\\mathbf{u}]_\\times\\,\\theta\\bigr) \\quad\\Rightarrow\\quad \\log(\\Delta R)=[\\mathbf{u}]_\\times\\theta, $$so the Frobenius norm of the log is $\\|[\\mathbf{u}]_\\times\\theta\\|_F=\\sqrt{2}\\,|\\theta|$. Hence $d(\\mathbf{R}_1,\\mathbf{R}_2)=|\\theta|$. The distance is exactly the rotation angle $\\theta$.\nRelating the Angle $\\theta$ to the Trace Start with Rodrigues’ formula for any $\\theta$ and unit axis $\\mathbf{u}$:\n$$ \\exp\\bigl([\\mathbf{u}]_\\times\\theta\\bigr) =I+\\sin\\theta\\,[\\mathbf{u}]_\\times +(1-\\cos\\theta)\\,[\\mathbf{u}]_\\times^{2}. $$Taking the trace and using $\\operatorname{tr}([\\mathbf{u}]_\\times)=0$ and $\\operatorname{tr}([\\mathbf{u}]_\\times^{2})=-2$:\n$$ \\operatorname{tr}(\\Delta \\mathbf{R})=\\operatorname{tr}(\\mathbf{I})+0+(-2)(1-\\cos\\theta) =1+2\\cos\\theta. $$Solve for $\\theta$:\n$$ \\cos\\theta=\\tfrac{\\operatorname{tr}(\\Delta \\mathbf{R})-1}{2} \\quad\\Longrightarrow\\quad \\theta=\\arccos\\!\\Bigl(\\tfrac{\\operatorname{tr}(\\Delta \\mathbf{R})-1}{2}\\Bigr). $$Putting it together\nCompute the relative rotation $\\Delta \\mathbf{R}=\\mathbf{R}_{\\text{gt}}^{\\top}\\mathbf{R}_{\\text{est}}$ . (Because the metric is right-invariant, left–multiplying both rotations by the same matrix has no effect.) Extract the rotation angle with the trace formula above. That angle is the geodesic distance on $SO(3)$, so we call it the rotation error $e_r$. Useful Properties Invariance: $e_r\\bigl(\\mathbf{Q}\\mathbf{R}_{\\text{gt}},\\,\\mathbf{Q}\\mathbf{R}_{\\text{est}}\\bigr)=e_r(\\mathbf{R}_{\\text{gt}},\\mathbf{R}_{\\text{est}})$ for any $\\mathbf{Q}\\in SO(3)$. Range: trace $(\\Delta R)\\in[-1,3] \\Rightarrow e_r\\in[0,\\pi]$. Small-angle limit: when $\\theta\\!\\ll\\!1$, $e_r\\approx\\|\\mathbf{r}\\|_2$ where $\\mathbf{r}$ is the 3-vector in the Lie-algebra log. Thus the trace-based formula is simply a closed-form way of reading off the minimal rotation angle between two orientations, which is exactly the geodesic distance on the manifold of rotations.\nInformation Matrix Derivation Definition of the Information Matrix The information matrix in ICP is typically the Hessian matrix of the cost function, which quantifies the curvature of the cost function at its minimum. This $6 \\times 6$ matrix corresponds to the six degrees of freedom in 3D rigid transformations (three for rotation, three for translation). In estimation theory, it is related to the Fisher information matrix, and its inverse provides the covariance matrix, indicating the uncertainty in the transformation estimate. A well-conditioned information matrix (with large eigenvalues) suggests high confidence in the alignment, while a poorly conditioned matrix indicates potential ambiguities, such as in featureless environments. [10]\nTheoretical Foundation The information matrix is derived from the Hessian of the ICP cost function. For least squares problems, the cost function is of the form\n$$ f = \\frac{1}{2} \\sum_i \\|\\mathbf{r}_i(\\boldsymbol{\\theta})\\|^2 $$where $\\mathbf{r}_i$ is the residual for each correspondence and $\\boldsymbol{\\theta}$ is the optimization variable. Take second derivative of the cost function with respect to the optimization variable, we have:\n$$ \\begin{aligned} \\nabla^2_{\\boldsymbol{\\theta}} f \u0026= \\nabla_{\\boldsymbol{\\theta}}\\left( \\sum_i \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i \\cdot \\mathbf{r}_i \\right) \\\\ \u0026= \\sum_i \\nabla^2_{\\boldsymbol{\\theta}} \\mathbf{r}_i \\cdot \\mathbf{r}_i + \\sum_i \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i^T \\end{aligned} $$when $\\mathbf{r}_i$ is approximately zero, the Hessian matrix is:\n$$ \\mathbf{H} = \\nabla^2_{\\boldsymbol{\\theta}} f \\approx \\sum_i \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i^T =\\mathbf{J}^\\top \\mathbf{J} $$where $\\mathbf{J}$ is the Jacobian of the residuals with respect to the transformation parameters.\nNote: the Jacobian is defined as $\\mathbf{J} = \\nabla_{\\boldsymbol{\\theta}} \\mathbf{r}_i^\\top$. The shape differs from traditional matrix calculus textbook.\nIn ICP, the transformation is parameterized by a 6-vector $\\xi = (\\boldsymbol{\\omega}, \\mathbf{t})$, where $\\boldsymbol{\\omega}$ represents the rotation (often as a rotation vector for small angles) and $\\mathbf{t}$ is the translation.\nPoint-to-Point ICP Cost Function For point-to-point ICP, the cost function is:\n$$ f = \\sum_i \\| \\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i \\|^2 $$where $\\mathbf{p}_i$ are source points, $\\mathbf{q}_i$ are target points, and $\\mathbf{R}$ and $\\mathbf{t}$ are the rotation matrix and translation vector. Let us represent the rotation as, $\\mathbf{R} = \\exp([\\boldsymbol{\\omega}]_\\times) \\mathbf{R}_0$ with an already obtained rotation matrix $\\mathbf{R}_0$ and a small perturbation, $\\boldsymbol{\\omega} \\in \\mathbb{R}^3$, around it, and $[\\boldsymbol{\\omega}]_\\times$ is the skew-symmetric matrix. (Here we can also use right perturbation, $\\mathbf{R} = \\mathbf{R}_0 \\exp([\\boldsymbol{\\omega}]_\\times)$. But for iterative optimization method, right perturbation is preferred. For right perturbation, you can compute $\\mathbf{R}_0\\mathbf{p}_i$ iteratively. When an updating is made, we can re-use the previous $\\mathbf{R}_0\\mathbf{p}_i$ to compute the new $\\mathbf{R}_0\\mathbf{p}_i$. However, for IMU, we use right perturbation $\\dot{\\mathbf{R}} = \\mathbf{R}[\\boldsymbol{\\omega}]_\\times$.) By expanding the matrix exponential and omit the higher order terms, we have, $\\mathbf{R} \\approx (\\mathbf{I} + [\\boldsymbol{\\omega}]_\\times) \\mathbf{R}_0$. The residual for each correspondence is:\n$$ r_i = \\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i \\approx \\mathbf{R}_0\\mathbf{p}_i - \\mathbf{q}_i + \\mathbf{t} + [\\boldsymbol{\\omega}]_\\times\\mathbf{R}_0\\mathbf{p}_i $$Since $[\\boldsymbol{\\omega}]_\\times \\mathbf{R}_0 \\mathbf{p}_i = \\boldsymbol{\\omega} \\times (\\mathbf{R}_0 \\mathbf{p}_i) = - \\mathbf{R}_0 \\mathbf{p}_i \\times \\boldsymbol{\\omega} = - [\\mathbf{R}_0 \\mathbf{p}_i]_\\times \\boldsymbol{\\omega}$, we have:\n$$ r_i \\approx (\\mathbf{R}_0\\mathbf{p}_i - \\mathbf{q}_i) + \\mathbf{t} - [\\mathbf{R}_0 \\mathbf{p}_i]_\\times \\boldsymbol{\\omega}. $$So, the Jacobian $\\mathbf{J}_i = [\\frac{\\partial \\mathbf{r}_i}{\\partial \\boldsymbol{\\omega}}, \\frac{\\partial \\mathbf{r}_i}{\\partial \\mathbf{t}}]$ is:\n$$ \\mathbf{J}_i = \\begin{bmatrix} -[\\mathbf{R}_0 \\mathbf{p}_i]_\\times \u0026 \\mathbf{I}_3 \\end{bmatrix} $$Hence, the Hessian is:\n$$ \\begin{aligned} \\mathbf{H} \u0026= \\sum_i \\mathbf{J}_i^T \\mathbf{J}_i \\\\ \u0026= \\begin{bmatrix} \\sum_i [\\mathbf{R}_0 \\mathbf{p}_i]_\\times^T [\\mathbf{R}_0 \\mathbf{p}_i]_\\times \u0026 \\sum_i [\\mathbf{R}_0 \\mathbf{p}_i]_\\times^T \\\\ \\sum_i [\\mathbf{R}_0 \\mathbf{p}_i]_\\times \u0026 \\sum_i \\mathbf{I}_3 \\end{bmatrix}. \\end{aligned} $$The Hessian with the translation is always constant and the correlation between the rotation and translation is $\\sum_i [\\mathbf{R}_0 \\mathbf{p}_i]_\\times$. The Hessian for the rotation is $\\sum_i [\\mathbf{R}_0 \\mathbf{p}_i]_\\times^T [\\mathbf{R}_0 \\mathbf{p}_i]_\\times$. Here, we are particularly interested in the rotation part.\nSince $[\\boldsymbol{\\omega}]_\\times^\\top = -[\\boldsymbol{\\omega}]_\\times$, and $[\\boldsymbol{\\omega}]_\\times^2 = \\boldsymbol{\\omega} \\boldsymbol{\\omega}^\\top - \\|\\boldsymbol{\\omega}\\|^2 \\mathbf{I}$. Therefore, the Hessian for the rotation is\n$$ \\mathbf{H}_{\\boldsymbol{\\omega}} = \\sum_i \\left( \\|\\mathbf{p}_i\\|^2 \\mathbf{I} - \\mathbf{R}_0 \\mathbf{p}_i (\\mathbf{R}_0 \\mathbf{p}_i)^\\top \\right). $$Let $\\mathbf{x}_i = \\mathbf{R}_0 \\mathbf{p}_i$, then the Hessian for the rotation is\n$$ \\mathbf{H}_{\\boldsymbol{\\omega}} = \\sum_i \\left( \\|\\mathbf{x}_i\\|^2 \\mathbf{I} - \\mathbf{x}_i \\mathbf{x}_i^\\top \\right). $$First, we should note that the Hessian is semi-positive definite by definition. A “good” Hessian should have\nThe smallest eigenvalue is large The condition number is close to 1 Note also that $\\sum_i \\|\\mathbf{x}_i\\|^2 \\mathbf{I} - \\sum_i \\mathbf{x}_i \\mathbf{x}_i^\\top$ is the null space of the space spanned by $\\{\\mathbf{x}_i\\}$. To satisfy the first condition, we need at least three points that not collinear or in the same plane. The second condition inspires us to distribute the points as evenly as possible.\nPoint-to-Plane ICP Cost Function For point-to-plane ICP, the cost function is:\n$$ f = \\frac{1}{2} \\sum_i \\left( \\mathbf{n}_i^\\top (\\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i) \\right)^2 $$where $\\mathbf{n}_i$ is the surface normal at $\\mathbf{q}_i$. The residual is:\n$$ \\mathbf{r}_i = \\mathbf{n}_i^T (\\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i) $$For small rotations $\\exp([\\boldsymbol{\\omega}]_\\times)$ around $\\mathbf{R}_0$, $\\mathbf{R} \\mathbf{p}_i \\approx \\mathbf{R}_0 \\mathbf{p}_i + [\\boldsymbol{\\omega}]_\\times \\mathbf{R}_0 \\mathbf{p}_i$, so:\n$$ \\mathbf{r}_i \\approx \\mathbf{n}_i^T (\\mathbf{R}_0 \\mathbf{p}_i + [\\boldsymbol{\\omega}]_\\times \\mathbf{R}_0 \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i) $$The Jacobian is:\n$$ \\mathbf{J}_i = \\begin{bmatrix} -\\mathbf{n}_i^\\top[\\mathbf{R}_0\\mathbf{p}_i]_\\times \u0026 \\mathbf{n}_i^T \\end{bmatrix} $$Let $\\mathbf{x}_i = \\mathbf{R}_0 \\mathbf{p}_i$, then the Hessian is:\n$$ \\begin{aligned} \\mathbf{H} \u0026= \\sum_i \\mathbf{J}_i^T \\mathbf{J}_i \\\\ \u0026= \\begin{bmatrix} \\sum_i [\\mathbf{x}_i]_\\times^T \\mathbf{n}_i \\mathbf{n}_i^T [\\mathbf{x}_i]_\\times \u0026 -\\sum_i [\\mathbf{x}_i]_\\times^T \\mathbf{n}_i \\mathbf{n}_i^\\top\\\\ -\\sum_i \\mathbf{n}_i\\mathbf{n}_i^\\top[\\mathbf{x}_i]_\\times \u0026 \\sum_i \\mathbf{n}_i \\mathbf{n}_i^\\top \\end{bmatrix} \\end{aligned} $$For the translation part, $\\mathbf{H}_{\\mathbf{t}} = \\sum_i \\mathbf{n}_i \\mathbf{n}_i^\\top$, remember the length of the normal vector is 1, i.e., $\\|\\mathbf{n}_i\\| = 1$, and $\\sum_i \\mathbf{n}_i \\mathbf{n}_i^\\top$ is the space spanned by $\\{\\mathbf{n}_i\\}$. Thus, a good Hessian indicates that the normal vectors should spatial evenly distributed.\nFor the rotation part, $\\mathbf{H}_{\\boldsymbol{\\omega}} = \\sum_i [\\mathbf{x}_i]_\\times^T \\mathbf{n}_i \\mathbf{n}_i^T [\\mathbf{x}_i]_\\times$. Let $\\boldsymbol{\\nu}_i = [\\mathbf{x}_i]_\\times\\mathbf{n}_i = \\mathbf{x}_i \\times \\mathbf{n}_i$, we have\n$$ \\mathbf{H}_{\\boldsymbol{\\omega}} = \\sum_i \\boldsymbol{\\nu}_i\\boldsymbol{\\nu}_i^\\top. $$This Hessian is the space spanned by $\\{\\boldsymbol{\\nu}_i\\}$. Since $\\boldsymbol{\\nu}_i$ is the cross product of $\\mathbf{x}_i$ and $\\mathbf{n}_i$, it is perpendicular to both $\\mathbf{x}_i$ and $\\mathbf{n}_i$. Thus, a good Hessian requires the normal of the plane spanned by pairs of $\\mathbf{x}_i$ and $\\mathbf{n}_i$ are evenly distributed. It is also intuitive that the points along the normal direction contribute no information to the rotation.\nPoint-to-Line ICP Cost Function For point-to-line ICP, the cost function is:\n$$ f = \\frac{1}{2} \\sum_i \\left\\| (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top)(\\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i) \\right\\|^2 $$where $\\mathbf{n}_i$ is the direction unit vector of the line. The residual is:\n$$ \\mathbf{r}_i = (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top)(\\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i) $$Follow similar steps as before, the Jacobian is:\n$$ \\mathbf{J}_i = \\begin{bmatrix} -(\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top)[\\mathbf{R}_0\\mathbf{p}_i]_\\times \u0026 \\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top \\end{bmatrix} $$Let $\\mathbf{x}_i = \\mathbf{R}_0 \\mathbf{p}_i$, then the Hessian is:\n$$ \\begin{aligned} \\mathbf{H} \u0026= \\sum_i \\mathbf{J}_i^T \\mathbf{J}_i \\\\ \u0026= \\begin{bmatrix} \\sum_i [\\mathbf{x}_i]_\\times^T (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top) [\\mathbf{x}_i]_\\times \u0026 \\sum_i [\\mathbf{x}_i]_\\times^T (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top) \\\\ \\sum_i (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top) [\\mathbf{x}_i]_\\times \u0026 \\sum_i (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top) \\end{bmatrix} \\end{aligned} $$This problem is like the dual of the point-to-plane ICP, where $\\mathbf{I}-\\mathbf{n}_i\\mathbf{n}_i^\\top$ is the projection matrix onto the plane orthogonal to $\\mathbf{n}_i$. So, for the translation part, a good Hessian requires of the normal vectors are evenly distributed in the unit sphere.\nFor the rotation part, the Hessian is:\n$$ \\begin{aligned} \\mathbf{H}_{\\boldsymbol{\\omega}} \u0026= \\sum_i [\\mathbf{x}_i]_\\times^T (\\mathbf{I} - \\mathbf{n}_i\\mathbf{n}_i^\\top) [\\mathbf{x}_i]_\\times \\\\ \u0026= \\sum_i \\left( [\\mathbf{x}_i]_\\times^T [\\mathbf{x}_i]_\\times - [\\mathbf{x}_i]_\\times^T \\mathbf{n}_i \\mathbf{n}_i^\\top[\\mathbf{x}_i]_\\times \\right)\\\\ \u0026=\\sum_i \\left( \\|\\mathbf{x}_i\\|^2 \\mathbf{I} - \\mathbf{x}_i \\mathbf{x}_i^\\top - \\mathbf{u}_i\\mathbf{u}_i^\\top \\right)\\\\ \u0026=\\sum_i \\|\\mathbf{x}_i\\|^2 \\left( \\mathbf{I} - \\mathbf{v}_i \\mathbf{v}_i^\\top - (\\mathbf{v_i\\times \\mathbf{n}_i})(\\mathbf{v_i\\times \\mathbf{n}_i})^\\top \\right) \\end{aligned} $$where $\\mathbf{u}_i = \\mathbf{x}_i \\times \\mathbf{n}_i$ is the cross product of $\\mathbf{x}_i$ and $\\mathbf{n}_i$ and $\\mathbf{v}_i = \\frac{\\mathbf{x}_i}{\\|\\mathbf{x}_i\\|}$ is the unit vector of $\\mathbf{x}_i$. Then, a good Hessian requires that\nThe length of $\\mathbf{x}_i$ is large The length of $\\mathbf{x}_i$ is similar $\\mathbf{x}_i$ is spatially evenly distributed The length of $\\mathbf{u}_i$ is similar The normal vectors of the planes spanned by $\\mathbf{x}_i$ and $\\mathbf{n}_i$ are evenly distributed, then $\\mathbf{n}_i$ is spatially evenly distributed References [1] K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-Squares Fitting of Two 3-D Point Sets,” IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698–700, Sep. 1987, doi: 10.1109/TPAMI.1987.4767965.\n[2] Y. Zheng, Y. Kuang, S. Sugimoto, K. Astrom, and M. Okutomi, “Revisiting the PnP Problem: A Fast, General and Optimal Solution,” in 2013 IEEE International Conference on Computer Vision, Sydney, Australia: IEEE, Dec. 2013, pp. 2344–2351. doi: 10.1109/ICCV.2013.291.\n[3] G. Terzakis and M. Lourakis, “A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem,” in Computer Vision – ECCV 2020, vol. 12346, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds., in Lecture Notes in Computer Science, vol. 12346. , Cham: Springer International Publishing, 2020, pp. 478–494. doi: 10.1007/978-3-030-58452-8_28.\n[4] Y. Cai, W. Xu, and F. Zhang, “ikd-Tree: An Incremental K-D Tree for Robotic Applications,” arXiv preprint arXiv:2102.10808, 2021.\n[5] Y. Chen and G. Medioni, “Object modelling by registration of multiple range images,” Image and Vision Computing, vol. 10, no. 3, pp. 145–155, Apr. 1992, doi: 10.1016/0262-8856(92)90066-C\n[6] Segal, Aleksandr, Dirk Haehnel, and Sebastian Thrun. “Generalized-icp.” Robotics: science and systems. Vol. 2. No. 4. 2009.\n[7] P. Biber and W. Strasser, “The normal distributions transform: a new approach to laser scan matching,” in Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453), Las Vegas, Nevada, USA: IEEE, 2003, pp. 2743–2748. doi: 10.1109/IROS.2003.1249285.\n[8] K. Koide, M. Yokozuka, S. Oishi, and A. Banno, “Voxelized GICP for Fast and Accurate 3D Point Cloud Registration,” in 2021 IEEE International Conference on Robotics and Automation (ICRA), Xi’an, China: IEEE, May 2021, pp. 11054–11059. doi: 10.1109/ICRA48506.2021.9560835.\n[9] J. Zhang, Y. Yao, and B. Deng, “Fast and robust iterative closest point,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3450–3466, Jul. 2022, doi: 10.1109/TPAMI.2021.3054619.\n[10] S. Choi, Q.-Y. Zhou, and V. Koltun, “Robust reconstruction of indoor scenes,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 2015, pp. 5556–5565, doi: 10.1109/CVPR.2015.7299195.\n[11] M. Barczyk and S. Bonnabel, “Observability, covariance and uncertainty of ICP-based scan-matching,” 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Chicago, IL, USA, 2014, pp. 1760–1765, doi: 10.1109/IROS.2014.6942762.\n[12] S. Bonnabel, M. Barczyk and F. Goulette, “On the covariance of ICP-based scan-matching techniques,” 2016 American Control Conference (ACC), Boston, MA, USA, 2016, pp. 5498-5503, doi: 10.1109/ACC.2016.7526532.\n","wordCount":"3968","inLanguage":"en","image":"https://livey.github.io/posts/2024-12-icp/%3Cimage%20path/url%3E","datePublished":"2024-12-26T00:00:00Z","dateModified":"2024-12-26T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2024-12-icp/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Iterative Closest Point Uncovered: Mathematical Foundations and Applications</h1><div class=post-description>A detailed derivation of the Iterative Closest Point (ICP) problem.</div><div class=post-meta><span title='2024-12-26 00:00:00 +0000 UTC'>December 26, 2024</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;3968 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem-formulation aria-label="Problem Formulation">Problem Formulation</a></li><li><a href=#iterative-solution aria-label="Iterative Solution">Iterative Solution</a><ul><li><a href=#solving-the-problem-with-fixed-pose aria-label="Solving the Problem with Fixed Pose">Solving the Problem with Fixed Pose</a></li><li><a href=#solving-the-problem-with-known-point-correspondence aria-label="Solving the Problem with Known Point Correspondence">Solving the Problem with Known Point Correspondence</a></li></ul></li><li><a href=#other-variants-of-icp aria-label="Other Variants of ICP">Other Variants of ICP</a><ul><li><a href=#robust-point-to-point-icp aria-label="Robust Point-to-point ICP">Robust Point-to-point ICP</a></li><li><a href=#point-to-plane-icp aria-label="Point to Plane ICP">Point to Plane ICP</a></li><li><a href=#point-to-line-icp aria-label="Point-to-Line ICP">Point-to-Line ICP</a></li><li><a href=#generalized-icp aria-label="Generalized ICP">Generalized ICP</a></li><li><a href=#plane-to-plane-icp aria-label="Plane to Plane ICP">Plane to Plane ICP</a></li><li><a href=#normal-distributions-transform aria-label="Normal Distributions Transform">Normal Distributions Transform</a></li><li><a href=#voxelized-gicp aria-label="Voxelized GICP">Voxelized GICP</a></li></ul></li><li><a href=#performance-metrics aria-label="Performance Metrics">Performance Metrics</a><ul><li><a href=#information-matrix aria-label="Information Matrix">Information Matrix</a></li><li><a href=#fitness-score aria-label="Fitness Score">Fitness Score</a></li><li><a href=#inlier-rmse aria-label="Inlier RMSE">Inlier RMSE</a></li><li><a href=#transformation-errors aria-label="Transformation Errors">Transformation Errors</a></li></ul></li><li><a href=#apppendix aria-label=Apppendix>Apppendix</a><ul><li><a href=#rotation-error-derivation aria-label="Rotation Error Derivation">Rotation Error Derivation</a><ul><li><a href=#distance-on-so3 aria-label="Distance on SO(3)">Distance on SO(3)</a></li><li><a href=#relating-the-angle-theta-to-the-trace aria-label="Relating the Angle $\theta$ to the Trace">Relating the Angle $\theta$ to the Trace</a></li><li><a href=#useful-properties aria-label="Useful Properties">Useful Properties</a></li></ul></li><li><a href=#information-matrix-derivation aria-label="Information Matrix Derivation">Information Matrix Derivation</a><ul><li><a href=#definition-of-the-information-matrix aria-label="Definition of the Information Matrix">Definition of the Information Matrix</a></li><li><a href=#theoretical-foundation aria-label="Theoretical Foundation">Theoretical Foundation</a></li><li><a href=#point-to-point-icp-cost-function aria-label="Point-to-Point ICP Cost Function">Point-to-Point ICP Cost Function</a></li><li><a href=#point-to-plane-icp-cost-function aria-label="Point-to-Plane ICP Cost Function">Point-to-Plane ICP Cost Function</a></li><li><a href=#point-to-line-icp-cost-function aria-label="Point-to-Line ICP Cost Function">Point-to-Line ICP Cost Function</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In this post, we will discuss the Iterative Closest Point (ICP) problem: from point-to-point and point-to-plane ICP to generalized ICP.</p><h1 id=problem-formulation>Problem Formulation<a hidden class=anchor aria-hidden=true href=#problem-formulation>#</a></h1><p>Let two 3D point-sets $\mathcal{X} = \{\mathbf{x}_i\}, i = 1, \ldots, N$ and $\mathcal{Y} = \{\mathbf{y}_j\}, j = 1, \ldots, M$, where $\mathbf{x}_i, \mathbf{y}_j \in \mathbb{R}^3$ are point coordinates, be the data point-set and the model point-set respectively. The goal is to estimate a rigid motion with rotation $\mathbf{R} \in SO(3)$ and translation $\mathbf{t} \in \mathbb{R}^3$ that minimizes the following $L_2$-error $E$:</p>$$\underset{\mathbf{R}, \mathbf{t}}{\arg\min E(\mathbf{R}}, \mathbf{t}) = \sum_{i=1}^N e_i(\mathbf{R}, \mathbf{t})^2 = \sum_{i=1}^N \left\| \mathbf{R} \mathbf{x}_i + \mathbf{t} - \mathbf{y}_{j^*} \right\|^2 \tag{1}$$<p>where $e_i(\mathbf{R}, \mathbf{t})$ is the per-point residual error for $x_i$. Given $\mathbf{R}$ and $\mathbf{t}$, the point $y_{j^*} \in \mathcal{Y}$ is denoted as the optimal correspondence of $x_i$, which is the closest point to the transformed $x_i$ in $\mathcal{Y}$, i.e.,</p>$$j^* = \underset{j \in \{1, \ldots, M\}}{\arg \min} \left\| \mathbf{R} x_i + \mathbf{t} - \mathbf{y}_j \right\|\tag{2}$$<p>Note the short-hand notation used here: $j^*$ varies as a function of $(\mathbf{R}, \mathbf{t})$ and also depends on $x_i$.</p><p>Iterative closest point algorithm solves problem (1) by iteratively solving problem (1) and (2).</p><h1 id=iterative-solution>Iterative Solution<a hidden class=anchor aria-hidden=true href=#iterative-solution>#</a></h1><h2 id=solving-the-problem-with-fixed-pose>Solving the Problem with Fixed Pose<a hidden class=anchor aria-hidden=true href=#solving-the-problem-with-fixed-pose>#</a></h2><p>With fixed pose, problem (2) can be solved with a computational complexity of $\mathcal{O}(NM)$. However, we can build an octree to accelerate the closest point search. Further, if the data points are received sequentially, we can use an incremental kd-tree to update the tree [4].</p><h2 id=solving-the-problem-with-known-point-correspondence>Solving the Problem with Known Point Correspondence<a hidden class=anchor aria-hidden=true href=#solving-the-problem-with-known-point-correspondence>#</a></h2><p>With known point correspondence, problem (1) can be solved analytically.</p>$$\min_{\mathbf{R}\in SO(3), \mathbf{t}}\|\mathbf{R}\mathbf{x}_i+t-\mathbf{y}_i\|_2^2$$<p>For any $\mathbf{R}$, taking the derivative of the objective with respect to $\mathbf{t}$ and letting it equal to zero we have</p>$$\mathbf{t} = \frac{1}{N}\sum_i(\mathbf{y}_i - \mathbf{R}\mathbf{x}_i) = \bar{\mathbf{y}} - \mathbf{R}\bar{\mathbf{x}}$$<p>where we have $\bar{\mathbf{y}}\triangleq\frac{1}{N}\sum_i \mathbf{y}_i$ and $\bar{\mathbf{x}}\triangleq\frac{1}{N}\sum_i \mathbf{x}_i$. So, plug them into the objective function and let $\mathbf{x}_i\triangleq \mathbf{x}_i-\bar{\mathbf{x}}$ and $\mathbf{y}_i\triangleq \mathbf{y}_i - \bar{\mathbf{y}}$, we have</p>$$\min_{\mathbf{R}\in SO(3)} \|\mathbf{R}\mathbf{x}_i - \mathbf{y}_i\|_2^2$$<p>It is equivalent to</p>$$\max_{\mathbf{R}\in SO(3)}\,\mathbf{y}_i^\top\mathbf{R}\mathbf{x}_i$$<p>and further we have</p>$$\max_{\mathbf{R}\in SO(3)}\text{tr}(\mathbf{R}\sum_i \mathbf{x}_i\mathbf{y}_i^\top)$$<p>Let $\mathbf{M} = \sum_i \mathbf{x}_i\mathbf{y}_i^\top$. We are thus solving:</p>$$\max_{\mathbf{R}\in SO(3)}\text{tr}(\mathbf{R}\mathbf{M})$$<p>Let the SVD of $\mathbf{M} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$. Then $\text{tr}(\mathbf{R}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top) = \text{tr}(\mathbf{V}^\top\mathbf{R}\mathbf{U}\mathbf{\Sigma}) \triangleq \text{tr}(\mathbf{Q}\mathbf{\Sigma})$, where $\mathbf{Q}=\mathbf{V}^\top\mathbf{R}\mathbf{U}$ is an orthonormal matrix.</p>$$\text{tr}(\mathbf{Q}\mathbf{\Sigma}) = \sum_i q_{i,i}\sigma_{i}\leq \sum_i \sigma_{i}$$<p>The last equality holds when $q_{i,i}=1$ for all $i$ and $\sigma_{i,i}>0$, which means $\mathbf{Q}=\mathbf{I}$. Therefore, we have $\mathbf{R}=\mathbf{V}\mathbf{U}^\top$.</p><p>Since $SO(3)$ puts a determinant constraint on the orthonormal matrix with $\text{det}(R)=1$, we have two cases:</p><ol><li>If $\text{det}(\mathbf{R})=1$, the optimal solution is $\mathbf{R}=\mathbf{V}\mathbf{U}^\top$.</li><li>If $\text{det}(\mathbf{R})=-1$, the algorithm fails.</li></ol><p>In the case where $\text{det}(\mathbf{R})=-1$ and there exists at least one $\sigma_i$ that is zero, we can multiply $-1$ to its corresponding column of $\mathbf{U}$ or $\mathbf{V}$.</p><p>In the case where $\text{det}(\mathbf{R})=-1$ and all $\sigma_i$&rsquo;s are positive, &ldquo;This can happen only when the noises are very large. In that case, the least-squares solution is probably useless anyway. A better approach would be to use a RANSAC-like technique (using 3 points at a time) to combat against outliers&rdquo; [1].</p><h1 id=other-variants-of-icp>Other Variants of ICP<a hidden class=anchor aria-hidden=true href=#other-variants-of-icp>#</a></h1><p>Let us first write the cost of problem (1) in a more general form:</p>$$
\underset{\mathbf{T}}{\text{argmin}}
\sum_{i=1}^N \ell( \mathbf{T}\mathbf{x}_i, \mathbf{y}_{j^*})
$$<p>where $\mathbf{T} \in SE(3)$ and we have written the points in its homogeneous form.</p><h2 id=robust-point-to-point-icp>Robust Point-to-point ICP<a hidden class=anchor aria-hidden=true href=#robust-point-to-point-icp>#</a></h2><p>When there are outliers in the dataset or not every point has a corresponding correspondence, we can use a robust loss function $\ell$ to handle them. Typical choices are</p>$$
\ell(\mathbf{T}\mathbf{x}, \mathbf{y}) = w\|\mathbf{T}\mathbf{x}-\mathbf{y}\|_2^2
$$<p>where</p>$$
w =
\begin{cases}
1, & \text{if } \|\mathbf{T}\mathbf{x}-\mathbf{y}\|_2 < d_{max} \\
0, & \text{otherwise}
\end{cases}
$$<p>which means when the distance is larger than $d_{max}$, the point is considered an outlier and ignored.</p><h2 id=point-to-plane-icp>Point to Plane ICP<a hidden class=anchor aria-hidden=true href=#point-to-plane-icp>#</a></h2><p>The point-to-plane ICP is used as a more robust and accurate variant of the standard ICP. It minimizes the distance between the point and the plane [5]. The corresponding cost function is</p>$$
\ell(\mathbf{T}\mathbf{x}, \mathbf{y}) = \|w\cdot\mathbf{n}^\top(\mathbf{T}\mathbf{x}-\mathbf{y})\|_2^2
$$<p>where $\mathbf{n}$ is the normal vector of the plane where the point $\mathbf{y}$ is located.</p><h2 id=point-to-line-icp>Point-to-Line ICP<a hidden class=anchor aria-hidden=true href=#point-to-line-icp>#</a></h2><p>This variant is used when we know the target feature is a line. So, the cost function is the distance between the point and the line.</p>$$
\ell(\mathbf{T}\mathbf{x}, \mathbf{y}) = \|w\cdot(\mathbf{I} - \mathbf{n}\mathbf{n}^\top)(\mathbf{T}\mathbf{x}-\mathbf{y})\|_2^2
$$<p>where $\mathbf{n}$ is the unit direction vector of the line in which the point $\mathbf{y}$ is located.</p><h2 id=generalized-icp>Generalized ICP<a hidden class=anchor aria-hidden=true href=#generalized-icp>#</a></h2><p>Assume points $\{\mathbf{x}_i\}_{i=1}^N$ and $\{\mathbf{y}_i\}_{i=1}^N$ are already aligned, $\mathbf{x}_i\sim \mathcal{N}(\bar{\mathbf{x}}_i, \mathbf{C}^x_i)$ and $\mathbf{y}_i\sim \mathcal{N}(\bar{\mathbf{y}}_i, \mathbf{C}^y_i)$. Further, assume $\mathbf{y}_i = \mathbf{T}\mathbf{x}_i$. Thus, the residual, $\mathbf{d}^T_i = \mathbf{y}_i - \mathbf{T}\mathbf{x}_i$, is distributed as $\mathcal{N}(0, \mathbf{C}^y_i + \mathbf{T}\mathbf{C}^x_i\mathbf{T}^\top)$.
Using the maximum log likelihood estimation, we have [6]</p>$$
\underset{\mathbf{T}}{\text{argmin}} \sum_{i=1}^N (\mathbf{d}_i^T)^\top\left(\mathbf{C}^y_i + \mathbf{T}\mathbf{C}^x_i\mathbf{T}^\top\right)^{-1}\mathbf{d}_i^T. \tag{3}
$$<p>If we set $\mathbf{C}_i^x = \mathbf{0}$ and $\mathbf{C}_i^y =\mathbf{I}$, problem (3) is equivalent to the standard point-to-point ICP.</p><p>If we set $\mathbf{C}_i^x =\mathbf{0}$ and $\mathbf{C}_i^y = \mathbf{P}_i^{-1}$, where $\mathbf{P}_i$ is the projection onto the space spanned by the plane normal. Then, problem (3) is equivalent to the point-to-plane ICP. Also, we can construct the point-to-line ICP from the generalized ICP problem formulation.</p><h2 id=plane-to-plane-icp>Plane to Plane ICP<a hidden class=anchor aria-hidden=true href=#plane-to-plane-icp>#</a></h2><p>If the points are generated from a plane, we can assume that the points are generated from the distribution $\mathcal{N}(\bar{\mathbf{x}}, \mathbf{\Sigma})$, where</p>$$
\mathbf{\Sigma} =
\mathbf{U}
\begin{bmatrix}
\mathbf{s_1} & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{s_2} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{\epsilon}
\end{bmatrix}
\mathbf{U}^\top,
$$<p>where $\epsilon$ should be a positive number near zero. By applying PCA on the point and along with its near by points, we can find such covariance matrix. Then, with the estimated covariance matrix, we can plug it into problem (3) to solve the generalized ICP problem, which is also called as the plane-to-plane ICP in [6]. In real practice, we should force $\mathbf{\Sigma}=\text{diag}(1, 1, \epsilon)$ to mitigate the effect of non uniformly distributed LiDAR points.</p><h2 id=normal-distributions-transform>Normal Distributions Transform<a hidden class=anchor aria-hidden=true href=#normal-distributions-transform>#</a></h2><p>The Normal Distributions Transform (NDT) method [7], published before Generalized ICP [6], offers an alternative approach to scan matching.</p><p>The NDT models the distribution of all reconstructed 2D-Points of one laser scan by a collection of local normal distributions. First, the 2D space around the robot is subdivided regularly into cells with constant size. Then for each cell, that contains at least three points, the following is done:</p><ol><li>Collect all 2D-Points $\mathbf{x}_i$, $i = 1..n$, contained in this box.</li><li>Calculate the mean $\mathbf{q} = \frac{1}{n} \sum_i \mathbf{x}_i$.</li><li>Calculate the covariance matrix</li></ol>$$
\Sigma = \frac{1}{n} \sum_i (\mathbf{x}_i - \mathbf{q})(\mathbf{x}_i - \mathbf{q})^t.
$$<p>The probability of measuring a sample at 2D-point $\mathbf{x}$ contained in this cell is now modeled by the normal distribution $N(\mathbf{q}, \Sigma)$:</p>$$
p(\mathbf{x}) \sim \exp\left( -\frac{(\mathbf{x} - \mathbf{q})^t \Sigma^{-1} (\mathbf{x} - \mathbf{q})}{2} \right).
$$<p>The outline of the proposed approach, given two scans (the first one and the second one), is as follows:</p><ol><li>Build the NDT of the first scan.</li><li>Initialize the estimate for the parameters (by zero or by using odometry data).</li><li>For each sample of the second scan: Map the reconstructed 2D point into the coordinate frame of the first scan according to the parameters.</li><li>Determine the corresponding normal distributions for each mapped point.</li><li>The score for the parameters is determined by evaluating the distribution for each mapped point and summing the result.</li><li>Calculate a new parameter estimate by trying to optimize the score. This is done by performing one step of Newton&rsquo;s Algorithm.</li><li>Go to step 3 until a convergence criterion is met.</li></ol><p>The first four steps are straightforward: Building the NDT was described in the last section. As noted above, odometry data could be used to initialize the estimate. Mapping the second scan is done using $T$ and finding the corresponding normal distribution is a simple lookup in the grid of the NDT.</p><p>The rest is now described in detail using the following notation:</p><ul><li>$\mathbf{p} = (p_i)_{i=1..3} = (t_x, t_y, \phi)^t$: The vector of the parameters to estimate.</li><li>$\mathbf{x}_i$: The reconstructed 2D point of laser scan sample $i$ of the second scan in the coordinate frame of the second scan.</li><li>$\mathbf{x}_i'$: The point $\mathbf{x}_i$ mapped into the coordinate frame of the first scan according to the parameters $\mathbf{p}$, that is $\mathbf{x}_i' = T(\mathbf{x}_i, \mathbf{p})$.</li><li>$\Sigma_i$, $\mathbf{q}_i$: The covariance matrix and the mean of the corresponding normal distribution to point $\mathbf{x}_i'$, looked up in the NDT of the first scan.</li></ul><p>The mapping according to $\mathbf{p}$ could be considered optimal if the sum evaluating the normal distributions of all points $\mathbf{x}_i'$ with parameters $\Sigma_i$ and $\mathbf{q}_i$ is a maximum. We call this sum the <strong>score</strong> of $\mathbf{p}$. It is defined as:</p>$$
\text{score}(\mathbf{p}) = \sum_i \exp\left( -\frac{(\mathbf{x}_i' - \mathbf{q}_i)^t \Sigma_i^{-1} (\mathbf{x}_i' - \mathbf{q}_i)}{2} \right).
$$<p>Finally, Newton&rsquo;s method is applied iteratively to find the optimal parameters.</p><h2 id=voxelized-gicp>Voxelized GICP<a hidden class=anchor aria-hidden=true href=#voxelized-gicp>#</a></h2><p>Voxelized GICP [8] is built upon GICP [6]. It employs a point to multi-voxel mapping so that it eliminates the nearest neighbor search in the GICP. Then, it can efficiently compute the loss function by parallelizing the computation.</p><p><img alt="ICP Illustration" loading=lazy src=/posts/2024-12-icp/resources/image.png></p><p>The following notation is similar to the one used in Eq. (3).</p><p>To derive the voxelized GICP algorithm, we first extend $\tilde{\mathbf{d}}_i$ so that it calculates the distances between $\mathbf{a}_i$ and its neighbor points $\{\mathbf{b}_j \mid \|\mathbf{a}_i - \mathbf{b}_j\| < r\}$ as follows:</p>$$
\tilde{\mathbf{d}}_i = \sum_j \left( \hat{\mathbf{b}}_j - \mathbf{T} \hat{\mathbf{a}}_i \right).
$$<p>This equation can be interpreted as smoothing the target point distributions. Then, the distribution of $\tilde{\mathbf{d}}_i$ is given by</p>$$
\tilde{\mathbf{d}}_i \sim \left( \boldsymbol{\mu}^{\tilde{d}_i}, \mathbf{C}^{\tilde{d}_i} \right),
$$$$
\mathbf{\mu}^{\tilde{d}_i} = \sum_j \left( \hat{\mathbf{b}}_j - \mathbf{T} \hat{\mathbf{a}}_i \right) = \mathbf{0},
$$$$
\mathbf{C}^{\tilde{d}_i} = \sum_j \left( \mathbf{C}^B_j + \mathbf{T} \mathbf{C}^A_i \mathbf{T}^T \right).
$$<p>We estimate the transformation $\mathbf{T}$ that maximizes the log-likelihood of as follows:</p>$$
\mathbf{T} = \arg\min_\mathbf{T} \sum_i \left( \tilde{\mathbf{d}}_i^T \tilde{\mathbf{C}}_i^{-1} \tilde{\mathbf{d}}_i \right),
$$<p>where</p>$$
\tilde{\mathbf{d}}_i = \sum_j \left( \mathbf{b}_j - \mathbf{T} \mathbf{a}_i \right),
$$$$
\tilde{\mathbf{C}}_i = \sum_j \left( \mathbf{C}^B_j + \mathbf{T} \mathbf{C}^A_i \mathbf{T}^T \right).
$$<p>To efficiently calculate the above equation, we modify it to:</p>$$
\mathbf{T} = \arg\min_\mathbf{T} \sum_i \left( N_i \tilde{\mathbf{d}}_i^T \tilde{\mathbf{C}}_i^{-1} \tilde{\mathbf{d}}_i \right),
$$<p>where:</p>$$
\tilde{\mathbf{d}}_i = \frac{\sum_j \mathbf{b}_j}{N_i} - \mathbf{T} \mathbf{a}_i,
$$$$
\tilde{\mathbf{C}}_i = \frac{\sum_j \mathbf{C}^B_j}{N_i} + \mathbf{T} \mathbf{C}^A_i \mathbf{T}^T.
$$<p>where $N_i$ is the number of neighbor points. It suggests that we can efficiently compute the objective function by substituting the mean of the distributions of the points ($\mathbf{b}_j$ and $\mathbf{C}^B_j$) around $\mathbf{a}_i$ for $\mathbf{b}_i$ and $\mathbf{C}^B_i$ and weighting the function by $N_i$.</p><p>We can naturally adapt this equation to voxel-based calculation by storing $\mathbf{b}'_i = \frac{\sum \mathbf{b}_j}{N_i}$ and $\mathbf{C}'_i = \frac{\sum \mathbf{C}^B_j}{N_i}$ in each voxel.</p><p>Following the log-likelihood function, it uses Gauss-Newton method to optimize the objective function.</p><h1 id=performance-metrics>Performance Metrics<a hidden class=anchor aria-hidden=true href=#performance-metrics>#</a></h1><h2 id=information-matrix>Information Matrix<a hidden class=anchor aria-hidden=true href=#information-matrix>#</a></h2><p>The information matrix in the context of ICP is typically the Hessian matrix of the ICP cost function, which quantifies the curvature of the cost function at its minimum. This matrix, often a 6x6 matrix in 3D (corresponding to three translational and three rotational degrees of freedom), represents the precision or reliability of the estimated transformation parameters. In optimization, the Hessian is related to the Fisher information matrix, and its inverse provides the covariance matrix, indicating the uncertainty in the transformation estimate.</p><p>The information matrix is used to assess the reliability of the ICP alignment by providing an estimate of the uncertainty in the transformation parameters. A well-conditioned information matrix (with large diagonal elements) indicates high confidence in the transformation, suggesting that the point clouds are well-aligned with sufficient geometric constraints. Conversely, a poorly conditioned matrix (e.g., with small eigenvalues) suggests high uncertainty, possibly due to insufficient overlap, flat surfaces, or noisy data.</p><h2 id=fitness-score>Fitness Score<a hidden class=anchor aria-hidden=true href=#fitness-score>#</a></h2><p>The fitness score measures the overlap between the source and target point clouds, defined as the ratio of inlier correspondences (points within a maximum correspondence distance) to the total number of points in the target cloud. A higher fitness score suggests better alignment coverage.</p><h2 id=inlier-rmse>Inlier RMSE<a hidden class=anchor aria-hidden=true href=#inlier-rmse>#</a></h2><p>Inlier RMSE is the RMSE calculated only for inlier correspondences, providing a measure of alignment precision for the overlapping regions. It is expressed as:</p>$$
\text{Inlier RMSE} = \sqrt{\frac{1}{N_{\text{inliers}}} \sum_{i \in \text{inliers}} \|\mathbf{p}_i - \mathbf{q}_i\|^2}
$$<p>where $N_{\text{inliers}}$ is the number of inliers. Lower values indicate higher precision.</p><h2 id=transformation-errors>Transformation Errors<a hidden class=anchor aria-hidden=true href=#transformation-errors>#</a></h2><p>When ground truth is available, transformation errors directly measure the accuracy of the estimated rotation and translation. These include:</p><ol><li>Translation Error $e_t$: The Euclidean norm of the difference between estimated and true translation vectors:</li></ol>$$
e_t = \sqrt{\Delta x^2 + \Delta y^2 + \Delta z^2}
$$<ol start=2><li>Rotation Error $e_r$: The geodesic distance between estimated and true rotation matrices:</li></ol>$$
e_r = \arccos\left(\frac{\text{trace}(\Delta \mathbf{R}) - 1}{2}\right)
$$<h1 id=apppendix>Apppendix<a hidden class=anchor aria-hidden=true href=#apppendix>#</a></h1><h2 id=rotation-error-derivation>Rotation Error Derivation<a hidden class=anchor aria-hidden=true href=#rotation-error-derivation>#</a></h2><p>Below is a compact derivation that shows: why the shortest‐path (geodesic) rotation error between two rotation matrices is captured by</p>$$
e_r=\arccos\!\Bigl(\tfrac{\operatorname{tr}(\Delta \mathbf{R})-1}{2}\Bigr),
\qquad
\Delta \mathbf{R}:=\mathbf{R}_{\text{gt}}^{\top}\mathbf{R}_{\text{est}}\in\mathrm{SO}(3).
$$<h3 id=distance-on-so3>Distance on SO(3)<a hidden class=anchor aria-hidden=true href=#distance-on-so3>#</a></h3><ul><li>$SO(3)$ is the Lie group of $3\times3$ rotation matrices.</li><li>A natural, <strong>bi-invariant Riemannian metric</strong> gives a geodesic distance</li></ul>$$
\begin{aligned}
d(\mathbf{R}_1,\mathbf{R}_2)
&=\|\log(\mathbf{R}_1^{\top}\mathbf{R}_2)\|_F/\sqrt{2}\\
&=\|\text{Log}(\mathbf{R}_1^{\top}\mathbf{R}_2)\|_2
\end{aligned}
$$<p>where &ldquo;$\log$&rdquo; is the matrix logarithm sending a rotation to its <strong>axis–angle</strong> element of the Lie algebra $\mathfrak{so}(3)$ and &ldquo;$\text{Log}$&rdquo; is the matrix logarithm sending a rotation to its vector space.</p><p>For $\Delta \mathbf{R}\in\mathrm{SO}(3)$ there exists an axis $\mathbf{u}\in\mathbb{S}^2$ and angle $\theta\in[-\pi,\pi]$ such that</p>$$
\Delta \mathbf{R}=\exp\bigl(\,[\mathbf{u}]_\times\,\theta\bigr)
\quad\Rightarrow\quad
\log(\Delta R)=[\mathbf{u}]_\times\theta,
$$<p>so the Frobenius norm of the log is $\|[\mathbf{u}]_\times\theta\|_F=\sqrt{2}\,|\theta|$.
Hence $d(\mathbf{R}_1,\mathbf{R}_2)=|\theta|$. The distance is exactly the rotation angle $\theta$.</p><h3 id=relating-the-angle-theta-to-the-trace>Relating the Angle $\theta$ to the Trace<a hidden class=anchor aria-hidden=true href=#relating-the-angle-theta-to-the-trace>#</a></h3><p>Start with <strong>Rodrigues&rsquo; formula</strong> for any $\theta$ and unit axis $\mathbf{u}$:</p>$$
\exp\bigl([\mathbf{u}]_\times\theta\bigr)
=I+\sin\theta\,[\mathbf{u}]_\times
+(1-\cos\theta)\,[\mathbf{u}]_\times^{2}.
$$<p>Taking the trace and using $\operatorname{tr}([\mathbf{u}]_\times)=0$ and
$\operatorname{tr}([\mathbf{u}]_\times^{2})=-2$:</p>$$
\operatorname{tr}(\Delta \mathbf{R})=\operatorname{tr}(\mathbf{I})+0+(-2)(1-\cos\theta)
=1+2\cos\theta.
$$<p>Solve for $\theta$:</p>$$
\cos\theta=\tfrac{\operatorname{tr}(\Delta \mathbf{R})-1}{2}
\quad\Longrightarrow\quad
\theta=\arccos\!\Bigl(\tfrac{\operatorname{tr}(\Delta \mathbf{R})-1}{2}\Bigr).
$$<p>Putting it together</p><ul><li>Compute the relative rotation $\Delta \mathbf{R}=\mathbf{R}_{\text{gt}}^{\top}\mathbf{R}_{\text{est}}$ .
(Because the metric is right-invariant, left–multiplying both rotations by the same matrix has no effect.)</li><li>Extract the rotation angle with the trace formula above.</li><li>That angle is the geodesic distance on $SO(3)$, so we call it the rotation error $e_r$.</li></ul><h3 id=useful-properties>Useful Properties<a hidden class=anchor aria-hidden=true href=#useful-properties>#</a></h3><ul><li><strong>Invariance:</strong> $e_r\bigl(\mathbf{Q}\mathbf{R}_{\text{gt}},\,\mathbf{Q}\mathbf{R}_{\text{est}}\bigr)=e_r(\mathbf{R}_{\text{gt}},\mathbf{R}_{\text{est}})$ for any $\mathbf{Q}\in SO(3)$.</li><li><strong>Range:</strong> trace $(\Delta R)\in[-1,3] \Rightarrow e_r\in[0,\pi]$.</li><li><strong>Small-angle limit:</strong> when $\theta\!\ll\!1$, $e_r\approx\|\mathbf{r}\|_2$ where $\mathbf{r}$ is the 3-vector in the Lie-algebra log.</li></ul><p>Thus the trace-based formula is simply a closed-form way of reading off the minimal rotation angle between two orientations, which is exactly the geodesic distance on the manifold of rotations.</p><h2 id=information-matrix-derivation>Information Matrix Derivation<a hidden class=anchor aria-hidden=true href=#information-matrix-derivation>#</a></h2><h3 id=definition-of-the-information-matrix>Definition of the Information Matrix<a hidden class=anchor aria-hidden=true href=#definition-of-the-information-matrix>#</a></h3><p>The information matrix in ICP is typically the Hessian matrix of the cost function, which quantifies the curvature of the cost function at its minimum. This $6 \times 6$ matrix corresponds to the six degrees of freedom in 3D rigid transformations (three for rotation, three for translation). In estimation theory, it is related to the Fisher information matrix, and its inverse provides the covariance matrix, indicating the uncertainty in the transformation estimate. A well-conditioned information matrix (with large eigenvalues) suggests high confidence in the alignment, while a poorly conditioned matrix indicates potential ambiguities, such as in featureless environments. [10]</p><h3 id=theoretical-foundation>Theoretical Foundation<a hidden class=anchor aria-hidden=true href=#theoretical-foundation>#</a></h3><p>The information matrix is derived from the Hessian of the ICP cost function. For least squares problems, the cost function is of the form</p>$$
f = \frac{1}{2} \sum_i \|\mathbf{r}_i(\boldsymbol{\theta})\|^2
$$<p>where $\mathbf{r}_i$ is the residual for each correspondence and $\boldsymbol{\theta}$ is the optimization variable. Take second derivative of the cost function with respect to the optimization variable, we have:</p>$$
\begin{aligned}
\nabla^2_{\boldsymbol{\theta}} f
&= \nabla_{\boldsymbol{\theta}}\left( \sum_i \nabla_{\boldsymbol{\theta}} \mathbf{r}_i \cdot \mathbf{r}_i \right) \\
&= \sum_i \nabla^2_{\boldsymbol{\theta}} \mathbf{r}_i \cdot \mathbf{r}_i + \sum_i \nabla_{\boldsymbol{\theta}} \mathbf{r}_i \nabla_{\boldsymbol{\theta}} \mathbf{r}_i^T
\end{aligned}
$$<p>when $\mathbf{r}_i$ is approximately zero, the Hessian matrix is:</p>$$
\mathbf{H} = \nabla^2_{\boldsymbol{\theta}} f \approx \sum_i \nabla_{\boldsymbol{\theta}} \mathbf{r}_i \nabla_{\boldsymbol{\theta}} \mathbf{r}_i^T
=\mathbf{J}^\top \mathbf{J}
$$<p>where $\mathbf{J}$ is the Jacobian of the residuals with respect to the transformation parameters.</p><p>Note: the Jacobian is defined as $\mathbf{J} = \nabla_{\boldsymbol{\theta}} \mathbf{r}_i^\top$. The shape differs from traditional matrix calculus textbook.</p><p>In ICP, the transformation is parameterized by a 6-vector $\xi = (\boldsymbol{\omega}, \mathbf{t})$, where $\boldsymbol{\omega}$ represents the rotation (often as a rotation vector for small angles) and $\mathbf{t}$ is the translation.</p><h3 id=point-to-point-icp-cost-function>Point-to-Point ICP Cost Function<a hidden class=anchor aria-hidden=true href=#point-to-point-icp-cost-function>#</a></h3><p>For point-to-point ICP, the cost function is:</p>$$
f = \sum_i \| \mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i \|^2
$$<p>where $\mathbf{p}_i$ are source points, $\mathbf{q}_i$ are target points, and $\mathbf{R}$ and $\mathbf{t}$ are the rotation matrix and translation vector. Let us represent the rotation as, $\mathbf{R} = \exp([\boldsymbol{\omega}]_\times) \mathbf{R}_0$ with an already obtained rotation matrix $\mathbf{R}_0$ and a small perturbation, $\boldsymbol{\omega} \in \mathbb{R}^3$, around it, and $[\boldsymbol{\omega}]_\times$ is the skew-symmetric matrix. (Here we can also use right perturbation, $\mathbf{R} = \mathbf{R}_0 \exp([\boldsymbol{\omega}]_\times)$. But for iterative optimization method, right perturbation is preferred. For right perturbation, you can compute $\mathbf{R}_0\mathbf{p}_i$ iteratively. When an updating is made, we can re-use the previous $\mathbf{R}_0\mathbf{p}_i$ to compute the new $\mathbf{R}_0\mathbf{p}_i$. However, for IMU, we use right perturbation $\dot{\mathbf{R}} = \mathbf{R}[\boldsymbol{\omega}]_\times$.) By expanding the matrix exponential and omit the higher order terms, we have, $\mathbf{R} \approx (\mathbf{I} + [\boldsymbol{\omega}]_\times) \mathbf{R}_0$. The residual for each correspondence is:</p>$$
r_i = \mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i \approx \mathbf{R}_0\mathbf{p}_i - \mathbf{q}_i + \mathbf{t} + [\boldsymbol{\omega}]_\times\mathbf{R}_0\mathbf{p}_i
$$<p>Since $[\boldsymbol{\omega}]_\times \mathbf{R}_0 \mathbf{p}_i = \boldsymbol{\omega} \times (\mathbf{R}_0 \mathbf{p}_i) = - \mathbf{R}_0 \mathbf{p}_i \times \boldsymbol{\omega} = - [\mathbf{R}_0 \mathbf{p}_i]_\times \boldsymbol{\omega}$, we have:</p>$$
r_i \approx (\mathbf{R}_0\mathbf{p}_i - \mathbf{q}_i) + \mathbf{t} - [\mathbf{R}_0 \mathbf{p}_i]_\times \boldsymbol{\omega}.
$$<p>So, the Jacobian $\mathbf{J}_i = [\frac{\partial \mathbf{r}_i}{\partial \boldsymbol{\omega}}, \frac{\partial \mathbf{r}_i}{\partial \mathbf{t}}]$ is:</p>$$
\mathbf{J}_i = \begin{bmatrix} -[\mathbf{R}_0 \mathbf{p}_i]_\times & \mathbf{I}_3 \end{bmatrix}
$$<p>Hence, the Hessian is:</p>$$
\begin{aligned}
\mathbf{H} &= \sum_i \mathbf{J}_i^T \mathbf{J}_i \\
&= 
\begin{bmatrix}
\sum_i [\mathbf{R}_0 \mathbf{p}_i]_\times^T [\mathbf{R}_0 \mathbf{p}_i]_\times & \sum_i [\mathbf{R}_0 \mathbf{p}_i]_\times^T \\
\sum_i [\mathbf{R}_0 \mathbf{p}_i]_\times & \sum_i  \mathbf{I}_3
\end{bmatrix}.
\end{aligned}
$$<p>The Hessian with the translation is always constant and the correlation between the rotation and translation is $\sum_i [\mathbf{R}_0 \mathbf{p}_i]_\times$. The Hessian for the rotation is $\sum_i [\mathbf{R}_0 \mathbf{p}_i]_\times^T [\mathbf{R}_0 \mathbf{p}_i]_\times$. Here, we are particularly interested in the rotation part.</p><p>Since $[\boldsymbol{\omega}]_\times^\top = -[\boldsymbol{\omega}]_\times$, and $[\boldsymbol{\omega}]_\times^2 = \boldsymbol{\omega} \boldsymbol{\omega}^\top - \|\boldsymbol{\omega}\|^2 \mathbf{I}$. Therefore, the Hessian for the rotation is</p>$$
\mathbf{H}_{\boldsymbol{\omega}} = \sum_i \left( \|\mathbf{p}_i\|^2 \mathbf{I} - \mathbf{R}_0 \mathbf{p}_i (\mathbf{R}_0 \mathbf{p}_i)^\top \right).
$$<p>Let $\mathbf{x}_i = \mathbf{R}_0 \mathbf{p}_i$, then the Hessian for the rotation is</p>$$
\mathbf{H}_{\boldsymbol{\omega}} = \sum_i \left( \|\mathbf{x}_i\|^2 \mathbf{I} - \mathbf{x}_i \mathbf{x}_i^\top \right).
$$<p>First, we should note that the Hessian is semi-positive definite by definition. A &ldquo;good&rdquo; Hessian should have</p><ol><li>The smallest eigenvalue is large</li><li>The condition number is close to 1</li></ol><p>Note also that $\sum_i \|\mathbf{x}_i\|^2 \mathbf{I} - \sum_i \mathbf{x}_i \mathbf{x}_i^\top$ is the null space of the space spanned by $\{\mathbf{x}_i\}$. To satisfy the first condition, we need at least three points that not collinear or in the same plane. The second condition inspires us to distribute the points as evenly as possible.</p><h3 id=point-to-plane-icp-cost-function>Point-to-Plane ICP Cost Function<a hidden class=anchor aria-hidden=true href=#point-to-plane-icp-cost-function>#</a></h3><p>For point-to-plane ICP, the cost function is:</p>$$
f = \frac{1}{2} \sum_i \left( \mathbf{n}_i^\top (\mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i) \right)^2
$$<p>where $\mathbf{n}_i$ is the surface normal at $\mathbf{q}_i$. The residual is:</p>$$
\mathbf{r}_i = \mathbf{n}_i^T (\mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i)
$$<p>For small rotations $\exp([\boldsymbol{\omega}]_\times)$ around $\mathbf{R}_0$, $\mathbf{R} \mathbf{p}_i \approx \mathbf{R}_0 \mathbf{p}_i + [\boldsymbol{\omega}]_\times \mathbf{R}_0 \mathbf{p}_i$, so:</p>$$
\mathbf{r}_i \approx \mathbf{n}_i^T (\mathbf{R}_0 \mathbf{p}_i + [\boldsymbol{\omega}]_\times \mathbf{R}_0 \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i)
$$<p>The Jacobian is:</p>$$
\mathbf{J}_i =
\begin{bmatrix}
-\mathbf{n}_i^\top[\mathbf{R}_0\mathbf{p}_i]_\times & \mathbf{n}_i^T \end{bmatrix}
$$<p>Let $\mathbf{x}_i = \mathbf{R}_0 \mathbf{p}_i$, then the Hessian is:</p>$$
\begin{aligned}
\mathbf{H} &= \sum_i \mathbf{J}_i^T \mathbf{J}_i \\
&= 
\begin{bmatrix}
\sum_i [\mathbf{x}_i]_\times^T \mathbf{n}_i \mathbf{n}_i^T [\mathbf{x}_i]_\times & -\sum_i [\mathbf{x}_i]_\times^T \mathbf{n}_i \mathbf{n}_i^\top\\
-\sum_i \mathbf{n}_i\mathbf{n}_i^\top[\mathbf{x}_i]_\times & \sum_i \mathbf{n}_i \mathbf{n}_i^\top
\end{bmatrix}
\end{aligned}
$$<p>For the translation part, $\mathbf{H}_{\mathbf{t}} = \sum_i \mathbf{n}_i \mathbf{n}_i^\top$, remember the length of the normal vector is 1, i.e., $\|\mathbf{n}_i\| = 1$, and $\sum_i \mathbf{n}_i \mathbf{n}_i^\top$ is the space spanned by $\{\mathbf{n}_i\}$. Thus, a good Hessian indicates that the normal vectors should spatial evenly distributed.</p><p>For the rotation part, $\mathbf{H}_{\boldsymbol{\omega}} = \sum_i [\mathbf{x}_i]_\times^T \mathbf{n}_i \mathbf{n}_i^T [\mathbf{x}_i]_\times$. Let $\boldsymbol{\nu}_i = [\mathbf{x}_i]_\times\mathbf{n}_i = \mathbf{x}_i \times \mathbf{n}_i$, we have</p>$$
\mathbf{H}_{\boldsymbol{\omega}} = \sum_i \boldsymbol{\nu}_i\boldsymbol{\nu}_i^\top.
$$<p>This Hessian is the space spanned by $\{\boldsymbol{\nu}_i\}$.
Since $\boldsymbol{\nu}_i$ is the cross product of $\mathbf{x}_i$ and $\mathbf{n}_i$, it is perpendicular to both $\mathbf{x}_i$ and $\mathbf{n}_i$. Thus, a good Hessian requires the normal of the plane spanned by pairs of $\mathbf{x}_i$ and $\mathbf{n}_i$ are evenly distributed. It is also intuitive that the points along the normal direction contribute no information to the rotation.</p><h3 id=point-to-line-icp-cost-function>Point-to-Line ICP Cost Function<a hidden class=anchor aria-hidden=true href=#point-to-line-icp-cost-function>#</a></h3><p>For point-to-line ICP, the cost function is:</p>$$
f = \frac{1}{2} \sum_i \left\| (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top)(\mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i) \right\|^2
$$<p>where $\mathbf{n}_i$ is the direction unit vector of the line. The residual is:</p>$$
\mathbf{r}_i = (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top)(\mathbf{R} \mathbf{p}_i + \mathbf{t} - \mathbf{q}_i)
$$<p>Follow similar steps as before, the Jacobian is:</p>$$
\mathbf{J}_i =
\begin{bmatrix}
-(\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top)[\mathbf{R}_0\mathbf{p}_i]_\times & \mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top \end{bmatrix}
$$<p>Let $\mathbf{x}_i = \mathbf{R}_0 \mathbf{p}_i$, then the Hessian is:</p>$$
\begin{aligned}
\mathbf{H} &= \sum_i \mathbf{J}_i^T \mathbf{J}_i \\
&= 
\begin{bmatrix}
\sum_i [\mathbf{x}_i]_\times^T (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top) [\mathbf{x}_i]_\times & \sum_i [\mathbf{x}_i]_\times^T (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top) \\
\sum_i (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top) [\mathbf{x}_i]_\times & \sum_i (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top)
\end{bmatrix}
\end{aligned}
$$<p>This problem is like the dual of the point-to-plane ICP, where $\mathbf{I}-\mathbf{n}_i\mathbf{n}_i^\top$ is the projection matrix onto the plane orthogonal to $\mathbf{n}_i$. So, for the translation part, a good Hessian requires of the normal vectors are evenly distributed in the unit sphere.</p><p>For the rotation part, the Hessian is:</p>$$
\begin{aligned}
\mathbf{H}_{\boldsymbol{\omega}} &= \sum_i [\mathbf{x}_i]_\times^T (\mathbf{I} - \mathbf{n}_i\mathbf{n}_i^\top) [\mathbf{x}_i]_\times \\
&= \sum_i \left( [\mathbf{x}_i]_\times^T [\mathbf{x}_i]_\times - [\mathbf{x}_i]_\times^T \mathbf{n}_i \mathbf{n}_i^\top[\mathbf{x}_i]_\times \right)\\
&=\sum_i \left( \|\mathbf{x}_i\|^2 \mathbf{I} - \mathbf{x}_i \mathbf{x}_i^\top - \mathbf{u}_i\mathbf{u}_i^\top \right)\\
&=\sum_i \|\mathbf{x}_i\|^2 \left( \mathbf{I} - \mathbf{v}_i \mathbf{v}_i^\top - (\mathbf{v_i\times \mathbf{n}_i})(\mathbf{v_i\times \mathbf{n}_i})^\top \right)
\end{aligned}
$$<p>where $\mathbf{u}_i = \mathbf{x}_i \times \mathbf{n}_i$ is the cross product of $\mathbf{x}_i$ and $\mathbf{n}_i$ and $\mathbf{v}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|}$ is the unit vector of $\mathbf{x}_i$. Then, a good Hessian requires that</p><ol><li>The length of $\mathbf{x}_i$ is large</li><li>The length of $\mathbf{x}_i$ is similar</li><li>$\mathbf{x}_i$ is spatially evenly distributed</li><li>The length of $\mathbf{u}_i$ is similar</li><li>The normal vectors of the planes spanned by $\mathbf{x}_i$ and $\mathbf{n}_i$ are evenly distributed, then $\mathbf{n}_i$ is spatially evenly distributed</li></ol><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] K. S. Arun, T. S. Huang, and S. D. Blostein, &ldquo;Least-Squares Fitting of Two 3-D Point Sets,&rdquo; IEEE Trans. Pattern Anal. Mach. Intell., vol. PAMI-9, no. 5, pp. 698–700, Sep. 1987, doi: 10.1109/TPAMI.1987.4767965.</p><p>[2] Y. Zheng, Y. Kuang, S. Sugimoto, K. Astrom, and M. Okutomi, &ldquo;Revisiting the PnP Problem: A Fast, General and Optimal Solution,&rdquo; in 2013 IEEE International Conference on Computer Vision, Sydney, Australia: IEEE, Dec. 2013, pp. 2344–2351. doi: 10.1109/ICCV.2013.291.</p><p>[3] G. Terzakis and M. Lourakis, &ldquo;A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem,&rdquo; in Computer Vision – ECCV 2020, vol. 12346, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds., in Lecture Notes in Computer Science, vol. 12346. , Cham: Springer International Publishing, 2020, pp. 478–494. doi: 10.1007/978-3-030-58452-8_28.</p><p>[4] Y. Cai, W. Xu, and F. Zhang, &ldquo;ikd-Tree: An Incremental K-D Tree for Robotic Applications,&rdquo; arXiv preprint arXiv:2102.10808, 2021.</p><p>[5] Y. Chen and G. Medioni, &ldquo;Object modelling by registration of multiple range images,&rdquo; Image and Vision Computing, vol. 10, no. 3, pp. 145–155, Apr. 1992, doi: 10.1016/0262-8856(92)90066-C</p><p>[6] Segal, Aleksandr, Dirk Haehnel, and Sebastian Thrun. &ldquo;Generalized-icp.&rdquo; Robotics: science and systems. Vol. 2. No. 4. 2009.</p><p>[7] P. Biber and W. Strasser, &ldquo;The normal distributions transform: a new approach to laser scan matching,&rdquo; in Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453), Las Vegas, Nevada, USA: IEEE, 2003, pp. 2743–2748. doi: 10.1109/IROS.2003.1249285.</p><p>[8] K. Koide, M. Yokozuka, S. Oishi, and A. Banno, &ldquo;Voxelized GICP for Fast and Accurate 3D Point Cloud Registration,&rdquo; in 2021 IEEE International Conference on Robotics and Automation (ICRA), Xi&rsquo;an, China: IEEE, May 2021, pp. 11054–11059. doi: 10.1109/ICRA48506.2021.9560835.</p><p>[9] J. Zhang, Y. Yao, and B. Deng, &ldquo;Fast and robust iterative closest point,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3450–3466, Jul. 2022, doi: 10.1109/TPAMI.2021.3054619.</p><p>[10] S. Choi, Q.-Y. Zhou, and V. Koltun, &ldquo;Robust reconstruction of indoor scenes,&rdquo; in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 2015, pp. 5556–5565, doi: 10.1109/CVPR.2015.7299195.</p><p>[11] M. Barczyk and S. Bonnabel, &ldquo;Observability, covariance and uncertainty of ICP-based scan-matching,&rdquo; 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Chicago, IL, USA, 2014, pp. 1760–1765, doi: 10.1109/IROS.2014.6942762.</p><p>[12] S. Bonnabel, M. Barczyk and F. Goulette, &ldquo;On the covariance of ICP-based scan-matching techniques,&rdquo; 2016 American Control Conference (ACC), Boston, MA, USA, 2016, pp. 5498-5503, doi: 10.1109/ACC.2016.7526532.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/iterative-closest-point/>Iterative Closest Point</a></li><li><a href=https://livey.github.io/tags/icp/>ICP</a></li><li><a href=https://livey.github.io/tags/signal-processing/>Signal Processing</a></li><li><a href=https://livey.github.io/tags/optimization/>Optimization</a></li><li><a href=https://livey.github.io/tags/slam/>SLAM</a></li></ul><nav class=paginav><a class=prev href=https://livey.github.io/posts/2024-12-fast-lio/><span class=title>« Prev</span><br><span>Understanding FAST-LIO: Supplementary Derivations and Explanations</span>
</a><a class=next href=https://livey.github.io/posts/2024-12-radar-processing/><span class=title>Next »</span><br><span>Radar Signal Processing: A Tutorial</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on x" href="https://x.com/intent/tweet/?text=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f&amp;hashtags=IterativeClosestPoint%2cICP%2cSignalProcessing%2cOptimization%2cSLAM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f&amp;title=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications&amp;summary=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f&title=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on whatsapp" href="https://api.whatsapp.com/send?text=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on telegram" href="https://telegram.me/share/url?text=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Iterative Closest Point Uncovered: Mathematical Foundations and Applications on ycombinator" href="https://news.ycombinator.com/submitlink?t=Iterative%20Closest%20Point%20Uncovered%3a%20Mathematical%20Foundations%20and%20Applications&u=https%3a%2f%2flivey.github.io%2fposts%2f2024-12-icp%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
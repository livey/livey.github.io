<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Simultaneous Localization and Mapping (SLAM) | Fuwei's Tech Notes</title>
<meta name=keywords content="Simultaneous Localization and Mapping,SLAM,LiDAR,Autonomous Driving,Iterative Closest Point"><meta name=description content="A tutorial on simultaneous localization and mapping."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-02-20-slam/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="A tutorial on simultaneous localization and mapping."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-02-20-slam/"><meta property="og:title" content="Simultaneous Localization and Mapping (SLAM)"><meta property="og:description" content="A tutorial on simultaneous localization and mapping."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Simultaneous Localization and Mapping (SLAM)"><meta name=twitter:description content="A tutorial on simultaneous localization and mapping."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-02-20-slam/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="Simultaneous Localization and Mapping (SLAM)"><meta property="og:description" content="A tutorial on simultaneous localization and mapping."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-20T00:00:00+00:00"><meta property="article:tag" content="Simultaneous Localization and Mapping"><meta property="article:tag" content="SLAM"><meta property="article:tag" content="LiDAR"><meta property="article:tag" content="Autonomous Driving"><meta property="article:tag" content="Iterative Closest Point"><meta property="og:image" content="https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E"><meta name=twitter:title content="Simultaneous Localization and Mapping (SLAM)"><meta name=twitter:description content="A tutorial on simultaneous localization and mapping."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Simultaneous Localization and Mapping (SLAM)","item":"https://livey.github.io/posts/2025-02-20-slam/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Simultaneous Localization and Mapping (SLAM)","name":"Simultaneous Localization and Mapping (SLAM)","description":"A tutorial on simultaneous localization and mapping.","keywords":["Simultaneous Localization and Mapping","SLAM","LiDAR","Autonomous Driving","Iterative Closest Point"],"articleBody":"In this post, we will introduce the basic concepts of SLAM and how SLAM is used in the autonomous driving system.\nIntroduction to SLAM Simultaneous Localization and Mapping (SLAM) is the process by which a robot or vehicle builds a map of an unknown environment while simultaneously tracking its own position within that map. In essence, SLAM tackles two coupled problems: mapping (creating a spatial representation of the environment) and localization (estimating the device’s pose within that map) at the same time. This capability is fundamental for autonomous systems like self-driving cars, drones, and mobile robots operating in GPS-denied or unfamiliar areas. For example, an autonomous vacuum uses SLAM to avoid getting lost or missing spots, by localizing via wheel odometry and laser range data and mapping obstacles as it goes. Modern SLAM algorithms leverage various sensors – cameras, LiDAR, sonar, IMUs – to perceive the environment. This tutorial focuses on LiDAR-based SLAM, which uses laser scanning to produce accurate 3D point clouds of surroundings. LiDAR SLAM has gained popularity in industry due to its high accuracy and robustness under diverse lighting conditions (unlike cameras, LiDAR works in darkness). However, it also comes with challenges like higher cost, power, and difficulty sensing transparent objects. We will delve into the principles of SLAM, with emphasis on LiDAR, covering mapping and localization techniques, key mathematical models, algorithms like ICP, and recent advances. By the end, you should understand how systems such as autonomous vehicles build and use maps in real-time, and be aware of state-of-the-art frameworks (e.g. LOAM, LIO-SAM) that implement these concepts.\nMapping Mapping in SLAM refers to building a representation of the environment using sensor data. For LiDAR SLAM, the map is often a set of spatial points or surfaces collected from laser scans. Several map representation strategies are used in practice:\nPoint Cloud Maps: The simplest approach is accumulating raw 3D points (X, Y, Z) from LiDAR into a global point cloud. This gives a detailed geometrical map of surfaces (e.g. walls, floors, obstacles). Point cloud maps preserve rich detail but can grow large; hence downsampling or dividing into submaps is common. Occupancy Grids (Voxel Maps): Space can be discretized into a 2D or 3D grid where each cell (voxel) stores whether it’s occupied, free, or unknown. For 2D SLAM (e.g. floorplan mapping with a 2D lidar), occupancy grids are widely used. In 3D, Octree-based occupancy maps (like OctoMap) hierarchically subdivide space into voxels, which is memory-efficient. Each voxel may hold a probability of occupancy updated with sensor readings. Octrees allow multi-resolution mapping – storing fine detail where needed and coarser representation elsewhere – and efficient queries for collision checking or raycasting. Feature-Based Maps: Instead of storing every point, the map can consist of abstract features (landmarks) such as line segments, planar surfaces, or corner points. For example, the LOAM algorithm selects sharp edge points and planar patches from each scan to use as map features. By mapping only these salient features, it achieves a sparse but descriptive map that is faster to align new scans with. Feature maps are often used in graph-based SLAM as the nodes (landmarks) in the graph. Surfels and Gaussian Voxels: Advanced representations model local surface geometry. Surfels (surface elements) are disks with position, normal, and radius, often used in RGB-D SLAM for dense modeling. In LiDAR SLAM, a related idea is the Gaussian voxel map – each voxel stores a Gaussian distribution (mean and covariance) of points within it. This is used in algorithms like Normal Distributions Transform (NDT) mapping or recent methods like LIO-GVM. Representing points as a Gaussian yields a smooth surface approximation and allows faster, more robust scan matching (by matching a point to a voxel’s distribution rather than to individual points). Mapping is an ongoing process during SLAM. As the robot moves, new scans are taken and merged into the map in the estimated pose. If the pose estimate is off, the mapped features will be misaligned, which is why accurate localization is critical for mapping. Maps can be built incrementally (each new scan integrated as we go) or in batch (optimize robot trajectory then merge all data). For large-scale environments, maps may be divided into sub-maps or tiles to manage memory and computational load. The representation chosen impacts not only memory but also how we do scan matching and loop closure later. For instance, point clouds allow precise geometric matching (e.g. ICP), whereas occupancy grids are useful for path planning and have probabilistic integration. In practice, many SLAM systems use hybrid mapping: a point cloud for visualization and precision, plus an occupancy grid for navigation and collision avoidance.\nLocalization Localization is the task of estimating the robot’s pose (position and orientation) within a map. In the context of SLAM, localization must be done concurrently with mapping. The robot uses incoming sensor measurements (e.g. LiDAR scans) to figure out where it is relative to the map it’s building. Effective localization is what keeps the map consistent – if the pose estimate drifts, the mapped features will appear in the wrong place.\nEarly in operation, the map is sparse or empty, so the robot relies on odometry (dead reckoning from wheel encoders or IMU) or initial alignment to guess its movement. As the map grows, the robot can localize by matching new sensor data to the existing map. In LiDAR SLAM, this typically involves a scan-matching algorithm: compare the new LiDAR scan to the map (or recent scans) to find the pose that best aligns it. Techniques like ICP (introduced later) are widely used for this purpose – essentially solving, “How must I move to make this scan overlap with the map points?” On a high level, localization can be seen as an optimization problem: find the pose that maximizes the likelihood of the observed sensor data given the current map.\nCommon localization approaches include:\nParticle Filters (Monte Carlo Localization): Maintain a set of pose hypotheses (particles) and update them based on how well the expected scan from that pose matches the actual scan. This is used in 2D LiDAR localization on a known map (AMCL in ROS). In SLAM (unknown map), particle filters can be extended (as in Rao-Blackwellized Particle Filters) where each particle carries its own map; GMapping is a classic example using this for 2D grid mapping. Kalman Filter / Extended Kalman Filter (EKF): If we assume unimodal Gaussian uncertainty, we can use an EKF to continuously estimate the robot’s pose. The filter uses a motion model to predict pose and a measurement model to correct it by matching features or scan data. EKFs were popular in early landmark-based SLAM: the state vector includes the robot pose and landmark positions, and the filter updates both. We will discuss the dynamic and measurement models in detail later. EKF localization can also be done with a pre-built map: e.g. treat nearest map features as measurements. The limitation is that a single Gaussian may not capture multi-hypothesis uncertainty (e.g. entering a symmetric corridor, there might be two possible locations that match the sensor data). Pose Graph Optimization: Modern SLAM often uses a pose graph approach where each robot pose is a node in a graph, and localization is achieved by optimizing this graph with relative pose constraints. In effect, the “localization” at each step is achieved by incremental alignment (odometry constraint from scan matching) and later global adjustment when loop closures are added. This falls under smoothing rather than filtering, but it accomplishes the same goal of determining poses. We’ll touch on this in loop closure. In LiDAR SLAM, localization accuracy is usually very high for short distances – a new scan typically overlaps heavily with the last scan, so scan-to-scan matching yields millimeter to centimeter accuracy locally. The bigger challenge is long-term drift: errors accumulate over time. That’s where loop closures and global optimization (later sections) come in to re-localize the robot in the global frame and correct any drift. Also, fusing other sensors (GPS when available, or an IMU for orientation) can help maintain a good pose estimate. In summary, mapping and localization are two sides of SLAM’s coin – the map helps us localize, and good localization yields a better map.\nApplications of SLAM SLAM algorithms have become crucial in a range of engineering and industrial applications. Here are a few notable areas where SLAM (and particularly LiDAR SLAM) is applied:\nFigure: Conceptual illustration of autonomous vehicles using SLAM. Multiple self-driving cars scan their surroundings with LiDAR/radar to build maps and localize (blue shaded areas indicate sensor coverage). SLAM allows each vehicle to understand its environment and position, enabling safe navigation in traffic.\nAutonomous Vehicles: Self-driving cars and other autonomous ground vehicles use SLAM to navigate roads without GPS reliability issues (e.g., in tunnels or urban canyons). A LiDAR-based SLAM system provides a continuously updated 3D map of surrounding vehicles, lanes, and obstacles and a precise localization for the car. High-definition 3D maps can also be built on the fly or refined in real-time. Notably, projects like Waymo’s and Uber’s use LiDAR SLAM for real-time perception and localization within prior maps. Mobile Robotics: Indoor service robots (warehouse AGVs, delivery robots, cleaning robots) rely on SLAM to move around unknown or dynamic environments. For instance, a warehouse robot with a 2D lidar builds an occupancy grid of the facility and tracks its pose to navigate between shelves. SLAM enables such robots to operate autonomously – planning paths on the constructed map and updating it if the environment changes (new obstacles, moved furniture). Aerial and Marine Drones: UAVs and UGVs (unmanned ground vehicles) in environments like mines, forests, or oceans use SLAM when GPS is denied or insufficiently precise. LiDAR SLAM is common on drones doing indoor mapping or terrain mapping (e.g., a drone scanning inside a building after a disaster). It allows the drone to both chart the 3D structure and know where it is to avoid collisions. Underwater robots use sonar-based SLAM similarly. Mapping and Surveying: LiDAR SLAM is widely used for mapping infrastructure. Handheld or vehicle-mounted LiDAR SLAM devices scan building interiors, caves, or industrial plants to create 3D reconstructions. The LOAM algorithm (Lidar Odometry and Mapping) and its variants are often used in commercial 3D mapping scanners to produce point cloud maps of factories, construction sites, or forests in real-time as an operator walks around. Augmented Reality (AR) and VR: Although primarily using visual SLAM, some AR systems use depth sensors (LIDAR on newer iPhones/iPads, for example) to improve SLAM in low-texture environments. This is a cross-modal application where LiDAR aids in mapping the room geometry so virtual objects can be placed consistently. Industrial Automation: Robots in domains like mining or agriculture use SLAM on heavy machinery for autonomous operation. SLAM helps a mining vehicle navigate an underground tunnel network, or a self-driving tractor to map a field’s rows. The robustness of LiDAR SLAM to lighting makes it well-suited for outdoor and subterranean scenarios where vision might fail (e.g., glare, darkness). These examples demonstrate why SLAM is a foundational technology in robotics and autonomy. It saves cost and time by allowing machines to operate without prior maps or external localization systems. As computing power and sensor technology have improved, SLAM has moved from research labs into real-world deployments, handling larger environments and running on real-time systems.\nChallenges in SLAM Despite significant progress, SLAM remains a challenging problem in practice. Some of the key challenges and limitations include:\nFast Ego Motion: When the vehicle or robot moves at high speeds, several problems emerge. Scan matching becomes difficult as there is less overlap between consecutive LiDAR scans. Motion distortion within a single scan is more severe, requiring sophisticated deskewing algorithms. High-speed movement also increases measurement uncertainty, as small timing errors result in larger positional errors. For autonomous vehicles traveling at highway speeds (\u003e100 km/h), maintaining accurate SLAM becomes particularly challenging, requiring higher sensor rates and better motion prediction models to compensate for the rapid displacement between measurements.\nLarge-Scale Mapping: Building and maintaining maps of very large areas (e.g., entire cities or highway networks) introduces significant computational and memory challenges. As the map grows, pose graph optimization becomes increasingly expensive, loop closure detection must search through more candidates, and memory requirements for storing dense point clouds become prohibitive. Techniques like hierarchical mapping, submapping, and efficient data structures (octrees, sparse voxel grids) help address these issues, but large-scale SLAM remains difficult, especially when real-time performance is required on resource-constrained platforms.\nHighly Repetitive Environments: Environments with repetitive structures (such as office corridors, parking garages with identical sections, or forests with similar-looking trees) pose a significant challenge for loop closure detection. The perceptual aliasing problem occurs when different places look nearly identical to sensors, causing false loop closures that can catastrophically corrupt the map. SLAM systems struggle to distinguish between truly revisited locations and just similar-looking new areas. This often requires additional context (sequence of observations) or multi-modal sensing (combining LiDAR with visual features) to resolve ambiguities.\nSensor Limitations: Each sensor has quirks. LiDAR provides accurate range data, but it struggles with glass or shiny surfaces (laser beams may pass through or reflect unpredictably). It also can be heavy and power-hungry, and its data rate is limited. Calibration of sensors is crucial; a mis-calibrated LiDAR (e.g., incorrect vertical angle offsets) can distort the map. Moreover, sensors produce noise – e.g., range noise in LiDAR – which the SLAM algorithm must accommodate.\nOdometry Drift: The dead-reckoning (integrating motion over time) invariably accumulates error. Wheel odometry slips or skids, and IMUs have bias drift. Even LiDAR odometry (scan-to-scan matching) can accumulate error if the environment has repetitive structure. This drift means that over long runs, the pose estimate can be substantially off if not corrected, causing map deformation (walls that should be straight appear bent or misaligned).\nComputational Complexity: SLAM algorithms can be computationally intensive, especially with high-resolution sensors. A 3D LiDAR can produce hundreds of thousands of points per second, and real-time SLAM must process this stream on an embedded computer. Algorithms like ICP are O(n·log n) or worse for each iteration (with n points). Large-scale maps also mean large optimization problems for loop closure. Efficiency techniques (sparsification, submapping, multi-threading, hardware acceleration) are essential to keep up. Memory usage is another aspect – storing millions of points or a dense grid can exhaust memory if not managed (hence octree compression, etc.). SLAM implementations must balance accuracy with real-time performance constraints.\nDynamic Environments: Most SLAM methods assume a mostly static world. Moving objects (people, vehicles, pets) can cause false associations or ghost artifacts in the map. For instance, a pedestrian that appears in one scan but not the next might be mistaken for a static obstacle, confusing the mapping process. Dynamic environments violate the static assumption and can throw off localization (the robot might try to match a car that’s no longer there). Solutions involve detecting moving objects (via clustering and tracking or using machine learning to identify people, etc.) and filtering them out of the SLAM update. Long-term changes (doors opening/closing, furniture moved) also pose a challenge for map accuracy over time.\nLoop Closure Detection: Detecting that the robot has returned to a previously visited area (closing a loop) is non-trivial. The robot’s odometry estimate may be off by a large amount by the time it returns, so recognizing a familiar place from sensor data can be like finding a needle in a haystack. In LiDAR SLAM, loop closure often involves comparing the current scan or submap with a library of past scans/submaps to find a match. This can be computationally heavy (comparing against many candidates) and prone to false positives/negatives. A false positive loop closure (wrongly matching two different places) can severely corrupt the map if accepted. Techniques like Scan Context (which creates a rotationally-invariant descriptor of a LiDAR scan) or FPFH features help make loop detection faster and more reliable. Still, ensuring robust loop closures is a core challenge.\nGlobal Consistency and Optimization: Even after detecting loop closures, adjusting the entire map and trajectory to be consistent is complex. Large pose graph optimizations can be slow if not done incrementally. Ensuring the optimization converges to the right solution (and not a local minimum) often requires good initialization. Outlier loop closures need to be identified and removed (outlier rejection strategies like using RANSAC on loop constraints or switchable constraints are used). Maintaining real-time performance while doing these global corrections (sometimes called “pose graph relaxation”) is tough, especially on resource-constrained platforms.\nRobustness and Reliability: SLAM systems have to be robust to a variety of conditions: lighting changes (for visual SLAM), weather (rain or dust can introduce noisy LiDAR readings), rough handling (vibrations affecting sensors), etc. Ensuring the SLAM doesn’t fail catastrophically (loss of tracking) is vital in safety-critical applications. The “kidnapped robot” problem, where a robot is moved unbeknownst to it (or loses track and needs to relocalize), is a classical challenge. Modern SLAM systems incorporate relocalization modules to recover if tracking is lost, but doing this reliably is still an open problem.\nHigh-Level Understanding: Traditional SLAM builds geometric maps, but for true autonomy, robots also need to understand semantic context (knowing objects, room types, etc.). Integrating semantic information (e.g., recognizing a dynamic object as a person and ignoring it for mapping) is challenging but necessary for robustness in human environments. This blends SLAM with AI perception and is an active area of research.\nMany of these challenges are active research topics, and incremental improvements are released regularly. For instance, the LIO-SAM system (2020) demonstrated robust performance by tightly coupling IMU and LiDAR, but it is sensitive to sensor calibration and configuration. The list above (sensor issues, drift, computation, dynamics, loop closure, etc.) highlights why building a fully reliable SLAM system is difficult. In practice, engineers must tune SLAM systems to the expected environment and use-case, and often fuse multiple approaches (and sensors) to mitigate individual weaknesses.\nVisual SLAM vs. LiDAR SLAM Visual SLAM and LiDAR SLAM differ primarily in the sensor data used, which affects the algorithm design, strengths, and weaknesses:\nLiDAR SLAM: Uses active laser scanning to directly measure distances to surfaces, producing precise 3D (or 2D) point clouds. LiDAR SLAM excels in accuracy and robustness in various conditions. It does not depend on ambient light or texture; a LiDAR can map in complete darkness or in feature-poor environments (like white walls) where cameras would struggle. LiDAR measurements have actual scale (meters) with high precision, leading to highly accurate localization (often within centimeters). This is why LiDAR-based methods generally out-perform vision in localization accuracy. However, LiDAR sensors are relatively expensive and bulkier. They also have a limited angular resolution and can produce a lot of data to process. A key limitation is that LiDAR cannot see glass or transparent surfaces (the beams pass through or refract) and can be confused by rain or fog (returns from particles). LiDAR SLAM systems also consume more power and processing due to the dense point clouds. Another consideration: LiDAR provides geometry but no color or texture information, which means recognizing places purely from geometry can be hard if environments are similar in shape. Visual SLAM: Uses passive cameras (monocular, stereo, or RGB-D) to infer motion and structure from imagery. Visual SLAM benefits from cheap, lightweight sensors (cameras) and can leverage rich appearance information (textures, colors, identifiable objects). It’s great for recognizing landmarks – e.g., a camera can recognize a previously seen painting on a wall, aiding loop closure. Visual SLAM algorithms (like ORB-SLAM2, VINS-Fusion) often detect and track feature points (corners, blobs) in images across frames. They can be very lightweight to run on mobile devices (ARKit/ARCore on smartphones). However, cameras struggle with lighting changes (night vs day, or entering a dark room) and lack direct depth: a single camera SLAM has scale ambiguity (it might not know if it moved 1m or 2m if everything just looks scaled). Depth can be obtained via stereo or depth sensors, but then calibration and noise come into play. Visual SLAM can also fail if the scene has few visual features (e.g., a blank wall or uniform texture floor). It is generally less robust in feature-poor or dynamically lit environments compared to LiDAR. Furthermore, visual methods may have higher drift in pure odometry due to scale uncertainty and narrow field of view (depending on camera lens). Hybrid approaches: Because of complementary strengths, many systems fuse camera and LiDAR (and IMU). For example, LVI-SAM (2021) is a system that combines LIO-SAM (LiDAR-Inertial) with VINS-Mono (Visual-Inertial) to get the benefits of both. The camera can provide texture cues for loop closure (e.g., recognizing a previously seen sign), and LiDAR provides accurate structure and scale. Such systems can even survive if one sensor fails (e.g., in a dark featureless tunnel, the LiDAR carries SLAM; in heavy smoke where the LiDAR might get noisy but some visual cues exist, the camera helps).\nIn summary, LiDAR SLAM is preferred for applications needing reliable, precise mapping in 3D (like autonomous driving, high-end robotics), especially outdoors or in large spaces. Visual SLAM shines in cost-sensitive or size-constrained applications (AR devices, small robots) and can provide semantic-rich mapping (recognizing objects in the scene). Many real-world solutions use a combination: LiDAR for geometry + camera for semantics, achieving the best of both. The choice also depends on environment: indoors, sometimes cameras plus a depth sensor suffice; outdoors in varying light, LiDAR might be indispensable. Industry engineers must weigh these trade-offs when selecting a SLAM solution, often opting for multi-sensor SLAM to maximize robustness.\nBasics of LiDAR To appreciate LiDAR-based SLAM, one must understand the basics of the LiDAR sensor itself. LiDAR (Light Detection and Ranging) emits laser pulses and measures the time it takes for each pulse to bounce off an object and return to the sensor. Given the speed of light, the round-trip time directly gives the distance to the object. By firing pulses in many directions, a LiDAR constructs a 3D “point cloud” of its surroundings.\nKey characteristics of LiDAR sensors:\nScanning Mechanism: Many LiDARs (like the popular Velodyne 16, 32, 64 channel models) use multiple laser diodes arranged in a vertical stack that spin 360° horizontally. Each laser fires rapidly as it spins, yielding a set of elevation angles and a continuous azimuth rotation. This produces a 360° horizontal field of view and a limited vertical FOV (e.g., 30° spread for a 16-beam sensor). The result is a dense ring of points at various elevations for each 360° sweep (often called a scan or a sweep). Newer solid-state LiDARs use MEMS mirrors or optical phased arrays to steer the beam without mechanical rotation; their patterns can be raster scan or other shapes, but the end result is still a point cloud of distances. Output Data: A LiDAR typically outputs a list of points (x, y, z in the LiDAR’s coordinate frame) or in polar form (distance, vertical angle, horizontal angle). Many also provide intensity (strength of return) which can sometimes aid in recognizing reflectivity of surfaces. A 3D LiDAR like a 64-beam might output ~100k to 1.3 million points per second. A 2D LiDAR (single plane) will output a few tens of thousands of points per second in a flat plane (useful for 2D SLAM like in robotics). Accuracy and Range: LiDAR range measurements are very precise (often ±2 cm or better) up to long ranges. High-end units can see 100-200 meters in good conditions, which is great for outdoor mapping. However, maximum range can drop in rain or fog due to scattering. The angular resolution (spacing between beams or between successive shots) dictates the density of the point cloud – common resolutions are 0.1° to 0.4°. Closer objects yield more returns (they intersect more beams) whereas distant small objects might slip through gaps in the scan. Data Rate and Coordination: A spinning LiDAR produces data in a continuous stream. Often one full revolution (say 10 Hz or 20 Hz) is considered one scan. Motion Distortion: If the LiDAR or platform moves during a scan (which it always does on a moving robot), points in one scan are captured at different times. This means the point cloud is slightly “smeared” by the motion. SLAM systems must address this by deskewing – using the robot’s rotation rate or IMU data to correct point positions within a scan. For example, if the vehicle rotated a bit while a scan was captured, the resulting cloud can be rotated back to a common time. LOAM performs such deskewing assuming constant velocity during the sweep, and LIO-SAM uses IMU integration to undistort scans in its pre-processing step. Coordinate Frames: LiDAR data usually starts in the LiDAR’s own frame. For SLAM we define a world frame (map frame) and the LiDAR’s pose in that frame. As SLAM estimates the pose, each point can be transformed into the world frame to build the map. From a SLAM perspective, LiDAR provides a very direct measurement of environment geometry. Unlike a camera that gives pixels one must interpret, LiDAR gives a set of 3D points on surfaces. This simplifies the mapping problem (no need to triangulate depth as in stereo vision) and makes data association more geometric. However, the large number of points means one must use efficient spatial data structures (e.g., k-d trees for nearest neighbor search, octrees for map storage) to make use of the data in real-time. Also, LiDAR’s lack of semantic info means SLAM might integrate anything it sees (including dynamic objects) unless steps are taken to filter those.\nIn summary, LiDAR is a powerful sensor for SLAM because it provides accurate and rich spatial information. Its usage comes with engineering considerations like handling large data throughput, compensating for motion distortion, and dealing with environments where certain materials might not reflect lasers well. The rest of this tutorial will leverage these LiDAR properties as we discuss SLAM algorithms specialized for them.\nMathematical Formulation of SLAM (Mapping, Localization, SLAM) Mapping Given robot state trajectory $\\mathbf{x}_{0:T}$ and sensor measurements $\\mathbf{z}_{0:T}$ with observation model $h$, build a map $\\mathbf{m}$ of the environment\n$$ \\min_{\\mathbf{m}} \\sum_{t=0}^{T} \\|\\mathbf{z}_t - h(\\mathbf{x}_t, \\mathbf{m})\\|_2^2 $$Localization Given a map $\\mathbf{m}$ of the environment, sensor measurements $\\mathbf{z}_{0:T}$ with observation model h, and control inputs $\\mathbf{u}_{0:T-1}$ with motion model f, estimate the robot state trajectory $\\mathbf{x}_{0:T}$\n$$ \\min_{\\mathbf{x}_{0:T}} \\sum_{t=0}^{T} \\|\\mathbf{z}_t - h(\\mathbf{x}_t, \\mathbf{m})\\|_2^2 + \\sum_{t=0}^{T-1} \\|\\mathbf{x}_{t+1} - f(\\mathbf{x}_t, \\mathbf{u}_t)\\|_2^2 $$SLAM Given initial robot state $\\mathbf{x}_0$, sensor measurements $\\mathbf{z}_{1:T}$ with observation model $h$, and control inputs $\\mathbf{u}_{0:T-1}$ with motion model $f$, estimate the robot state trajectory $\\mathbf{x}_{1:T}$ and build a map $\\mathbf{m}$\n$$ \\min_{\\mathbf{x}_{1:T}, \\mathbf{m}} \\sum_{t=1}^{T} \\|z_t - h(x_t, \\mathbf{m})\\|_2^2 + \\sum_{t=0}^{T-1} \\|x_{t+1} - f(x_t, u_t)\\|_2^2 $$To formally understand SLAM, it helps to express it as an estimation problem. We can denote:\nThe robot’s pose at time $t$ as $x_t$ (this could be a 2D pose $[x, y, \\theta]$ or 3D pose $[x, y, z, roll, pitch, yaw]$). The map of the environment as $m$ (this could be a set of landmarks or an occupancy grid or point cloud – basically all parameters describing the map). The control inputs (odometry or movement commands) from time $t-1$ to $t$ as $u_t$. The observations (sensor measurements) at time $t$ as $z_t$ (e.g., a LiDAR scan). Mapping-only problem: If the robot’s trajectory $x_{1:t}$ were known (e.g., via very precise GPS or motion capture), the mapping task is to compute $m$ given all observations. This reduces to sensor fusion: integrate all sensor data in a common frame to build the map. With known poses, mapping is straightforward – just place each LiDAR point in the global frame and accumulate (with perhaps probabilistic updates for occupancy). This is sometimes called the “mapping with known poses” problem. There isn’t much uncertainty here except sensor noise, which can be handled by filtering (e.g., averaging multiple measurements per cell). In probabilistic terms, $P(m \\mid x_{1:T}, z_{1:T})$ is the quantity of interest (the map posterior given the true poses and measurements). Many mapping algorithms assume known pose (like building a map from a logged trajectory).\nLocalization-only problem: If a map $m$ is known (say we have a prior map of the building), localization means computing $x_t$ given the sensor observations and the map. Essentially we want $P(x_t \\mid m, z_{1:t}, u_{1:t})$. This is the classic localization or tracking problem, solved by methods like the Kalman Filter or particle filter. The robot uses the known map as a reference to match its current sensors and find its pose. This is easier than full SLAM because the map is fixed (the problem is only in 6 DOF or so of the pose, not in potentially hundreds of map variables). Monte Carlo Localization (MCL) and Extended Kalman Filter localization are examples of solving this. In localization mode, the map doesn’t change, only our belief of where we are on that map is updated.\nFull SLAM problem: Both the trajectory $x_{1:T}$ and the map $m$ are unknown and must be estimated simultaneously from all observations $z_{1:T}$ and controls $u_{1:T}$. We seek the posterior $P(x_{1:T}, m \\mid z_{1:T}, u_{1:T})$. This joint estimation is what makes SLAM hard – the robot doesn’t know the map a priori, and it needs the map to localize and vice versa. In practice, we often factor this problem or approximate it:\nOne approach (online SLAM) is to estimate the current pose and map incrementally (this is what filtering methods do, maintaining $P(x_t, m \\mid z_{1:t}, u_{1:t})$ and updating with each new measurement). Another approach (full smoothing) is to estimate the entire trajectory and map as one big optimization (this is what graph-SLAM does, maintaining a factor graph over all poses and landmarks). A common probabilistic formulation is through the Bayes filter (recursive estimation). At each step:\nPrediction (Motion Update): Use the motion model $P(x_t \\mid x_{t-1}, u_t)$ to predict the new pose distribution from the previous pose. This model encapsulates how we think the state changes given control input (or odometry). For instance, if the robot moved forward 1m with some uncertainty, we predict its pose accordingly. In full SLAM, we also carry along the map (which typically doesn’t change in prediction, since map is static). Correction (Measurement Update): Incorporate the observation $z_t$ via the measurement model $P(z_t \\mid x_t, m)$. This tells us how likely a measurement is given a pose and map. Inversely, it helps adjust the belief of $x_t$ (and potentially $m$) to better explain the actual measurement. For example, if the LiDAR expected to see a wall 1m away but actually saw nothing, the pose (or map) estimate might be off and is corrected. In an EKF SLAM setting, for instance, the state vector is $[x, m]$ and the above is done with linearization: the motion model $f(x_{t-1}, u_t)$ linearized gives a predicted mean and covariance, then the measurement model $h(x_t, m)$ linearized provides a Kalman gain for update. In a particle filter SLAM (like FastSLAM), the prediction moves particles according to the motion model, and the update weights particles by likelihood of observations given the map that particle has.\nAnother useful view is the graphical model of SLAM: we can construct a factor graph where:\nPose variables $x_1, x_2, ..., x_T$ are connected by factors from odometry (each $u_t$ induces a factor between $x_{t-1}$ and $x_t$ encoding the motion constraint). Each observation connects pose $x_t$ to some part of the map $m$ (if landmarks, a factor between $x_t$ and landmark $l_i$; if dense map, effectively a factor between $x_t$ and the map surface measured). Loop closures appear as additional measurement factors connecting a later pose $x_j$ back to an earlier pose or landmark observed previously. Solving SLAM then is performing inference on this factor graph – basically finding the configuration of all pose and map variables that best satisfies all these constraints (maximum a posteriori estimation). Modern SLAM back-ends use this formulation: bundle adjustment in visual SLAM is essentially graph optimization on camera poses and feature points; pose graph optimization in LiDAR SLAM optimizes the chain of poses with loop closure constraints for consistency. Mapping vs Localization vs SLAM formulation: In summary, mapping alone or localization alone are relatively simpler sub-problems with established solutions (integration of measurements for mapping, Bayes filter for localization). SLAM is harder because it must tackle both with coupled uncertainty. The mathematical formulation as a joint probability problem guides how we design algorithms:\nIf we assume Gaussian distributions and linearize models, we get the EKF solution (with a big covariance matrix over pose and landmarks). If we allow multiple hypotheses, we get particle solutions like FastSLAM (which factor the posterior into a product of a trajectory posterior and landmark posteriors). If we optimize globally, we set up a nonlinear least squares problem (graph-SLAM) that is solved by iterative methods (like Levenberg-Marquardt or Gauss-Newton on the factor graph). Each approach makes certain assumptions for tractability. In the next sections, we’ll discuss the components that feed into these formulations: the motion model (how we predict poses), the measurement model and data association (how we align sensor data to map), and specific algorithms like ICP that implement these under the hood.\nMotion Model in SLAM The motion model (or dynamic model) predicts the robot’s next state based on its previous state and control inputs. It encapsulates our understanding of how the robot moves. A good motion model is crucial for SLAM because it provides the prior before observing new sensor data. In probabilistic terms, it’s $P(x_t \\mid x_{t-1}, u_t)$.\nCommon motion models in robotics:\nDifferential Drive Kinematics (for wheeled robots): For a robot with wheel encoders, the control input might be the distance traveled $\\Delta d$ and rotation $\\Delta \\theta\\$ since last update. The motion model (in 2D) could be: $$x_t = x_{t-1} + \\Delta d \\cos(\\theta_{t-1})$$$$y_t = y_{t-1} + \\Delta d \\sin(\\theta_{t-1})$$$$\\theta_t = \\theta_{t-1} + \\Delta \\theta,$$plus some noise in each term (to account for wheel slip or uneven terrain). This is essentially what’s used in typical mobile robot odometry. The noise can be modeled as Gaussian with variance proportional to $\\Delta d$ and $\\Delta \\theta$ (e.g. uncertainty grows with distance traveled).\nAckermann or Car Model: For car-like vehicles, controls might be steering angle and speed. The motion model involves turning radius and tends to be more constrained (cannot turn on spot). Still, one can derive a prediction of new pose given old pose and controls, with appropriate noise. Omnidirectional or Holonomic Models: For drones or omniwheel robots, you might have velocity in 3 axes and angular velocities. An IMU gives linear acceleration and angular velocity; integrating those (with gravity removal and drift) provides a dead-reckoned pose. More sophisticated models incorporate IMU bias as part of the state. Constant Velocity or Constant Turn Rate models: In absence of direct odometry, some SLAM front-ends assume a simple motion prior like “the robot continues moving as it was”. For example, in LOAM’s odometry, between scans they might assume constant velocity motion to predict a rough guess of the next pose, which is then refined by scan matching. This helps in reducing the search space for matching. In SLAM, the motion model is used in two ways:\nState Prediction: When doing filtering (EKF, particle), we propagate the pose estimate forward using the motion model and control. This gives us a prior for the sensor update. If the robot moved 1 meter forward according to odometry, we predict the pose changed accordingly, but we also inflate the uncertainty because odometry isn’t perfect. This predicted distribution is often called the proposal for the next state. As a Factor/Constraint: In graph-based SLAM, each odometry measurement yields a constraint between consecutive poses. For instance, a factor might say “according to wheel odometry, the transform from $x_{t-1}$ to $x_t$ is $\\Delta x$ with some covariance”. This is essentially the motion model constraint. Even without explicit odometry sensors, we can get a motion constraint from the SLAM front-end itself (ex: the initial guess from scan matching plays the role of odometry). It’s important that the motion model be consistent with the robot’s actual movement capabilities. If it’s too naive, the SLAM system might make poor predictions and find it harder to converge during sensor updates. If it’s accurate, it keeps the pose estimate close to the true value before sensor correction, which can improve data association and convergence.\nNoise and uncertainty growth: Generally, as the robot moves without a new observation, its position uncertainty increases. For instance, if you drive 10m straight, you might accumulate a significant positional uncertainty because small wheel slips integrated over 10m become large. This is often modeled by something like a covariance matrix $Q$ added during prediction. The Kalman Filter prediction step does $P\\_{t|t-1} = F P\\_{t-1|t-1} F^T + Q$ where $F$ is the Jacobian of the motion model, and $Q$ is motion noise covariance. More uncertainty means we rely more on measurement to correct.\nIn LiDAR-based SLAM, interestingly, some implementations forego an explicit motion model by using scan matching as the “prediction”. For example, Hector SLAM (2D lidar SLAM) assumes a motion model of constant velocity and directly aligns scans, not even using wheel odometry. It effectively trusts the scan matching more than any prior motion estimate. Others like Cartographer use an IMU to constrain orientation (motion model for roll/pitch) and wheel odometry for linear motion when available – this makes the system more robust and drift less over short distances.\nIn summary, the motion model anchors SLAM by providing an initial guess of where the robot moved. It is as important as the sensor data in maintaining a stable estimate. A mismatch (say using a straight-line model for a robot that actually strafes) can lead to inconsistency. Tuning the noise parameters of the motion model is also key: too confident in odometry and you might under-weight LIDAR corrections; too little confidence and the filter might over-react to noisy measurements. A good balance keeps SLAM stable.\nMeasurement Association in SLAM When the robot receives a sensor measurement, the SLAM system must determine which part of the map (or which landmark) that measurement corresponds to – this is the data association or measurement association problem. In other words: given a new observation, how do we associate it with a feature or location in our existing map?\nThis is one of the trickiest parts of SLAM, especially in feature-based formulations:\nIn landmark-based SLAM (e.g., EKF-SLAM with point landmarks), when the robot observes some feature (like a visual feature or a corner in a laser scan), it must decide if this is:\na new landmark that hasn’t been seen before, in which case we add it to the map, or a re-observation of an existing landmark, in which case we update that landmark’s estimate. In dense mapping (like with occupancy grids or raw point clouds), data association is implicit in the scan matching process. Essentially, when we align a new scan to the map, we are associating points in the scan to nearby points/surfaces in the map (usually by nearest neighbor search). If the map is an occupancy grid, association is checking which grid cells correspond to detected obstacles in the scan.\nKey techniques for measurement association:\nNearest Neighbor (NN) Association: The simplest method: for each observed feature, find the closest matching feature in the map by some distance metric. For LiDAR, this might mean taking a point from the new scan and finding the nearest point in the existing map cloud (or nearest plane/line feature). Nearest neighbor works if the density of features is high enough and the initial pose estimate is reasonably good so that true correspondences are near in space. However, it can mismatch in ambiguous situations (corners, repetitive structures). Gating: Before accepting an association, apply a gating threshold – e.g., only consider a match if the distance is below some threshold. In EKF-SLAM, a Mahalanobis distance gate is used: an innovation (measurement minus expected measurement) that is too large (statistically unlikely) is rejected as a possible match. Gating prevents very wrong associations that would severely skew the estimate. Joint Compatibility: More advanced data association considers sets of correspondences that are mutually compatible. For example, in a laser scan of multiple landmarks, you want to find the assignment of observed points to map landmarks that overall makes sense (like solving a small assignment problem). Algorithms like JCBB (Joint Compatibility Branch and Bound) search for a consistent matching that maximizes likelihood while rejecting combinations that are incompatible with the vehicle’s possible motion. Descriptors for Features: In visual SLAM, features have descriptors (ORB, SIFT etc.), which help match observations to the same landmark seen before by comparing descriptor similarity. In LiDAR, one can use shape descriptors (e.g., histogram of surrounding points) for loop closure matching (e.g., Scan Context gives a descriptor for a scan to match places). While not exactly point-to-point association, these methods assist in recognizing previously visited places or re-observing a mapped area from a different angle. Outlier Rejection: Even with gating, some wrong associations can slip through (especially in loop closure detection where the initial pose guess might be off by a lot). RANSAC (Random Sample Consensus) is commonly used to robustly match sets of points: for instance, fitting a transform between two scans by randomly sampling correspondences and finding a consensus transform that aligns many points. Any points that don’t agree with this transform are considered outliers and rejected. RANSAC effectively finds a large subset of consistent pairings, filtering out spurious matches. In LiDAR ICP (scan-to-scan or scan-to-map alignment), data association is the step of finding closest points (or closest plane surface) for each point in the source scan. By iterating, ICP refines the associations: initially, with a rough pose guess, many correspondences might be wrong, but as the pose converges, the set of nearest neighbors hopefully represents the true matching of scan points to map points. Some variants weigh correspondences by how close or how planar they are, and discard those above a certain distance (this is effectively gating).\nA major challenge in association is perceptual aliasing – different places look the same to the sensor. For example, in an office with many similar cubicles, a scan in one corridor might look just like a scan in another corridor. Nearest neighbor might incorrectly associate to the wrong hallway. In such cases, global context or history is needed to avoid false loop closures. Many SLAM systems will be conservative in accepting a loop closure: they may require multiple corroborating cues (e.g., vision and LiDAR both agree it’s the same place) or a sequence of measurements matching, before committing to closing the loop.\nIn EKF-SLAM, a known trick is that if a wrong association is made even once, the filter can become inconsistent and diverge. Graph-based methods are a bit more resilient (you can later remove a bad constraint), but they also can get stuck in a wrong configuration if a bad loop closure is added without realization.\nAutomatic mapping of new features: In mapping, if a measurement doesn’t match any known feature within reasonable uncertainty, it spawns a new map feature. For instance, a new corner is detected by LiDAR in a place where no mapped corner exists nearby, then we add a new landmark at that location. Tuning the threshold for new feature creation vs. matching to existing is important to not explode the number of landmarks while not missing real ones.\nIn occupancy grid SLAM, data association might not be explicitly discussed, but it’s happening when integrating a scan into the grid: each laser ray is associated with a set of grid cells along its path (free space) and the endpoint cell (occupied). If the pose is off, the updates go into the wrong cells. Particle filter mapping handles this by trying many pose hypotheses, essentially deferring association until after pose probability is computed.\nIn summary, measurement association is about figuring out what the sensor is seeing in terms of the map. Doing this correctly maintains map consistency and bounds the error. Doing it poorly leads to corrupted maps or lost localization. As environments grow more complex, robust association remains a key difficulty – it often drives the need for richer sensor data (like adding vision or semantics to distinguish places) and better algorithms (like SLAM back-ends that can identify and discard outlier loop closures to recover from mistakes).\nIterative Closest Point (ICP) One of the workhorse algorithms in LiDAR-based SLAM is Iterative Closest Iterative Closest Point (ICP). Iterative Closest Point (ICP) is a fundamental algorithm for aligning two point clouds and is heavily used in LiDAR SLAM for scan matching (both for odometry and loop closure). Given a “source” point cloud (e.g. the new LiDAR scan) and a “target” point cloud (e.g. the previous scan or a map), ICP iteratively refines the pose transform (rotation $R$ and translation $t$) that best aligns the source to the target. The basic ICP procedure is:\nInitialize the transform. Start with an initial guess for the relative pose (this could be from odometry or simply the identity if no prior). Find Closest Points: For each point in the source cloud, find the nearest neighbor in the target cloud (according to Euclidean distance, or sometimes along a line-of-sight for 2D). These pairs are the current correspondences. Compute Transform: Estimate the optimal rigid transform (rotation $R$ and translation $t$) that minimizes the alignment error for these correspondences. For point-to-point ICP, this often means minimizing the sum of squared distances $\\sum_i | R \\cdot p_i^{\\text{source}} + t - q_i^{\\text{target}}|^2$. A closed-form solution (using SVD on the correlation matrix of paired points) gives the best fit $R, t$. Apply Transform: Move the source cloud by this computed transform to (hopefully) better align with target. Iterate: With the source now moved, go back to step 2 (recompute nearest correspondences) and repeat. Continue until convergence – i.e., the change in error or in pose becomes very small – or until a maximum number of iterations is reached. Figure: Conceptual illustration of ICP aligning two shapes (red and blue). The algorithm iteratively matches points (colored lines linking red–blue pairs) and refines the transform. In the rightmost image, the blue shape (transformed source) aligns closely to the red shape (target). ICP will stop when the alignment error is below a threshold.\nICP will converge to a local minimum of the alignment error. If the initial guess is good and the surfaces overlap significantly, ICP usually converges to the correct alignment. However, if the initial guess is far off (beyond the “capture range”), ICP can converge to a wrong match (e.g., matching a pillar in source to a different pillar in target). Thus, providing a reasonable initial pose (via odometry or IMU) is important for ICP in SLAM.\nVariants of ICP: Standard ICP uses point-to-point distance. Variants improve convergence:\nPoint-to-plane ICP: Instead of minimizing point-to-point distances, if you can estimate surface normals for target points, you minimize the distance of each source point to the tangent plane of the nearest target point. This often converges faster and more accurately on planar environments, as it accounts for the surface orientation. Generalized ICP (GICP): A merge of point-to-point and point-to-plane, modeling each point with a covariance (uncertainty shape) often based on local structure. GICP treats the alignment as matching two Gaussian distributions per correspondence and tends to be robust. Trimmed ICP: It rejects a fraction of correspondences with largest errors in each iteration, to avoid being skewed by outliers (like a moving object in one scan that isn’t in the other). Multi-Scale ICP: Perform ICP on a downsampled (coarse) version of the clouds first to get close, then on finer detail – helps avoid local minima. Color ICP: (not as relevant for pure LiDAR, but for RGB-D data) uses color information to improve matching. In LiDAR SLAM front-ends (odometry), ICP (or its variant) is often running continuously: every new scan aligns to the previous scan (or a small local map) to yield an estimated transform since the last frame. This gives LiDAR odometry. For example, LOAM uses a form of ICP but on feature points: it extracts edge and plane features and then does nearest-neighbor matching of these features to the last frame’s features, solving for the motion that aligns them. This is essentially ICP restricted to features.\nHere’s a pseudocode of a basic ICP algorithm for clarity:\nfunction ICP(source_points, target_points, max_iterations, tolerance): Transform := Identity // initial guess for iter = 1 to max_iterations: // 1. Find nearest neighbor correspondences correspondences := [] for p in source_points: q := NearestNeighbor(p, target_points) correspondences.append((p, q)) // 2. Compute optimal R, t aligning all (p,q) pairs (R, t) := ComputeRigidTransform(correspondences) // 3. Apply the transform to the source points source_points := [R * p + t for p in source_points] Transform := (R, t) o Transform // update overall transform // 4. Check convergence (e.g., max correspondence error change) error := ComputeAlignmentError(correspondences, R, t) if error \u003c tolerance: break return Transform In practice, efficient data structures like k-d trees are used for the nearest neighbor search to speed up step 2. Also, one might not use all points (downsampling by a voxel grid filter can make ICP faster with little loss of accuracy).\nICP in Loop Closure: ICP isn’t only for consecutive frames. It can be used to align a new scan to an older scan or submap when detecting loop closure. For instance, if place recognition suggests that “scan at time 100 is near scan at time 10”, we can use ICP to refine the exact relative pose between those scans, yielding a loop closure constraint.\nOne must be cautious that ICP is local. For global alignment (big loops), one often combines it with a global descriptor match to get close, then ICP to fine-tune. A successful ICP alignment significantly reduces relative pose error, which is then fed into the SLAM back-end (e.g., as a factor in pose graph).\nIn summary, ICP provides the mathematical method to answer “how did the LiDAR move from scan A to scan B?” by geometric alignment. It is ubiquitous in LiDAR SLAM implementations (from 2D algorithms like Hector SLAM which essentially do a variant of ICP on occupancy grids, to 3D ones like LOAM, Cartographer’s scan matching, etc.). Its strengths are simplicity and precision in matching geometry; its weaknesses are computational load for large clouds and the local-minimum issue requiring good initial guess.\nOdometry and its Role in SLAM Odometry refers to the estimation of the robot’s incremental motion over time. In SLAM, odometry (whether from wheel encoders, IMU, or LiDAR visual odometry) plays the role of providing a real-time local pose estimate that keeps track of where the robot has moved between updates. While odometry on its own will drift, it is indispensable for giving the SLAM system short-term accuracy and a starting guess for alignment.\nTypes of odometry:\nWheel Odometry: Calculated from wheel rotations. For example, a differential drive robot integrates wheel encoder ticks into an $(x,y,\\theta)$ pose. This is straightforward and high-rate, but subject to wheel slip and cannot account for changes in elevation or rough terrain. Visual Odometry (VO): Using camera images to estimate motion (by tracking feature displacements between frames). VO gives 6-DoF motion and works where wheel odometry might not (e.g., legged robots or flying drones). It can drift if features are far or if there’s motion blur. IMU integration: An IMU (accelerometers + gyros) can provide orientation changes and short-term motion with high frequency. Integrating IMU data (which gives velocity and orientation when integrated) yields odometry that is very smooth and low-latency, but drifts over time (especially position due to acceleration bias integration). LiDAR Odometry (LO): This is essentially using the LiDAR itself to compute odometry by scan matching successive frames. It’s what LOAM’s first stage does: it computes a 10 Hz pose from aligning scan k to scan k-1. Many modern SLAM systems (LIO-SAM, FAST-LIO, etc.) also have a “odometry node” that runs ICP or a Kalman Filter on a combination of IMU + LiDAR to output a continuous pose estimate. In a SLAM system, odometry is often treated as the prediction:\nThe odometry gives a quick estimate of the pose change since the last sensor update. This can be used to propagate the filter or to initialize ICP for the next scan alignment. Even if odometry drifts in the long run, over a short interval (say 0.1s), it’s usually quite accurate. This ensures that when the next sensor reading comes in, we don’t have to search blindly over a huge space to align things; we can start near the odometry predicted position. Odometry alone, if one simply dead-reckons, will eventually produce an inaccurate trajectory (e.g., wheel slip might cause a 5% distance error, leading to 5m error after 100m traveled). However, SLAM’s mapping/loop closure will correct this drift over time by referencing external landmarks. Essentially, odometry keeps the pose estimate coherent in the short term, and SLAM provides global corrections in the long term.\nMany SLAM architectures split into front-end and back-end:\nThe front-end often includes odometry estimation (e.g., tracking) and loop closure detection. The back-end does optimization (adjust poses to satisfy constraints). For instance, in Cartographer (by Google) there is a local SLAM that integrates IMU + scan matching (odometry) to build small submaps, and a global SLAM that performs loop closure optimization on submap poses. The local SLAM acts as odometry, providing immediate pose feedback.\nIn LOAM, they explicitly break it into two threads:\nA high-frequency odometry thread (10 Hz) that uses a subset of points to estimate motion quickly but with some drift. A low-frequency mapping thread (1 Hz) that uses accumulated data to refine the pose and update the map, and can correct drift over larger windows. This design shows how odometry (the fast thread) is used to keep up with motion, while mapping/loop closures (slow thread) correct it gradually.\nOdometry sources can also be fused: LIO-SAM tightly fuses IMU pre-integration with LiDAR scan matching to produce a robust odometry estimate that is better than either alone. The IMU helps to deskew and provide initial guess for scan matching, and scan matching corrects IMU drift – resulting in a more stable odometry.\nIn summary, odometry is the backbone that carries the pose forward between absolute corrections. Good odometry reduces the burden on the rest of the SLAM system:\nIf odometry is very accurate, the mapping just has to fine-tune and mostly handle loop closures occasionally. If odometry is poor, the SLAM system has to rely more on matching observations to past map data even for short-term stabilization, which is harder and computationally expensive. From an engineering perspective, always leverage any available odometry (wheel, IMU, etc.) because it makes the SLAM problem more tractable. But also be aware that odometry drift is inevitable – thus the SLAM must incorporate loop closures or other global references (like GPS, landmarks, or known map alignment) to remain globally consistent.\nOctree and Octree of Features Octrees are tree data structures that recursively divide 3D space into eight octants (halving each axis at each level). They are widely used in 3D mapping to manage spatial information efficiently. An Octree-based map provides a memory-efficient representation by adapting resolution: empty or uniformly occupied regions can be stored coarsely, and complex regions stored at finer granularity. The popular OctoMap library represents a probabilistic occupancy grid in an octree format, enabling large maps to be handled without exorbitant memory.\nUsing Octrees in SLAM:\n3D Occupancy Mapping: As the robot moves, LiDAR points can be inserted into an octree. Each node in the octree corresponds to a cubic volume of space. OctoMap maintains a log-odds of occupancy for each node (voxel) based on how many points hit inside it vs pass through it. This naturally handles sensor uncertainty and can merge multiple observations. Octrees allow quick queries like “is there an obstacle at (x,y,z)?” or ray casting for line-of-sight, which are useful for navigation and sensor simulation. They also make it easy to downsample for visualization (since you can display only leaf nodes at appropriate levels). Multi-Resolution Maps: The octree inherently is multi-scale. For example, high detail (small voxels) near the robot or in areas of interest, and large voxels farther or in sparse areas. This can drastically reduce the number of points stored. A corridor with flat walls: all those wall points can be compressed into large planar cells once the wall has been fully observed. Dynamic Updates: Octrees can be updated online – e.g., if something changes or if new space is explored, the tree expands or updates those nodes. Also, sensor range data provides free-space info: every beam not just gives an occupied endpoint but also many empty cells along its path, which OctoMap marks as free. This helps for navigation (identifying free space). Octree of Features: In some SLAM systems, instead of occupancy, the map stores features in an octree. For example, LOAM’s mapping module maintains a voxel grid of feature points: it divides space into voxels (like 1m^3 or so) and keeps at most one or a few edge/plane feature points per voxel to limit density. This is effectively an octree (or grid) of features – it sparsifies the feature map to ensure computational efficiency (constant time lookup per voxel). When a new feature is extracted from a scan, if its voxel already has a feature, they might average them or skip adding, to keep map size bounded.\nAnother use is for place recognition: One approach (NoctuSLAM, as hinted by its name) uses an octree structure to store appearance of places for loop closure detection. An octree can store at each node a summary of the shape (e.g., a hash or signature of the submap in that octant). This can speed up matching large point clouds by coarse-to-fine matching.\nIn recent research, Gaussian voxel maps (GVM) have emerged, where each voxel (often implemented via an octree) stores a Gaussian distribution (mean and covariance of points in that cell) instead of just occupancy or a single point. This is beneficial for scan matching: rather than matching individual points, one can match a point to a Gaussian blob representing a surface. The GVM approach (as in LIO-GVM) drastically compresses the map (only one Gaussian per voxel) and provides analytical derivatives for alignment. Essentially, this is a marriage of octree mapping and ICP (like a continuous occupancy field for matching).\nBenefits of Octrees:\nEfficient memory usage (only subdivide where needed). Fast Nearest Neighbor search if done properly (you can traverse the tree following the binary space partition logic). Naturally organizes data for multi-scale algorithms (e.g., coarse registration at higher levels, then fine tune at leaves). Well-suited for mapping large 3D environments like multi-floor buildings, caves, outdoor terrains. Usage in SLAM systems:\nMany SLAM pipelines use octree maps for collision avoidance and planning, even if the SLAM algorithm itself uses point clouds for scan matching. For instance, after SLAM produces a point cloud, one might convert it to an OctoMap for a robot to navigate. Some SLAM algorithms integrate mapping and pose graph: after optimization, they can update an octree map globally. Example: A drone mapping a forest might use LiDAR SLAM to get its poses and a sparse map of tree trunks (features). Parallelly, it can build an octree occupancy map of the environment for path planning (ensuring it doesn’t hit branches). The octree will compress all those empty spaces and uniformly filled spaces (like ground plane) effectively.\nIn conclusion, octrees provide a powerful framework for managing 3D map data in SLAM. Whether it’s an occupancy grid or a feature map, octrees help scale SLAM to large environments by reducing memory and allowing quicker queries. Many state-of-the-art LiDAR SLAM systems (Cartographer 3D, BLAM, etc.) leverage octrees in some form for map representation, even if the front-end uses other methods. As 3D SLAM becomes more common (with autonomous cars and robots), octrees or similar spatial structures are becoming a standard component of the SLAM toolkit.\nKalman Filter (Dynamic and Measurement Models) The Kalman Filter (KF) is a state estimation algorithm that provides an optimal recursive solution (for linear systems with Gaussian noise) to combine a dynamic model (predicting state evolution) and measurement model (relating state to observations). In the context of SLAM, especially early approaches, the Extended Kalman Filter (EKF) was a prominent framework to estimate the joint state of robot pose and map.\nState, Dynamic Model, Measurement Model:\nThe filter maintains a state vector and a covariance matrix $P$ representing uncertainty. In full SLAM EKF, the state might include the robot pose and the positions of all landmarks (this gets very large, which is one EKF-SLAM issue). In a simpler localization EKF, the state is just the robot pose. The dynamic model (motion model) $x_{t} = f(x_{t-1}, u_t) + \\text{noise}$ propagates the state. For a nonlinear $f$, EKF linearizes it around the current estimate using the Jacobian $F$ (also called $A$ sometimes). The process noise covariance $Q$ represents uncertainty in this motion. The measurement model $z_{t} = h(x_t, m) + \\text{noise}$ relates state to expected observation. For example, if a landmark $j$ with position $(x_j, y_j)$ is in state, and the robot pose $(x, y, \\theta)$ is part of state, the measurement could be the bearing and range to that landmark: $h(x, m_j) =[ \\sqrt{(x_j-x)^2 + (y_j-y)^2}, \\text{atan2}(y_j-y, x_j-x) - \\theta ]$. This too is linearized (Jacobian $H$) for the EKF update. Measurement noise covariance $R$ is used here. EKF SLAM procedure: Each time step:\nPredict: Use motion $f$ to predict new state $\\hat{x}_{t|t-1}$ and propagate covariance $P_{t|t-1} = F_t P_{t-1|t-1} F_t^T + Q_t$. This expands uncertainty in the direction of motion uncertainty. Update: When a measurement $z_t$ arrives, compute expected measurement from predicted state $\\hat{z} = h(\\hat{x}_{t|t-1})$. Compute innovation $y = z_t - \\hat{z}$ and the Kalman gain $K = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}$. Then correct state: $x_{t|t} = \\hat{x}_{t|t-1} + K y$, and update covariance $P_{t|t} = (I - K H) P_{t|t-1}$. In EKF-SLAM, the state includes landmarks, so when a new landmark is first observed and recognized as new, it gets appended to the state with appropriate initialization and covariance (EKF can augment state). Each observation can update both robot and landmark parts of the state (correlations built up in P ensure a measurement of a landmark adjusts the robot pose and all correlated landmarks).\nThe Kalman Filter provides a mathematically elegant way to handle sensor fusion too: e.g., one can combine LiDAR and IMU by modeling IMU outputs as part of the measurement update or directly in the motion model. Unscented Kalman Filter (UKF) is an alternative to EKF that avoids Jacobians by using sigma points to propagate means and covariances; it’s useful for highly nonlinear systems.\nHowever, EKF-SLAM has limitations:\nComputational complexity is $O(n^2)$ with $n$ being state size (landmarks) due to covariance updates, making it hard to scale beyond a few hundreds of landmarks before it becomes slow. It assumes Gaussian distributions; a single mode, which is not always valid if data association is ambiguous. Linearization errors can cause inconsistency (over-confident estimates). Despite this, EKFs were historically successful on small-scale SLAM (like mapping a single room with some beacons). Many modern SLAM systems don’t use a monolithic EKF for full SLAM, but Kalman Filters are still used in local pose tracking and sensor fusion:\nRobot Localization: Often an EKF fuses wheel odometry, IMU, and maybe GPS to get a robust pose estimate (this is essentially an localization EKF, not mapping). ROS’s robot_localization package does this. Filtering-based SLAM: Some systems use an EKF or UKF for mapping sparse feature maps (e.g., ORB-SLAM3’s inertial version uses a filter for IMU integration between keyframes). Kalman Filter for LiDAR Odometry: One could use a KF to estimate the velocity of the robot such that the predicted motion aligns subsequent scans, instead of pure ICP. For instance, a constant velocity model plus ICP measurements can be fused in a KF framework. An example of a filter-based LiDAR SLAM is Kalman Filter SLAM in mapping with NDT: The state is the vehicle pose and velocity; motion from IMU is prediction; LiDAR scan matching (NDT providing relative pose) is the measurement update. This yields a smooth state estimate at high rates.\nSummary of dynamic vs measurement models:\nThe dynamic model introduces temporal coherence (tie between successive poses, as discussed in the Motion Model section). It’s typically relatively high certainty over short term (especially with wheel/IMU). The measurement model introduces environment feedback (tie between pose and map features, as in measurement association). It often has higher uncertainty initially (if you don’t know map well) but as map converges, measurements provide strong corrections. Kalman Filters assume both models are reasonably linear (or linearizable) and that noise is Gaussian. If LiDAR gives you a million points, you can’t put that directly into a KF; instead, you’d extract a few significant features or do a compression (like using submap match yields a relative pose measurement).\nOne notable approach is EKF Localization on an occupancy grid: known as the AMCL’s analog but in KF form (less common than particle filters, since the observation model of a full scan given a pose is highly nonlinear/non-Gaussian).\nEven though graph-based SLAM is more common now, understanding KF models is useful because:\nIt teaches the importance of models and uncertainty in SLAM. Many parts of a SLAM system can be designed using Kalman filter principles, then implemented with non-linear optimization. For instance, the information matrix in an EKF corresponds to the Hessian in graph optimization; the math lines up. In conclusion, Kalman Filter brings together the motion prediction and measurement update in a principled way. In SLAM, EKF was historically used to directly estimate map and pose. Now, its role is often in a reduced capacity (e.g., state estimation for odometry or sensor fusion) or conceptually in factor graphs. Nonetheless, the concepts of dynamic and measurement models, propagated and update uncertainties, are integral to all SLAM methods – even if solved via optimization, one can derive them from the same fundamentals that the Kalman Filter uses.\nPost-Processing and Loop Closure No matter how good the odometry (front-end) is, drift accumulates in any SLAM. Loop closure is the process of detecting when the robot has returned to a previously visited area (closing the loop) and correcting the map and trajectory to enforce the consistency of that loop. This is typically handled in the back-end optimization.\nLoop Closure Detection: The first step is to realize “we are in a known place.” There are various approaches:\nSpatial Proximity: If using GPS or known initial poses, one might guess a loop closure when the robot is physically near the start. But without external references, proximity must be established by the sensor data itself. Place Recognition (Scan Matching): For LiDAR, one way is to match the current scan against a database of past scans or submaps. Techniques like Scan Context create a descriptor (e.g., a ring image) of a LiDAR scan that is rotation-invariant; new scans are compared to stored descriptors to find potential matches (candidates for loop closure). Another approach is simply trying ICP alignment of the current scan with some previous keyframe scans (not all, but maybe every 10th or so) to see if a good fit (low error) can be found. This can be expensive, so usually a coarse pre-check is used (like comparing histograms or using some semantic clues). Visual Place Recognition: In a multi-sensor SLAM (camera+LiDAR), the camera could detect loop closure (using Bag-of-Words or deep learned features) and then that can trigger a LiDAR loop alignment. LVI-SAM does this: it uses a vision module to detect loops and then incorporate them for LiDAR+visual graph optimization. Map Matching: If a global map is being built incrementally (like a growing point cloud or submap), one can sometimes detect loop closure by aligning the current local map with the global map in a broad sense (e.g., using correlation on a grid map, or recognizing that certain structures line up). Once a loop candidate is detected, one typically uses ICP or a similar registration to get an accurate relative pose between the current pose and the previously visited pose (or map). For example, if we think we are back at the start, align the current scan with the map area around the start position to compute the transform (this gives the loop closure constraint).\nAdding Loop Constraints: Loop closures are added as additional constraints in the SLAM optimization problem. In a factor graph, this appears as an edge connecting the current pose node to an old pose node, with a certain relative transformation measurement (and uncertainty). For instance: pose_50 and pose_5 might get a loop closure factor saying “according to loop detection, pose_50 is at (dx, dy, dtheta) relative to pose_5 with some covariance”.\nBack-End Optimization (Pose Graph): Once added, we solve for the new set of poses that satisfies all constraints (odometry + loop). This often means doing a nonlinear least squares optimization (if using Gauss-Newton, Levenberg-Marquardt, or iSAM incremental smoothing). The outcome is that the poses will adjust, distributing the loop error across the trajectory:\nBefore loop closure, the robot’s estimated trajectory might not line up where it started (there’s a gap if you plot start vs end). After adding a loop closure and optimizing, the trajectory will deform so the end meets the start (closing the gap), and this deformation is spread over many poses so that no single jump is huge (minimizing squared error typically leads to distributing error evenly). In EKF-based SLAM, loop closure happens implicitly when a landmark observed earlier is seen again – the filter’s update will adjust the pose and landmark positions. In graph-based SLAM, it’s more explicit: you add an edge and solve. Graph-based methods are more flexible to handle many loop closures and large updates.\nExample: Cartographer builds submaps and when loop closure finds an overlap between a current scan and an older submap, it adds constraints and then runs a sparse pose graph optimization. This process can run in a background thread. Cartographer uses branch-and-bound to efficiently search for loop closure matches in submaps (to avoid brute force).\nLoop Closure Challenges: As mentioned, false loops are dangerous. If a wrong loop closure is added (thinking two different places are same), optimization can warp the map incorrectly. Many systems use outlier robust cost functions or check consistency of multiple loop closures before trusting them. Some maintain a “whitelist” or “blacklist” of loop closures.\nAfter Loop Closure – Map Correction: Once the poses are adjusted, the map built from those poses must also be corrected. If we maintain a global point cloud map, we can transform all points by the corrected poses. In practice, many SLAM systems don’t explicitly rebuild the entire map from scratch after every loop optimization (would be too slow), instead they often keep the map in pose-referenced form:\nIn a pose graph SLAM, each scan or submap is associated with a pose node. When that node moves in optimization, effectively the scan is now at a new location in the global frame. If needed, one can regenerate the global point cloud by fusing scans at their new poses. Some online SLAM systems apply corrections on the fly to recent poses (sliding window) but wait until the end or sparsely for big global updates. Graph SLAM vs EKF in loop closure: Graph SLAM tends to handle loop closure more gracefully because you’re optimizing a lot of variables together. EKF would just do one huge update that might add a lot of correlation and potentially inconsistency if the loop error was big. Graph methods allow a big residual to spread over the whole loop.\nLoop Closure vs Relocalization: Loop closure is often when still continuously tracking. If the robot got lost (kidnapped) and then recognizes a known place, that’s more of a relocalization event – similar effect (you add a constraint to a past pose or global map alignment), but in real-time system, you might have to essentially “teleport” the pose estimate to the correct one. Many SLAM systems integrate relocalization by detecting known features (e.g., a QR code or a distinctive geometry that was mapped before).\nPost-Processing: Sometimes, especially in mapping applications, loop closures and full optimizations are done offline (post-processing). The robot logs data, then later a big optimization is performed to build the final consistent map. However, in an online system (like a car driving), you need to close loops on the fly (or within seconds) to correct your path.\nA famous post-processing technique is Bundle Adjustment in visual SLAM (which is basically loop closing by optimizing all camera poses and points). In LiDAR SLAM, the analogous step is global pose graph optimization with all loop closures included.\nFinally, Map Refinement: After loop closure, maps can be further refined by dense methods if needed. For example, one could do a global ICP alignment using all poses as initial guess to fine-tune the point cloud map (this is like distributing the error not just on poses but adjusting points too). Some SLAM systems will also cull redundant points or apply consistency checks after big loop closures (e.g., remove duplicate map points that now overlap).\nTo illustrate, consider a robot driving around a block and returning close to start:\nInitially, due to drift, its map of the block might not perfectly close (the roads might overlap or gap slightly). Loop closure detects it’s back at start, adds a constraint to align end pose to start pose. Optimization shifts the intermediate poses slightly so that the road aligns perfectly end-to-start. The map (street edges, buildings) now aligns with itself; where there were double features (from start and end passes) they now coincide. Thus, post-processing (loop closure and optimization) turns a locally consistent map into a globally consistent map. It is an essential component for global accuracy. Without it, SLAM would degrade to dead-reckoning over long loops. Modern SLAM back-ends like Google’s Ceres Solver, GTSAM, or g2o are highly optimized to do these calculations quickly even for hundreds or thousands of pose variables and constraints.\nRecent Advances in SLAM SLAM is a very active field of research and development. In recent years, several emerging trends and technologies have shaped new SLAM approaches, especially for LiDAR-based SLAM. We highlight a few:\nIncorporating Deep Learning Deep Learning has started to influence SLAM in various ways:\nPlace Recognition \u0026 Loop Closure: CNN-based descriptors have been developed to recognize places from sensor data. For LiDAR, researchers have created learning-based scan descriptors (e.g., PointNetVLAD, MinkLoc3D) that can tell if two LiDAR scans are from the same place even under different viewpoints. These learned descriptors can improve loop closure detection robustness over handcrafted methods. For instance, a neural network might learn to emphasize stable features (building outlines, road layout) and ignore transient ones (cars, pedestrians) when generating a scan embedding. Feature Extraction: Instead of using fixed edge/plane detection heuristics (like LOAM does), some recent works train neural networks to extract distinctive features or keypoints from point clouds. The idea is similar to learning features in vision (like SuperPoint, etc.), but for 3D data. These learned features could be more reliable in unstructured environments. End-to-End SLAM: There are experimental end-to-end approaches where a deep network is trained to directly estimate odometry from sensor input (learning an approximate localization without explicit mapping). Examples on LiDAR include learning-based odometry networks (often using occupancy grid as input to a CNN that outputs pose change). While not yet as accurate as classical methods for LiDAR, they show potential to handle sensor noise or failures by learned priors. Neural Mapping: A very cutting-edge direction is representing maps with neural networks (implicit representations). For example, PIN-SLAM (2021) represents the map as an implicit field (like a neural network that can output if a point is on a surface) instead of point clouds. Each new scan updates a neural map rather than a voxel grid. This is related to Neural Radiance Fields (NeRF) but for LiDAR geometry. The promise is a continuous, compressed map that can be optimized with gradient descent. It’s computationally heavy though. Semantic SLAM: Deep learning is heavily used to add semantic understanding to SLAM. Networks can segment LiDAR scans into car, pedestrian, ground, building, etc. This semantic info can make SLAM more robust: e.g., dynamic objects can be identified and excluded from mapping, or semantic landmarks (like “parking sign on a pole”) can be used for loop closure because they are recognizable. Some SLAM systems now build semantic maps (with object labels) alongside geometric maps. Learning Localization Models: Given enough data, a network can learn to regress the pose of the vehicle given sensor input by recognizing the scene (this is like relocalization). Companies working on self-driving often train networks on LiDAR data for localization against a known map – effectively encoding the map in the network weights to get a fast localization system that is robust to partial observations. Overall, deep learning is complementing SLAM rather than replacing it. The strongest trend is using learning for perception tasks within SLAM: place recognition, feature extraction, segmentation. The core SLAM estimation (pose optimization) still relies on classical optimization or filtering, but learning helps provide better data association and loop candidates (which addresses some long-standing hard parts of SLAM).\nEdge Computing and Resource-Constrained SLAM As SLAM moves onto smaller devices (drones, AR glasses, IoT robots), there is a push for lightweight SLAM that can run on edge hardware with limited compute, memory, and power:\nAlgorithmic Optimization: Techniques like keyframe selection (processing only some frames), submap-based mapping, and sparsification of point clouds (like using feature points instead of full cloud) reduce load. KISS-ICP (2022) is an example of a recent LiDAR odometry that is extremely simple and efficient, yet as accurate as heavier methods, suitable for embedded use. Hardware Acceleration: There’s work on FPGA or ASIC implementations of SLAM components (scan matching, filter update). For example, building a dedicated accelerator for occupancy grid update or for nearest neighbor search in ICP can speed up SLAM on a robot with low-power chips. Streaming and Communication: In multi-robot or cloud-connected scenarios, sometimes the heavy lifting can be offloaded: e.g., a robot sends data to a more powerful computer (edge server) that runs the SLAM and sends back the pose/map updates. The term Edge SLAM can also imply distributing the SLAM computation between the robot and edge/cloud. This requires dealing with network latency and bandwidth limits (e.g., compressing LiDAR data). Map Compression: Storing a large map on an embedded device is challenging. Octrees help, but also people use map pruning (discarding parts of the map not needed or using lower detail for far away areas) or even procedural map generation (store boundaries of free space instead of every cell). Some research uses learning to compress maps (autoencoders for point clouds, etc.). Real-time Constraints: On edge devices, real-time performance is critical. This has led to development of constant-time SLAM algorithms (where complexity doesn’t grow over time). For example, some SLAM systems limit the number of active landmarks to a fixed number by replacing older ones, thereby keeping computation bounded. Others use sliding window optimization (only optimize the last N poses, not the whole trajectory, except when closing a loop). For industry engineers, an important recent development is that robust open-source SLAM is available that can run on modest hardware (e.g., Cartographer can run on a Raspberry Pi for 2D, and newer 3D methods like LIO-SAM can run on a GPU-equipped laptop in real-time). The emphasis now is on portability and efficiency so that SLAM can be deployed on everything from a tiny microcontroller to a large cloud server, depending on use-case.\nMulti-Agent and Distributed SLAM With the rise of multiple robots working together, Multi-Agent SLAM has gained attention:\nIn multi-agent SLAM, several robots explore an environment and need to build a common map and understand their relative poses. The challenge is that they start with unknown relative positions. Map Merging: When two robots meet or have overlap in view, they must recognize each other’s maps (data association across robots) and then merge the maps into one consistent global map. This is like a loop closure, but between two different maps. Techniques involve finding common landmarks or places (perhaps using global coordinates if available, or by direct communication of scans when in proximity). Distributed Optimization: Instead of sending all data to a central node, robots might each run their own SLAM and only exchange summarized information (like loop closure constraints between their coordinate frames). Distributed pose graph optimization algorithms exist that converge to the same result as a centralized one but through message passing between robots. Communication constraints: Multi-agent SLAM has to deal with limited bandwidth. Robots might share compressed maps or only share when they expect a loop closure. For example, a drone might broadcast a signature of its recent map; if another drone recognizes that signature from its own map, they then perform a more detailed exchange to align and merge. Use-cases: Multi-agent SLAM is crucial in search-and-rescue (teams of drones mapping a collapsed building), warehouse fleets (ensuring all robots agree on the world map), and autonomous driving fleets (sharing map updates). In 2020, for instance, DARPA’s Subterranean Challenge pushed multi-agent SLAM in caves/tunnels; teams developed systems where ground robots and drones would share mapping data to efficiently cover large areas. A concrete example: Assume two LiDAR-equipped rovers start at different ends of a mine. They each build a map. When their maps (frontiers) meet in the middle, they need to align. They could use a handshake procedure: when they detect each other via sensors or communication, they exchange some scans of the area. Running ICP between Rover A’s scan and Rover B’s scan finds the transform to align their coordinate frames. Then they merge maps and continue with one common map. This requires careful coordination to avoid double-counting overlapped areas and to handle any slight discrepancies.\nMulti-agent SLAM is essentially adding another layer of loop closure (between maps of different agents). Modern frameworks like LAMP (by NASA) or Cartographer have extensions for multi-robot mapping.\nRobustness and Resilience Robustness has been a recurring theme, but new approaches continue to improve SLAM’s ability to handle tough scenarios:\nRobust Estimation: Using robust loss functions in optimization (Huber loss, etc.) to reduce influence of outliers (like a false loop closure or a mis-matched feature). Some state-of-the-art systems automatically detect inconsistent loops and either reject or down-weight them (e.g., switchable constraints, dynamic covariance scaling). Dynamic Environments: As mentioned, integrating computer vision or AI to identify dynamic objects and remove them from consideration. Some LiDAR SLAM methods now include tracking of moving objects concurrently with SLAM – effectively doing SLAM in dynamic environments by treating moving objects separately (SLAM++ concept for rigid objects). Changing Environments (Long-term SLAM): Over weeks or months, environments change (furniture moves, seasons change foliage, construction happens). Recent research addresses how to update maps over time or localize in a map that’s somewhat outdated. One approach is meta-SLAM: maintain multiple map hypotheses or an ever-updating map. Another is to incorporate environment-change detection (if a known wall is suddenly gone, treat it as a change rather than a SLAM error). Failure Detection: New SLAM systems might monitor their own health (e.g., the covariance of the filter or the graph residual errors). If a sudden jump or inconsistency is detected, they can signal an alert or try to re-initialize. For example, if tracking loss occurs (too few features), a visual SLAM might switch to relocalization mode. Precision and Calibration: Advanced methods do online sensor calibration (extrinsics/intrinsics) during SLAM. For LiDAR-IMU SLAM, some algorithms can fine-tune the timing/extrinsic offset between LiDAR and IMU in situ to improve consistency. This makes the system more robust to slight miscalibrations that would otherwise degrade accuracy. High-Definition Maps \u0026 SLAM: In autonomous driving, often a prior HD map is available. SLAM in that case can be simplified to localization against a known map. But if the map is slightly wrong or outdated, the SLAM must be robust to those differences (not get confused by a missing sign or an extra temporary barrier). Combining SLAM (building a local map) with map-matching (aligning to a global map) is an area of development; it provides redundancy and robustness (if one fails, the other might carry on). Finally, benchmarking and datasets have pushed robustness: New datasets include adverse weather (rain, snow for LiDAR), aggressive motions, and diverse scenes. SLAM algorithms are evolving to handle these: e.g., using LiDAR returns intensity and number of returns to deal with fog, or combining radar with LiDAR for robust sensing in heavy rain (radar SLAM is an emerging field because radar can see through fog/rain better, albeit at lower resolution).\nIn summary, the SLAM landscape in 2025 is quite rich. We see classical geometric methods enhanced with learning (for perception), systems getting lighter and more deployable on small devices, multiple agents collaborating in mapping, and a focus on making SLAM work reliably in the real world’s messy conditions. Frameworks like LIO-SAMdemonstrate how integrating IMU, loop closures via scan context, and factor graph optimization yields a system that outperforms its predecessors. As compute power grows and algorithms mature, we can expect SLAM to become a standard component in most autonomous systems – often running in the background, robustly providing localization and mapping without much babysitting. The continued research into SLAM ensures that issues like scalability, robustness, and versatility will be incrementally solved, making truly autonomous exploration and operation feasible in the coming years.\nReferences [1] J. Lee and C. Hwang, “Lidar SLAM: The Ultimate Guide to Simultaneous Localization and Mapping,” Wevolver, Mar. 2023. [Online]. Available: https://www.wevolver.com/article/lidar-slam\n[2] C. Fernández-Caramés, V. Moreno, B. Curto, and F. J. Rodríguez, “A Review of Simultaneous Localization and Mapping for the Robotic-Based Nondestructive Evaluation of Infrastructures,” Sensors, vol. 25, no. 3, p. 712, Jan. 2025. [Online]. Available: https://www.mdpi.com/1424-8220/25/3/712\n[3] L. Tai, J. Zhang, M. Liu, and W. Burgard, “A Review of Simultaneous Localization and Mapping Algorithms,” World Electric Vehicle Journal, vol. 16, no. 2, p. 56, Feb. 2024. [Online]. Available: https://www.mdpi.com/2032-6653/16/2/56\n[4] H. Peng, J. Y. Huang, F. Shen, S. S. Song, and Y. Liu, “OMU: A Probabilistic 3D Occupancy Mapping Accelerator for Real-Time Applications,” arXiv preprint arXiv:2205.03325, May 2022. [Online]. Available: https://arxiv.org/pdf/2205.03325\n[5] S. Xu, H. Lin, and H. Zhang, “LIO-GVM: an Accurate, Tightly-Coupled Lidar-Inertial Odometry with Gaussian Voxel Map,” arXiv preprint arXiv:2306.17436v3, Jun. 2023. [Online]. Available: https://arxiv.org/html/2306.17436v3\n[6] S. Lee, C. Park, and H. Myung, “LiDAR-Based SLAM under Semantic Constraints in Dynamic Environments,” Remote Sensing, vol. 13, no. 18, p. 3651, Sep. 2021. [Online]. Available: https://www.mdpi.com/2072-4292/13/18/3651\n[7] X. Chen, A. Milioto, E. Palazzolo, P. Giguère, J. Behley, and C. Stachniss, “Stabilize an Unsupervised Feature Learning for LiDAR-Based Place Recognition,” Biorobotics Lab, CMU, 2018. [Online]. Available: http://biorobotics.ri.cmu.edu/papers/paperUploads/Xu_Stabilize_Unsupervised_2018.pdf\n[8] L. Pan, R. Hartley, M. Liu, and C. Stachniss, “PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation,” IEEE Transactions on Robotics, vol. 40, no. 2, pp. 1271-1289, Apr. 2024. [Online]. Available: https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/pan2024tro.pdf\n[9] A. Schaefer, L. Luft, and W. Burgard, “Robust Factor Graphs for Pose Graph SLAM,” Technical University of Chemnitz, 2022. [Online]. Available: https://www.tu-chemnitz.de/etit/proaut/en/research/robustslam.html\n[10] “Iterative closest point,” Wikipedia, 2024. [Online]. Available: https://en.wikipedia.org/wiki/Iterative_closest_point. [Accessed: Feb. 1, 2025]\n","wordCount":"14493","inLanguage":"en","image":"https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E","datePublished":"2025-02-20T00:00:00Z","dateModified":"2025-02-20T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-02-20-slam/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Simultaneous Localization and Mapping (SLAM)</h1><div class=post-description>A tutorial on simultaneous localization and mapping.</div><div class=post-meta><span title='2025-02-20 00:00:00 +0000 UTC'>February 20, 2025</span>&nbsp;·&nbsp;69 min&nbsp;·&nbsp;14493 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-to-slam aria-label="Introduction to SLAM">Introduction to SLAM</a></li><li><a href=#mapping aria-label=Mapping>Mapping</a></li><li><a href=#localization aria-label=Localization>Localization</a></li><li><a href=#applications-of-slam aria-label="Applications of SLAM">Applications of SLAM</a></li><li><a href=#challenges-in-slam aria-label="Challenges in SLAM">Challenges in SLAM</a></li><li><a href=#visual-slam-vs-lidar-slam aria-label="Visual SLAM vs. LiDAR SLAM">Visual SLAM vs. LiDAR SLAM</a></li><li><a href=#basics-of-lidar aria-label="Basics of LiDAR">Basics of LiDAR</a></li><li><a href=#mathematical-formulation-of-slam-mapping-localization-slam aria-label="Mathematical Formulation of SLAM (Mapping, Localization, SLAM)">Mathematical Formulation of SLAM (Mapping, Localization, SLAM)</a><ul><ul><li><a href=#mapping-1 aria-label=Mapping>Mapping</a></li><li><a href=#localization-1 aria-label=Localization>Localization</a></li><li><a href=#slam aria-label=SLAM>SLAM</a></li></ul></ul></li><li><a href=#motion-model-in-slam aria-label="Motion Model in SLAM">Motion Model in SLAM</a></li><li><a href=#measurement-association-in-slam aria-label="Measurement Association in SLAM">Measurement Association in SLAM</a><ul><li><a href=#iterative-closest-point-icp aria-label="Iterative Closest Point (ICP)">Iterative Closest Point (ICP)</a></li></ul></li><li><a href=#odometry-and-its-role-in-slam aria-label="Odometry and its Role in SLAM">Odometry and its Role in SLAM</a></li><li><a href=#octree-and-octree-of-features aria-label="Octree and Octree of Features">Octree and Octree of Features</a></li><li><a href=#kalman-filter-dynamic-and-measurement-models aria-label="Kalman Filter (Dynamic and Measurement Models)">Kalman Filter (Dynamic and Measurement Models)</a></li><li><a href=#post-processing-and-loop-closure aria-label="Post-Processing and Loop Closure">Post-Processing and Loop Closure</a></li><li><a href=#recent-advances-in-slam aria-label="Recent Advances in SLAM">Recent Advances in SLAM</a><ul><li><a href=#incorporating-deep-learning aria-label="Incorporating Deep Learning">Incorporating Deep Learning</a></li><li><a href=#edge-computing-and-resource-constrained-slam aria-label="Edge Computing and Resource-Constrained SLAM">Edge Computing and Resource-Constrained SLAM</a></li><li><a href=#multi-agent-and-distributed-slam aria-label="Multi-Agent and Distributed SLAM">Multi-Agent and Distributed SLAM</a></li><li><a href=#robustness-and-resilience aria-label="Robustness and Resilience">Robustness and Resilience</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In this post, we will introduce the basic concepts of SLAM and how SLAM is used in the autonomous driving system.</p><h1 id=introduction-to-slam>Introduction to SLAM<a hidden class=anchor aria-hidden=true href=#introduction-to-slam>#</a></h1><p>Simultaneous Localization and Mapping (SLAM) is the process by which a robot or vehicle builds a map of an unknown environment <strong>while simultaneously tracking its own position within that map</strong>. In essence, SLAM tackles two coupled problems: <strong>mapping</strong> (creating a spatial representation of the environment) and <strong>localization</strong> (estimating the device’s pose within that map) at the same time. This capability is fundamental for autonomous systems like self-driving cars, drones, and mobile robots operating in GPS-denied or unfamiliar areas. For example, an autonomous vacuum uses SLAM to avoid getting lost or missing spots, by localizing via wheel odometry and laser range data and mapping obstacles as it goes. Modern SLAM algorithms leverage various sensors – <strong>cameras, LiDAR, sonar, IMUs</strong> – to perceive the environment. This tutorial focuses on <strong>LiDAR-based SLAM</strong>, which uses laser scanning to produce accurate 3D point clouds of surroundings. LiDAR SLAM has gained popularity in industry due to its high accuracy and robustness under diverse lighting conditions (unlike cameras, LiDAR works in darkness). However, it also comes with challenges like higher cost, power, and difficulty sensing transparent objects. We will delve into the principles of SLAM, with emphasis on LiDAR, covering mapping and localization techniques, key mathematical models, algorithms like ICP, and recent advances. By the end, you should understand how systems such as autonomous vehicles build and use maps in real-time, and be aware of state-of-the-art frameworks (e.g. <strong>LOAM, LIO-SAM</strong>) that implement these concepts.</p><h1 id=mapping>Mapping<a hidden class=anchor aria-hidden=true href=#mapping>#</a></h1><p><strong>Mapping</strong> in SLAM refers to building a representation of the environment using sensor data. For LiDAR SLAM, the map is often a set of spatial points or surfaces collected from laser scans. Several map representation strategies are used in practice:</p><ul><li><strong>Point Cloud Maps:</strong> The simplest approach is accumulating raw 3D points (X, Y, Z) from LiDAR into a global point cloud. This gives a detailed geometrical map of surfaces (e.g. walls, floors, obstacles). Point cloud maps preserve rich detail but can grow large; hence downsampling or dividing into submaps is common.</li><li><strong>Occupancy Grids (Voxel Maps):</strong> Space can be discretized into a 2D or 3D grid where each cell (voxel) stores whether it’s occupied, free, or unknown. For 2D SLAM (e.g. floorplan mapping with a 2D lidar), occupancy grids are widely used. In 3D, <strong>Octree-based</strong> occupancy maps (like <strong>OctoMap</strong>) hierarchically subdivide space into voxels, which is memory-efficient. Each voxel may hold a probability of occupancy updated with sensor readings. Octrees allow multi-resolution mapping – storing fine detail where needed and coarser representation elsewhere – and efficient queries for collision checking or raycasting.</li><li><strong>Feature-Based Maps:</strong> Instead of storing every point, the map can consist of <strong>abstract features</strong> (landmarks) such as line segments, planar surfaces, or corner points. For example, the LOAM algorithm selects <strong>sharp edge points and planar patches</strong> from each scan to use as map features. By mapping only these salient features, it achieves a sparse but descriptive map that is faster to align new scans with. Feature maps are often used in <strong>graph-based SLAM</strong> as the nodes (landmarks) in the graph.</li><li><strong>Surfels and Gaussian Voxels:</strong> Advanced representations model local surface geometry. <strong>Surfels</strong> (surface elements) are disks with position, normal, and radius, often used in RGB-D SLAM for dense modeling. In LiDAR SLAM, a related idea is the <strong>Gaussian voxel</strong> map – each voxel stores a Gaussian distribution (mean and covariance) of points within it. This is used in algorithms like <strong>Normal Distributions Transform (NDT)</strong> mapping or recent methods like LIO-GVM. Representing points as a Gaussian yields a smooth surface approximation and allows faster, more robust scan matching (by matching a point to a voxel’s distribution rather than to individual points).</li></ul><p>Mapping is an ongoing process during SLAM. As the robot moves, new scans are taken and <strong>merged into the map</strong> in the estimated pose. If the pose estimate is off, the mapped features will be misaligned, which is why accurate localization is critical for mapping. Maps can be built incrementally (each new scan integrated as we go) or in batch (optimize robot trajectory then merge all data). For large-scale environments, maps may be divided into <strong>sub-maps or tiles</strong> to manage memory and computational load. The representation chosen impacts not only memory but also how we do scan matching and loop closure later. For instance, point clouds allow precise geometric matching (e.g. ICP), whereas occupancy grids are useful for path planning and have probabilistic integration. In practice, many SLAM systems use <strong>hybrid mapping</strong>: a point cloud for visualization and precision, plus an occupancy grid for navigation and collision avoidance.</p><h1 id=localization>Localization<a hidden class=anchor aria-hidden=true href=#localization>#</a></h1><p><strong>Localization</strong> is the task of estimating the robot’s pose (position and orientation) within a map. In the context of SLAM, localization must be done concurrently with mapping. The robot uses incoming sensor measurements (e.g. LiDAR scans) to <strong>figure out where it is relative to the map it’s building</strong>. Effective localization is what keeps the map consistent – if the pose estimate drifts, the mapped features will appear in the wrong place.</p><p>Early in operation, the map is sparse or empty, so the robot relies on <strong>odometry</strong> (dead reckoning from wheel encoders or IMU) or initial alignment to guess its movement. As the map grows, the robot can localize by <strong>matching new sensor data to the existing map</strong>. In LiDAR SLAM, this typically involves a scan-matching algorithm: compare the new LiDAR scan to the map (or recent scans) to find the pose that best aligns it. Techniques like ICP (introduced later) are widely used for this purpose – essentially solving, “How must I move to make this scan overlap with the map points?” On a high level, localization can be seen as an <strong>optimization problem</strong>: find the pose that maximizes the likelihood of the observed sensor data given the current map.</p><p>Common localization approaches include:</p><ul><li><strong>Particle Filters (Monte Carlo Localization):</strong> Maintain a set of pose hypotheses (particles) and update them based on how well the expected scan from that pose matches the actual scan. This is used in 2D LiDAR localization on a known map (AMCL in ROS). In SLAM (unknown map), particle filters can be extended (as in <strong>Rao-Blackwellized Particle Filters</strong>) where each particle carries its own map; <strong>GMapping</strong> is a classic example using this for 2D grid mapping.</li><li><strong>Kalman Filter / Extended Kalman Filter (EKF):</strong> If we assume unimodal Gaussian uncertainty, we can use an EKF to continuously estimate the robot’s pose. The filter uses a motion model to predict pose and a measurement model to correct it by matching features or scan data. EKFs were popular in early landmark-based SLAM: the state vector includes the robot pose and landmark positions, and the filter updates both. We will discuss the dynamic and measurement models in detail later. EKF localization can also be done with a pre-built map: e.g. treat nearest map features as measurements. The limitation is that a single Gaussian may not capture multi-hypothesis uncertainty (e.g. entering a symmetric corridor, there might be two possible locations that match the sensor data).</li><li><strong>Pose Graph Optimization:</strong> Modern SLAM often uses a pose graph approach where each robot pose is a node in a graph, and localization is achieved by optimizing this graph with relative pose constraints. In effect, the “localization” at each step is achieved by <strong>incremental alignment</strong> (odometry constraint from scan matching) and later <strong>global adjustment</strong> when loop closures are added. This falls under smoothing rather than filtering, but it accomplishes the same goal of determining poses. We’ll touch on this in loop closure.</li></ul><p>In LiDAR SLAM, localization accuracy is usually very high for short distances – a new scan typically overlaps heavily with the last scan, so scan-to-scan matching yields millimeter to centimeter accuracy locally. The bigger challenge is long-term drift: errors accumulate over time. That’s where loop closures and global optimization (later sections) come in to re-localize the robot in the global frame and correct any drift. Also, fusing other sensors (GPS when available, or an IMU for orientation) can help maintain a good pose estimate. In summary, <strong>mapping</strong> and <strong>localization</strong> are two sides of SLAM’s coin – the map helps us localize, and good localization yields a better map.</p><h1 id=applications-of-slam>Applications of SLAM<a hidden class=anchor aria-hidden=true href=#applications-of-slam>#</a></h1><p>SLAM algorithms have become crucial in a range of engineering and industrial applications. Here are a few notable areas where SLAM (and particularly LiDAR SLAM) is applied:</p><p><em>Figure:</em> Conceptual illustration of autonomous vehicles using SLAM. Multiple self-driving cars scan their surroundings with LiDAR/radar to build maps and localize (blue shaded areas indicate sensor coverage). SLAM allows each vehicle to understand its environment and position, enabling safe navigation in traffic.</p><ul><li><strong>Autonomous Vehicles:</strong> Self-driving cars and other autonomous ground vehicles use SLAM to navigate roads without GPS reliability issues (e.g., in tunnels or urban canyons). A LiDAR-based SLAM system provides a continuously updated 3D map of surrounding vehicles, lanes, and obstacles and a precise localization for the car. High-definition 3D maps can also be built on the fly or refined in real-time. Notably, projects like Waymo’s and Uber’s use LiDAR SLAM for real-time perception and localization within prior maps.</li><li><strong>Mobile Robotics:</strong> Indoor service robots (warehouse AGVs, delivery robots, cleaning robots) rely on SLAM to move around unknown or dynamic environments. For instance, a warehouse robot with a 2D lidar builds an occupancy grid of the facility and tracks its pose to navigate between shelves. SLAM enables such robots to operate <strong>autonomously</strong> – planning paths on the constructed map and updating it if the environment changes (new obstacles, moved furniture).</li><li><strong>Aerial and Marine Drones:</strong> UAVs and UGVs (unmanned ground vehicles) in environments like mines, forests, or oceans use SLAM when GPS is denied or insufficiently precise. LiDAR SLAM is common on drones doing indoor mapping or terrain mapping (e.g., a drone scanning inside a building after a disaster). It allows the drone to both chart the 3D structure and know where it is to avoid collisions. Underwater robots use sonar-based SLAM similarly.</li><li><strong>Mapping and Surveying:</strong> LiDAR SLAM is widely used for mapping infrastructure. Handheld or vehicle-mounted LiDAR SLAM devices scan building interiors, caves, or industrial plants to create 3D reconstructions. The <strong>LOAM</strong> algorithm (Lidar Odometry and Mapping) and its variants are often used in commercial 3D mapping scanners to produce point cloud maps of factories, construction sites, or forests in real-time as an operator walks around.</li><li><strong>Augmented Reality (AR) and VR:</strong> Although primarily using visual SLAM, some AR systems use depth sensors (LIDAR on newer iPhones/iPads, for example) to improve SLAM in low-texture environments. This is a cross-modal application where LiDAR aids in mapping the room geometry so virtual objects can be placed consistently.</li><li><strong>Industrial Automation:</strong> Robots in domains like mining or agriculture use SLAM on heavy machinery for autonomous operation. SLAM helps a mining vehicle navigate an underground tunnel network, or a self-driving tractor to map a field’s rows. The robustness of LiDAR SLAM to lighting makes it well-suited for outdoor and subterranean scenarios where vision might fail (e.g., glare, darkness).</li></ul><p>These examples demonstrate why SLAM is a foundational technology in robotics and autonomy. It <strong>saves cost and time</strong> by allowing machines to operate without prior maps or external localization systems. As computing power and sensor technology have improved, SLAM has moved from research labs into <strong>real-world deployments</strong>, handling larger environments and running on real-time systems.</p><h1 id=challenges-in-slam>Challenges in SLAM<a hidden class=anchor aria-hidden=true href=#challenges-in-slam>#</a></h1><p>Despite significant progress, SLAM remains a challenging problem in practice. Some of the key challenges and limitations include:</p><ul><li><p><strong>Fast Ego Motion:</strong> When the vehicle or robot moves at high speeds, several problems emerge. Scan matching becomes difficult as there is less overlap between consecutive LiDAR scans. Motion distortion within a single scan is more severe, requiring sophisticated deskewing algorithms. High-speed movement also increases measurement uncertainty, as small timing errors result in larger positional errors. For autonomous vehicles traveling at highway speeds (>100 km/h), maintaining accurate SLAM becomes particularly challenging, requiring higher sensor rates and better motion prediction models to compensate for the rapid displacement between measurements.</p></li><li><p><strong>Large-Scale Mapping:</strong> Building and maintaining maps of very large areas (e.g., entire cities or highway networks) introduces significant computational and memory challenges. As the map grows, pose graph optimization becomes increasingly expensive, loop closure detection must search through more candidates, and memory requirements for storing dense point clouds become prohibitive. Techniques like hierarchical mapping, submapping, and efficient data structures (octrees, sparse voxel grids) help address these issues, but large-scale SLAM remains difficult, especially when real-time performance is required on resource-constrained platforms.</p></li><li><p><strong>Highly Repetitive Environments:</strong> Environments with repetitive structures (such as office corridors, parking garages with identical sections, or forests with similar-looking trees) pose a significant challenge for loop closure detection. The perceptual aliasing problem occurs when different places look nearly identical to sensors, causing false loop closures that can catastrophically corrupt the map. SLAM systems struggle to distinguish between truly revisited locations and just similar-looking new areas. This often requires additional context (sequence of observations) or multi-modal sensing (combining LiDAR with visual features) to resolve ambiguities.</p></li><li><p><strong>Sensor Limitations:</strong> Each sensor has quirks. LiDAR provides accurate range data, but it struggles with glass or shiny surfaces (laser beams may pass through or reflect unpredictably). It also can be heavy and power-hungry, and its data rate is limited. <strong>Calibration</strong> of sensors is crucial; a mis-calibrated LiDAR (e.g., incorrect vertical angle offsets) can distort the map. Moreover, sensors produce noise – e.g., range noise in LiDAR – which the SLAM algorithm must accommodate.</p></li><li><p><strong>Odometry Drift:</strong> The dead-reckoning (integrating motion over time) invariably accumulates error. Wheel odometry slips or skids, and IMUs have bias drift. Even LiDAR odometry (scan-to-scan matching) can accumulate error if the environment has repetitive structure. This drift means that over long runs, the pose estimate can be substantially off if not corrected, causing <strong>map deformation</strong> (walls that should be straight appear bent or misaligned).</p></li><li><p><strong>Computational Complexity:</strong> SLAM algorithms can be computationally intensive, especially with high-resolution sensors. A 3D LiDAR can produce hundreds of thousands of points per second, and real-time SLAM must process this stream on an embedded computer. Algorithms like ICP are <strong>O(n·log n)</strong> or worse for each iteration (with n points). Large-scale maps also mean large optimization problems for loop closure. Efficiency techniques (sparsification, submapping, multi-threading, hardware acceleration) are essential to keep up. Memory usage is another aspect – storing millions of points or a dense grid can exhaust memory if not managed (hence octree compression, etc.). SLAM implementations must balance accuracy with real-time performance constraints.</p></li><li><p><strong>Dynamic Environments:</strong> Most SLAM methods assume a mostly static world. <strong>Moving objects</strong> (people, vehicles, pets) can cause false associations or ghost artifacts in the map. For instance, a pedestrian that appears in one scan but not the next might be mistaken for a static obstacle, confusing the mapping process. Dynamic environments violate the static assumption and can throw off localization (the robot might try to match a car that’s no longer there). Solutions involve detecting moving objects (via clustering and tracking or using machine learning to identify people, etc.) and <strong>filtering them out</strong> of the SLAM update. Long-term changes (doors opening/closing, furniture moved) also pose a challenge for map accuracy over time.</p></li><li><p><strong>Loop Closure Detection:</strong> Detecting that the robot has returned to a previously visited area (closing a loop) is non-trivial. The robot’s odometry estimate may be off by a large amount by the time it returns, so recognizing a familiar place from sensor data can be like finding a needle in a haystack. In LiDAR SLAM, loop closure often involves comparing the current scan or submap with a library of past scans/submaps to find a match. This can be computationally heavy (comparing against many candidates) and prone to false positives/negatives. A false positive loop closure (wrongly matching two different places) can severely corrupt the map if accepted. Techniques like <strong>Scan Context</strong> (which creates a rotationally-invariant descriptor of a LiDAR scan) or <strong>FPFH features</strong> help make loop detection faster and more reliable. Still, ensuring robust loop closures is a core challenge.</p></li><li><p><strong>Global Consistency and Optimization:</strong> Even after detecting loop closures, adjusting the entire map and trajectory to be consistent is complex. Large pose graph optimizations can be slow if not done incrementally. Ensuring the optimization converges to the right solution (and not a local minimum) often requires good initialization. Outlier loop closures need to be identified and removed (outlier rejection strategies like using RANSAC on loop constraints or switchable constraints are used). Maintaining <strong>real-time performance</strong> while doing these global corrections (sometimes called “pose graph relaxation”) is tough, especially on resource-constrained platforms.</p></li><li><p><strong>Robustness and Reliability:</strong> SLAM systems have to be robust to a variety of conditions: lighting changes (for visual SLAM), weather (rain or dust can introduce noisy LiDAR readings), rough handling (vibrations affecting sensors), etc. Ensuring the SLAM doesn’t fail catastrophically (loss of tracking) is vital in safety-critical applications. The <strong>“kidnapped robot”</strong> problem, where a robot is moved unbeknownst to it (or loses track and needs to relocalize), is a classical challenge. Modern SLAM systems incorporate <strong>relocalization</strong> modules to recover if tracking is lost, but doing this reliably is still an open problem.</p></li><li><p><strong>High-Level Understanding:</strong> Traditional SLAM builds geometric maps, but for true autonomy, robots also need to understand <strong>semantic context</strong> (knowing objects, room types, etc.). Integrating <strong>semantic information</strong> (e.g., recognizing a dynamic object as a person and ignoring it for mapping) is challenging but necessary for robustness in human environments. This blends SLAM with AI perception and is an active area of research.</p></li></ul><p>Many of these challenges are active research topics, and incremental improvements are released regularly. For instance, the <strong>LIO-SAM</strong> system (2020) demonstrated robust performance by tightly coupling IMU and LiDAR, but it is sensitive to sensor calibration and configuration. The list above (sensor issues, drift, computation, dynamics, loop closure, etc.) highlights why building a <em>fully reliable</em> SLAM system is difficult. In practice, engineers must tune SLAM systems to the expected environment and use-case, and often fuse multiple approaches (and sensors) to mitigate individual weaknesses.</p><h1 id=visual-slam-vs-lidar-slam>Visual SLAM vs. LiDAR SLAM<a hidden class=anchor aria-hidden=true href=#visual-slam-vs-lidar-slam>#</a></h1><p><strong>Visual SLAM</strong> and <strong>LiDAR SLAM</strong> differ primarily in the sensor data used, which affects the algorithm design, strengths, and weaknesses:</p><ul><li><strong>LiDAR SLAM:</strong> Uses active laser scanning to directly measure distances to surfaces, producing precise 3D (or 2D) point clouds. LiDAR SLAM excels in accuracy and robustness in various conditions. It does not depend on ambient light or texture; a LiDAR can map in complete darkness or in feature-poor environments (like white walls) where cameras would struggle. LiDAR measurements have actual scale (meters) with high precision, leading to highly accurate localization (often within centimeters). This is why LiDAR-based methods generally out-perform vision in localization accuracy. However, LiDAR sensors are relatively expensive and bulkier. They also have a limited angular resolution and can produce a lot of data to process. A key limitation is that LiDAR cannot see glass or transparent surfaces (the beams pass through or refract) and can be confused by rain or fog (returns from particles). LiDAR SLAM systems also consume more power and processing due to the dense point clouds. Another consideration: LiDAR provides geometry but no color or texture information, which means recognizing places purely from geometry can be hard if environments are similar in shape.</li><li><strong>Visual SLAM:</strong> Uses passive cameras (monocular, stereo, or RGB-D) to infer motion and structure from imagery. Visual SLAM benefits from cheap, lightweight sensors (cameras) and can leverage rich appearance information (textures, colors, identifiable objects). It’s great for <strong>recognizing landmarks</strong> – e.g., a camera can recognize a previously seen painting on a wall, aiding loop closure. Visual SLAM algorithms (like ORB-SLAM2, VINS-Fusion) often detect and track <strong>feature points</strong> (corners, blobs) in images across frames. They can be very lightweight to run on mobile devices (ARKit/ARCore on smartphones). However, cameras struggle with <strong>lighting changes</strong> (night vs day, or entering a dark room) and <strong>lack direct depth</strong>: a single camera SLAM has scale ambiguity (it might not know if it moved 1m or 2m if everything just looks scaled). Depth can be obtained via stereo or depth sensors, but then calibration and noise come into play. Visual SLAM can also fail if the scene has few visual features (e.g., a blank wall or uniform texture floor). It is generally less robust in feature-poor or dynamically lit environments compared to LiDAR. Furthermore, visual methods may have higher drift in pure odometry due to scale uncertainty and narrow field of view (depending on camera lens).</li></ul><p><strong>Hybrid approaches:</strong> Because of complementary strengths, many systems fuse camera and LiDAR (and IMU). For example, <strong>LVI-SAM</strong> (2021) is a system that combines LIO-SAM (LiDAR-Inertial) with VINS-Mono (Visual-Inertial) to get the benefits of both. The camera can provide texture cues for loop closure (e.g., recognizing a previously seen sign), and LiDAR provides accurate structure and scale. Such systems can even survive if one sensor fails (e.g., in a dark featureless tunnel, the LiDAR carries SLAM; in heavy smoke where the LiDAR might get noisy but some visual cues exist, the camera helps).</p><p>In summary, <strong>LiDAR SLAM</strong> is preferred for applications needing reliable, precise mapping in 3D (like autonomous driving, high-end robotics), especially outdoors or in large spaces. <strong>Visual SLAM</strong> shines in cost-sensitive or size-constrained applications (AR devices, small robots) and can provide semantic-rich mapping (recognizing objects in the scene). Many real-world solutions use a combination: LiDAR for geometry + camera for semantics, achieving the best of both. The choice also depends on environment: indoors, sometimes cameras plus a depth sensor suffice; outdoors in varying light, LiDAR might be indispensable. Industry engineers must weigh these trade-offs when selecting a SLAM solution, often opting for multi-sensor SLAM to maximize robustness.</p><h1 id=basics-of-lidar>Basics of LiDAR<a hidden class=anchor aria-hidden=true href=#basics-of-lidar>#</a></h1><p>To appreciate LiDAR-based SLAM, one must understand the basics of the LiDAR sensor itself. <strong>LiDAR</strong> (Light Detection and Ranging) emits laser pulses and measures the time it takes for each pulse to bounce off an object and return to the sensor. Given the speed of light, the round-trip time directly gives the distance to the object. By firing pulses in many directions, a LiDAR constructs a 3D “point cloud” of its surroundings.</p><p>Key characteristics of LiDAR sensors:</p><ul><li><strong>Scanning Mechanism:</strong> Many LiDARs (like the popular Velodyne 16, 32, 64 channel models) use multiple laser diodes arranged in a vertical stack that spin 360° horizontally. Each laser fires rapidly as it spins, yielding a set of elevation angles and a continuous azimuth rotation. This produces a 360° horizontal field of view and a limited vertical FOV (e.g., 30° spread for a 16-beam sensor). The result is a dense ring of points at various elevations for each 360° sweep (often called a <strong>scan</strong> or a <strong>sweep</strong>). Newer solid-state LiDARs use MEMS mirrors or optical phased arrays to steer the beam without mechanical rotation; their patterns can be raster scan or other shapes, but the end result is still a point cloud of distances.</li><li><strong>Output Data:</strong> A LiDAR typically outputs a list of points (x, y, z in the LiDAR’s coordinate frame) or in polar form (distance, vertical angle, horizontal angle). Many also provide <strong>intensity</strong> (strength of return) which can sometimes aid in recognizing reflectivity of surfaces. A 3D LiDAR like a 64-beam might output ~100k to 1.3 million points per second. A 2D LiDAR (single plane) will output a few tens of thousands of points per second in a flat plane (useful for 2D SLAM like in robotics).</li><li><strong>Accuracy and Range:</strong> LiDAR range measurements are very precise (often ±2 cm or better) up to long ranges. High-end units can see 100-200 meters in good conditions, which is great for outdoor mapping. However, maximum range can drop in rain or fog due to scattering. The angular resolution (spacing between beams or between successive shots) dictates the density of the point cloud – common resolutions are 0.1° to 0.4°. Closer objects yield more returns (they intersect more beams) whereas distant small objects might slip through gaps in the scan.</li><li><strong>Data Rate and Coordination:</strong> A spinning LiDAR produces data in a continuous stream. Often one full revolution (say 10 Hz or 20 Hz) is considered one scan. <em>Motion Distortion:</em> If the LiDAR or platform moves during a scan (which it always does on a moving robot), points in one scan are captured at different times. This means the point cloud is slightly “smeared” by the motion. SLAM systems must address this by <strong>deskewing</strong> – using the robot’s rotation rate or IMU data to correct point positions within a scan. For example, if the vehicle rotated a bit while a scan was captured, the resulting cloud can be rotated back to a common time. LOAM performs such deskewing assuming constant velocity during the sweep, and LIO-SAM uses IMU integration to undistort scans in its pre-processing step.</li><li><strong>Coordinate Frames:</strong> LiDAR data usually starts in the LiDAR’s own frame. For SLAM we define a world frame (map frame) and the LiDAR’s pose in that frame. As SLAM estimates the pose, each point can be transformed into the world frame to build the map.</li></ul><p>From a SLAM perspective, LiDAR provides a very <strong>direct measurement of environment geometry</strong>. Unlike a camera that gives pixels one must interpret, LiDAR gives a set of 3D points on surfaces. This simplifies the mapping problem (no need to triangulate depth as in stereo vision) and makes data association more geometric. However, the large number of points means one must use efficient spatial data structures (e.g., k-d trees for nearest neighbor search, octrees for map storage) to make use of the data in real-time. Also, LiDAR’s lack of semantic info means SLAM might integrate <em>anything</em> it sees (including dynamic objects) unless steps are taken to filter those.</p><p>In summary, LiDAR is a powerful sensor for SLAM because it provides accurate and rich spatial information. Its usage comes with engineering considerations like handling large data throughput, compensating for motion distortion, and dealing with environments where certain materials might not reflect lasers well. The rest of this tutorial will leverage these LiDAR properties as we discuss SLAM algorithms specialized for them.</p><h1 id=mathematical-formulation-of-slam-mapping-localization-slam>Mathematical Formulation of SLAM (Mapping, Localization, SLAM)<a hidden class=anchor aria-hidden=true href=#mathematical-formulation-of-slam-mapping-localization-slam>#</a></h1><h3 id=mapping-1>Mapping<a hidden class=anchor aria-hidden=true href=#mapping-1>#</a></h3><p>Given robot state trajectory $\mathbf{x}_{0:T}$ and sensor measurements $\mathbf{z}_{0:T}$ with observation model $h$, build a map $\mathbf{m}$ of the environment</p>$$
\min_{\mathbf{m}} \sum_{t=0}^{T} \|\mathbf{z}_t - h(\mathbf{x}_t, \mathbf{m})\|_2^2
$$<h3 id=localization-1>Localization<a hidden class=anchor aria-hidden=true href=#localization-1>#</a></h3><p>Given a map $\mathbf{m}$ of the environment, sensor measurements $\mathbf{z}_{0:T}$ with observation model h, and control inputs $\mathbf{u}_{0:T-1}$ with <strong>motion model f</strong>, estimate the robot state trajectory $\mathbf{x}_{0:T}$</p>$$
\min_{\mathbf{x}_{0:T}} \sum_{t=0}^{T} \|\mathbf{z}_t - h(\mathbf{x}_t, \mathbf{m})\|_2^2 + \sum_{t=0}^{T-1} \|\mathbf{x}_{t+1} - f(\mathbf{x}_t, \mathbf{u}_t)\|_2^2
$$<h3 id=slam>SLAM<a hidden class=anchor aria-hidden=true href=#slam>#</a></h3><p>Given initial robot state $\mathbf{x}_0$, sensor measurements $\mathbf{z}_{1:T}$ with observation model $h$, and control inputs $\mathbf{u}_{0:T-1}$ with motion model $f$, estimate the robot state trajectory $\mathbf{x}_{1:T}$ and build a map $\mathbf{m}$</p>$$
\min_{\mathbf{x}_{1:T}, \mathbf{m}} \sum_{t=1}^{T} \|z_t - h(x_t, \mathbf{m})\|_2^2 + \sum_{t=0}^{T-1} \|x_{t+1} - f(x_t, u_t)\|_2^2
$$<p>To formally understand SLAM, it helps to express it as an estimation problem. We can denote:</p><ul><li>The robot’s pose at time $t$ as $x_t$ (this could be a 2D pose $[x, y, \theta]$ or 3D pose $[x, y, z, roll, pitch, yaw]$).</li><li>The map of the environment as $m$ (this could be a set of landmarks or an occupancy grid or point cloud – basically all parameters describing the map).</li><li>The control inputs (odometry or movement commands) from time $t-1$ to $t$ as $u_t$.</li><li>The observations (sensor measurements) at time $t$ as $z_t$ (e.g., a LiDAR scan).</li></ul><p><strong>Mapping-only problem:</strong> If the robot’s trajectory $x_{1:t}$ were known (e.g., via very precise GPS or motion capture), the mapping task is to compute $m$ given all observations. This reduces to <strong>sensor fusion</strong>: integrate all sensor data in a common frame to build the map. With known poses, mapping is straightforward – just place each LiDAR point in the global frame and accumulate (with perhaps probabilistic updates for occupancy). This is sometimes called the “mapping with known poses” problem. There isn’t much uncertainty here except sensor noise, which can be handled by filtering (e.g., averaging multiple measurements per cell). In probabilistic terms, $P(m \mid x_{1:T}, z_{1:T})$ is the quantity of interest (the map posterior given the true poses and measurements). Many mapping algorithms assume known pose (like building a map from a logged trajectory).</p><p><strong>Localization-only problem:</strong> If a map $m$ is known (say we have a prior map of the building), localization means computing $x_t$ given the sensor observations and the map. Essentially we want $P(x_t \mid m, z_{1:t}, u_{1:t})$. This is the classic <em>localization</em> or tracking problem, solved by methods like the Kalman Filter or particle filter. The robot uses the known map as a reference to match its current sensors and find its pose. This is easier than full SLAM because the map is fixed (the problem is only in 6 DOF or so of the pose, not in potentially hundreds of map variables). Monte Carlo Localization (MCL) and Extended Kalman Filter localization are examples of solving this. In localization mode, the map doesn’t change, only our belief of where we are on that map is updated.</p><p><strong>Full SLAM problem:</strong> Both the trajectory $x_{1:T}$ and the map $m$ are unknown and must be estimated <strong>simultaneously</strong> from all observations $z_{1:T}$ and controls $u_{1:T}$. We seek the posterior $P(x_{1:T}, m \mid z_{1:T}, u_{1:T})$. This joint estimation is what makes SLAM hard – the robot doesn’t know the map a priori, and it needs the map to localize and vice versa. In practice, we often factor this problem or approximate it:</p><ul><li>One approach (online SLAM) is to estimate the <strong>current pose and map</strong> incrementally (this is what filtering methods do, maintaining $P(x_t, m \mid z_{1:t}, u_{1:t})$ and updating with each new measurement).</li><li>Another approach (full smoothing) is to estimate the <strong>entire trajectory and map</strong> as one big optimization (this is what graph-SLAM does, maintaining a factor graph over all poses and landmarks).</li></ul><p>A common probabilistic formulation is through the Bayes filter (recursive estimation). At each step:</p><ol><li><strong>Prediction (Motion Update):</strong> Use the motion model $P(x_t \mid x_{t-1}, u_t)$ to predict the new pose distribution from the previous pose. This model encapsulates how we think the state changes given control input (or odometry). For instance, if the robot moved forward 1m with some uncertainty, we predict its pose accordingly. In full SLAM, we also carry along the map (which typically doesn’t change in prediction, since map is static).</li><li><strong>Correction (Measurement Update):</strong> Incorporate the observation $z_t$ via the measurement model $P(z_t \mid x_t, m)$. This tells us how likely a measurement is given a pose and map. Inversely, it helps adjust the belief of $x_t$ (and potentially $m$) to better explain the actual measurement. For example, if the LiDAR expected to see a wall 1m away but actually saw nothing, the pose (or map) estimate might be off and is corrected.</li></ol><p>In an EKF SLAM setting, for instance, the state vector is $[x, m]$ and the above is done with linearization: the motion model $f(x_{t-1}, u_t)$ linearized gives a predicted mean and covariance, then the measurement model $h(x_t, m)$ linearized provides a Kalman gain for update. In a particle filter SLAM (like FastSLAM), the prediction moves particles according to the motion model, and the update weights particles by likelihood of observations given the map that particle has.</p><p>Another useful view is the <strong>graphical model</strong> of SLAM: we can construct a factor graph where:</p><ul><li>Pose variables $x_1, x_2, ..., x_T$ are connected by factors from odometry (each $u_t$ induces a factor between $x_{t-1}$ and $x_t$ encoding the motion constraint).</li><li>Each observation connects pose $x_t$ to some part of the map $m$ (if landmarks, a factor between $x_t$ and landmark $l_i$; if dense map, effectively a factor between $x_t$ and the map surface measured). Loop closures appear as additional measurement factors connecting a later pose $x_j$ back to an earlier pose or landmark observed previously.
Solving SLAM then is performing inference on this factor graph – basically finding the configuration of all pose and map variables that best satisfies all these constraints (maximum a posteriori estimation). Modern SLAM back-ends use this formulation: <strong>bundle adjustment</strong> in visual SLAM is essentially graph optimization on camera poses and feature points; pose graph optimization in LiDAR SLAM optimizes the chain of poses with loop closure constraints for consistency.</li></ul><p><strong>Mapping vs Localization vs SLAM formulation:</strong> In summary, mapping alone or localization alone are relatively simpler sub-problems with established solutions (integration of measurements for mapping, Bayes filter for localization). SLAM is harder because it must tackle both with coupled uncertainty. The mathematical formulation as a joint probability problem guides how we design algorithms:</p><ul><li>If we assume Gaussian distributions and linearize models, we get the EKF solution (with a big covariance matrix over pose and landmarks).</li><li>If we allow multiple hypotheses, we get particle solutions like FastSLAM (which factor the posterior into a product of a trajectory posterior and landmark posteriors).</li><li>If we optimize globally, we set up a nonlinear least squares problem (graph-SLAM) that is solved by iterative methods (like Levenberg-Marquardt or Gauss-Newton on the factor graph).</li></ul><p>Each approach makes certain assumptions for tractability. In the next sections, we’ll discuss the components that feed into these formulations: the <strong>motion model</strong> (how we predict poses), the <strong>measurement model and data association</strong> (how we align sensor data to map), and specific algorithms like ICP that implement these under the hood.</p><h1 id=motion-model-in-slam>Motion Model in SLAM<a hidden class=anchor aria-hidden=true href=#motion-model-in-slam>#</a></h1><p>The <strong>motion model</strong> (or dynamic model) predicts the robot’s next state based on its previous state and control inputs. It encapsulates our understanding of how the robot moves. A good motion model is crucial for SLAM because it provides the <strong>prior</strong> before observing new sensor data. In probabilistic terms, it’s $P(x_t \mid x_{t-1}, u_t)$.</p><p>Common motion models in robotics:</p><ul><li><strong>Differential Drive Kinematics (for wheeled robots):</strong> For a robot with wheel encoders, the control input might be the distance traveled $\Delta d$ and rotation $\Delta \theta\$ since last update. The motion model (in 2D) could be:</li></ul>$$x_t = x_{t-1} + \Delta d \cos(\theta_{t-1})$$$$y_t = y_{t-1} + \Delta d \sin(\theta_{t-1})$$$$\theta_t = \theta_{t-1} + \Delta \theta,$$<p>plus some noise in each term (to account for wheel slip or uneven terrain). This is essentially what’s used in typical mobile robot odometry. The noise can be modeled as Gaussian with variance proportional to $\Delta d$ and $\Delta \theta$ (e.g. uncertainty grows with distance traveled).</p><ul><li><strong>Ackermann or Car Model:</strong> For car-like vehicles, controls might be steering angle and speed. The motion model involves turning radius and tends to be more constrained (cannot turn on spot). Still, one can derive a prediction of new pose given old pose and controls, with appropriate noise.</li><li><strong>Omnidirectional or Holonomic Models:</strong> For drones or omniwheel robots, you might have velocity in 3 axes and angular velocities. An IMU gives linear acceleration and angular velocity; integrating those (with gravity removal and drift) provides a dead-reckoned pose. More sophisticated models incorporate IMU bias as part of the state.</li><li><strong>Constant Velocity or Constant Turn Rate models:</strong> In absence of direct odometry, some SLAM front-ends assume a simple motion prior like “the robot continues moving as it was”. For example, in LOAM’s odometry, between scans they might assume constant velocity motion to predict a rough guess of the next pose, which is then refined by scan matching. This helps in reducing the search space for matching.</li></ul><p>In SLAM, the motion model is used in two ways:</p><ol><li><strong>State Prediction:</strong> When doing filtering (EKF, particle), we propagate the pose estimate forward using the motion model and control. This gives us a prior for the sensor update. If the robot moved 1 meter forward according to odometry, we predict the pose changed accordingly, but we also inflate the uncertainty because odometry isn’t perfect. This predicted distribution is often called the <strong>proposal</strong> for the next state.</li><li><strong>As a Factor/Constraint:</strong> In graph-based SLAM, each odometry measurement yields a constraint between consecutive poses. For instance, a factor might say “according to wheel odometry, the transform from $x_{t-1}$ to $x_t$ is $\Delta x$ with some covariance”. This is essentially the motion model constraint. Even without explicit odometry sensors, we can get a motion constraint from the SLAM front-end itself (ex: the initial guess from scan matching plays the role of odometry).</li></ol><p>It’s important that the motion model be <strong>consistent with the robot’s actual movement capabilities</strong>. If it’s too naive, the SLAM system might make poor predictions and find it harder to converge during sensor updates. If it’s accurate, it keeps the pose estimate close to the true value before sensor correction, which can improve data association and convergence.</p><p><strong>Noise and uncertainty growth:</strong> Generally, as the robot moves without a new observation, its position uncertainty increases. For instance, if you drive 10m straight, you might accumulate a significant positional uncertainty because small wheel slips integrated over 10m become large. This is often modeled by something like a covariance matrix $Q$ added during prediction. The <strong>Kalman Filter</strong> prediction step does $P\_{t|t-1} = F P\_{t-1|t-1} F^T + Q$ where $F$ is the Jacobian of the motion model, and $Q$ is motion noise covariance. More uncertainty means we rely more on measurement to correct.</p><p>In LiDAR-based SLAM, interestingly, some implementations forego an explicit motion model by using scan matching as the “prediction”. For example, <strong>Hector SLAM</strong> (2D lidar SLAM) assumes a motion model of constant velocity and directly aligns scans, not even using wheel odometry. It effectively trusts the scan matching more than any prior motion estimate. Others like Cartographer use an IMU to constrain orientation (motion model for roll/pitch) and wheel odometry for linear motion when available – this makes the system more robust and drift less over short distances.</p><p>In summary, the motion model anchors SLAM by providing an initial guess of where the robot moved. It is as important as the sensor data in maintaining a stable estimate. A mismatch (say using a straight-line model for a robot that actually strafes) can lead to inconsistency. Tuning the noise parameters of the motion model is also key: too confident in odometry and you might under-weight LIDAR corrections; too little confidence and the filter might over-react to noisy measurements. A good balance keeps SLAM stable.</p><h1 id=measurement-association-in-slam>Measurement Association in SLAM<a hidden class=anchor aria-hidden=true href=#measurement-association-in-slam>#</a></h1><p>When the robot receives a sensor measurement, the SLAM system must determine <strong>which part of the map (or which landmark) that measurement corresponds to</strong> – this is the <strong>data association</strong> or measurement association problem. In other words: given a new observation, how do we associate it with a feature or location in our existing map?</p><p>This is one of the trickiest parts of SLAM, especially in feature-based formulations:</p><ul><li><p>In landmark-based SLAM (e.g., EKF-SLAM with point landmarks), when the robot observes some feature (like a visual feature or a corner in a laser scan), it must decide if this is:</p><ul><li>a <strong>new landmark</strong> that hasn’t been seen before, in which case we add it to the map,</li><li>or a <strong>re-observation of an existing landmark</strong>, in which case we update that landmark’s estimate.</li></ul></li><li><p>In dense mapping (like with occupancy grids or raw point clouds), data association is implicit in the scan matching process. Essentially, when we align a new scan to the map, we are associating points in the scan to nearby points/surfaces in the map (usually by nearest neighbor search). If the map is an occupancy grid, association is checking which grid cells correspond to detected obstacles in the scan.</p></li></ul><p>Key techniques for measurement association:</p><ul><li><strong>Nearest Neighbor (NN) Association:</strong> The simplest method: for each observed feature, find the closest matching feature in the map by some distance metric. For LiDAR, this might mean taking a point from the new scan and finding the nearest point in the existing map cloud (or nearest plane/line feature). Nearest neighbor works if the density of features is high enough and the initial pose estimate is reasonably good so that true correspondences are near in space. However, it can mismatch in ambiguous situations (corners, repetitive structures).</li><li><strong>Gating:</strong> Before accepting an association, apply a gating threshold – e.g., only consider a match if the distance is below some threshold. In EKF-SLAM, a <strong>Mahalanobis distance gate</strong> is used: an innovation (measurement minus expected measurement) that is too large (statistically unlikely) is rejected as a possible match. Gating prevents very wrong associations that would severely skew the estimate.</li><li><strong>Joint Compatibility:</strong> More advanced data association considers sets of correspondences that are mutually compatible. For example, in a laser scan of multiple landmarks, you want to find the assignment of observed points to map landmarks that overall makes sense (like solving a small assignment problem). Algorithms like <strong>JCBB (Joint Compatibility Branch and Bound)</strong> search for a consistent matching that maximizes likelihood while rejecting combinations that are incompatible with the vehicle’s possible motion.</li><li><strong>Descriptors for Features:</strong> In visual SLAM, features have descriptors (ORB, SIFT etc.), which help match observations to the same landmark seen before by comparing descriptor similarity. In LiDAR, one can use shape descriptors (e.g., histogram of surrounding points) for loop closure matching (e.g., <strong>Scan Context</strong> gives a descriptor for a scan to match places). While not exactly point-to-point association, these methods assist in recognizing previously visited places or re-observing a mapped area from a different angle.</li><li><strong>Outlier Rejection:</strong> Even with gating, some wrong associations can slip through (especially in loop closure detection where the initial pose guess might be off by a lot). <strong>RANSAC (Random Sample Consensus)</strong> is commonly used to robustly match sets of points: for instance, fitting a transform between two scans by randomly sampling correspondences and finding a consensus transform that aligns many points. Any points that don’t agree with this transform are considered outliers and rejected. RANSAC effectively finds a large subset of consistent pairings, filtering out spurious matches.</li></ul><p>In LiDAR ICP (scan-to-scan or scan-to-map alignment), data association is the step of finding closest points (or closest plane surface) for each point in the source scan. By iterating, ICP refines the associations: initially, with a rough pose guess, many correspondences might be wrong, but as the pose converges, the set of nearest neighbors hopefully represents the true matching of scan points to map points. Some variants weigh correspondences by how close or how planar they are, and discard those above a certain distance (this is effectively gating).</p><p>A major challenge in association is <strong>perceptual aliasing</strong> – different places look the same to the sensor. For example, in an office with many similar cubicles, a scan in one corridor might look just like a scan in another corridor. Nearest neighbor might incorrectly associate to the wrong hallway. In such cases, global context or history is needed to avoid false loop closures. Many SLAM systems will be conservative in accepting a loop closure: they may require multiple corroborating cues (e.g., vision and LiDAR both agree it’s the same place) or a sequence of measurements matching, before committing to closing the loop.</p><p>In EKF-SLAM, a known trick is that if a wrong association is made even once, the filter can become inconsistent and diverge. Graph-based methods are a bit more resilient (you can later remove a bad constraint), but they also can get stuck in a wrong configuration if a bad loop closure is added without realization.</p><p><strong>Automatic mapping of new features:</strong> In mapping, if a measurement doesn’t match any known feature within reasonable uncertainty, it spawns a new map feature. For instance, a new corner is detected by LiDAR in a place where no mapped corner exists nearby, then we add a new landmark at that location. Tuning the threshold for new feature creation vs. matching to existing is important to not explode the number of landmarks while not missing real ones.</p><p>In occupancy grid SLAM, data association might not be explicitly discussed, but it’s happening when integrating a scan into the grid: each laser ray is associated with a set of grid cells along its path (free space) and the endpoint cell (occupied). If the pose is off, the updates go into the wrong cells. Particle filter mapping handles this by trying many pose hypotheses, essentially deferring association until after pose probability is computed.</p><p>In summary, measurement association is about <strong>figuring out what the sensor is seeing</strong> in terms of the map. Doing this correctly maintains map consistency and bounds the error. Doing it poorly leads to corrupted maps or lost localization. As environments grow more complex, robust association remains a key difficulty – it often drives the need for richer sensor data (like adding vision or semantics to distinguish places) and better algorithms (like SLAM back-ends that can identify and discard outlier loop closures to recover from mistakes).</p><h2 id=iterative-closest-point-icp>Iterative Closest Point (ICP)<a hidden class=anchor aria-hidden=true href=#iterative-closest-point-icp>#</a></h2><p>One of the workhorse algorithms in LiDAR-based SLAM is <strong>Iterative Closest Iterative Closest Point (ICP)</strong>.
<strong>Iterative Closest Point (ICP)</strong> is a fundamental algorithm for aligning two point clouds and is heavily used in LiDAR SLAM for scan matching (both for odometry and loop closure). Given a “source” point cloud (e.g. the new LiDAR scan) and a “target” point cloud (e.g. the previous scan or a map), ICP iteratively refines the pose transform (rotation $R$ and translation $t$) that best aligns the source to the target. The basic ICP procedure is:</p><ol><li><strong>Initialize</strong> the transform. Start with an initial guess for the relative pose (this could be from odometry or simply the identity if no prior).</li><li><strong>Find Closest Points:</strong> For each point in the source cloud, find the nearest neighbor in the target cloud (according to Euclidean distance, or sometimes along a line-of-sight for 2D). These pairs are the current <em>correspondences</em>.</li><li><strong>Compute Transform:</strong> Estimate the optimal rigid transform (rotation $R$ and translation $t$) that minimizes the alignment error for these correspondences. For point-to-point ICP, this often means minimizing the sum of squared distances $\sum_i | R \cdot p_i^{\text{source}} + t - q_i^{\text{target}}|^2$. A closed-form solution (using SVD on the correlation matrix of paired points) gives the best fit $R, t$.</li><li><strong>Apply Transform:</strong> Move the source cloud by this computed transform to (hopefully) better align with target.</li><li><strong>Iterate:</strong> With the source now moved, go back to step 2 (recompute nearest correspondences) and repeat. Continue until convergence – i.e., the change in error or in pose becomes very small – or until a maximum number of iterations is reached.</li></ol><p><em>Figure:</em> Conceptual illustration of ICP aligning two shapes (red and blue). The algorithm iteratively matches points (colored lines linking red–blue pairs) and refines the transform. In the rightmost image, the blue shape (transformed source) aligns closely to the red shape (target). ICP will stop when the alignment error is below a threshold.</p><p>ICP will converge to a local minimum of the alignment error. If the initial guess is good and the surfaces overlap significantly, ICP usually converges to the correct alignment. However, if the initial guess is far off (beyond the “capture range”), ICP can converge to a wrong match (e.g., matching a pillar in source to a different pillar in target). Thus, providing a reasonable initial pose (via odometry or IMU) is important for ICP in SLAM.</p><p><strong>Variants of ICP:</strong> Standard ICP uses point-to-point distance. Variants improve convergence:</p><ul><li><em>Point-to-plane ICP:</em> Instead of minimizing point-to-point distances, if you can estimate surface normals for target points, you minimize the distance of each source point to the tangent plane of the nearest target point. This often converges faster and more accurately on planar environments, as it accounts for the surface orientation.</li><li><em>Generalized ICP (GICP):</em> A merge of point-to-point and point-to-plane, modeling each point with a covariance (uncertainty shape) often based on local structure. GICP treats the alignment as matching two Gaussian distributions per correspondence and tends to be robust.</li><li><em>Trimmed ICP:</em> It rejects a fraction of correspondences with largest errors in each iteration, to avoid being skewed by outliers (like a moving object in one scan that isn’t in the other).</li><li><em>Multi-Scale ICP:</em> Perform ICP on a downsampled (coarse) version of the clouds first to get close, then on finer detail – helps avoid local minima.</li><li><em>Color ICP:</em> (not as relevant for pure LiDAR, but for RGB-D data) uses color information to improve matching.</li></ul><p>In LiDAR SLAM front-ends (odometry), ICP (or its variant) is often running continuously: every new scan aligns to the previous scan (or a small local map) to yield an estimated transform since the last frame. This gives <strong>LiDAR odometry</strong>. For example, <strong>LOAM</strong> uses a form of ICP but on feature points: it extracts edge and plane features and then does nearest-neighbor matching of these features to the last frame’s features, solving for the motion that aligns them. This is essentially ICP restricted to features.</p><p>Here’s a pseudocode of a basic ICP algorithm for clarity:</p><pre tabindex=0><code class=language-pseudo data-lang=pseudo>function ICP(source_points, target_points, max_iterations, tolerance):
    Transform := Identity  // initial guess
    for iter = 1 to max_iterations:
        // 1. Find nearest neighbor correspondences
        correspondences := []
        for p in source_points:
            q := NearestNeighbor(p, target_points)
            correspondences.append((p, q))
        // 2. Compute optimal R, t aligning all (p,q) pairs
        (R, t) := ComputeRigidTransform(correspondences)
        // 3. Apply the transform to the source points
        source_points := [R * p + t for p in source_points]
        Transform := (R, t) o Transform  // update overall transform
        // 4. Check convergence (e.g., max correspondence error change)
        error := ComputeAlignmentError(correspondences, R, t)
        if error &lt; tolerance:
            break
    return Transform
</code></pre><p>In practice, efficient data structures like k-d trees are used for the nearest neighbor search to speed up step 2. Also, one might not use all points (downsampling by a voxel grid filter can make ICP faster with little loss of accuracy).</p><p><strong>ICP in Loop Closure:</strong> ICP isn’t only for consecutive frames. It can be used to align a new scan to an <em>older scan or submap</em> when detecting loop closure. For instance, if place recognition suggests that “scan at time 100 is near scan at time 10”, we can use ICP to refine the exact relative pose between those scans, yielding a loop closure constraint.</p><p>One must be cautious that ICP is local. For global alignment (big loops), one often combines it with a <strong>global descriptor match</strong> to get close, then ICP to fine-tune. A successful ICP alignment significantly reduces relative pose error, which is then fed into the SLAM back-end (e.g., as a factor in pose graph).</p><p>In summary, ICP provides the mathematical method to answer “how did the LiDAR move from scan A to scan B?” by geometric alignment. It is ubiquitous in LiDAR SLAM implementations (from 2D algorithms like Hector SLAM which essentially do a variant of ICP on occupancy grids, to 3D ones like LOAM, Cartographer’s scan matching, etc.). Its strengths are simplicity and precision in matching geometry; its weaknesses are computational load for large clouds and the local-minimum issue requiring good initial guess.</p><h1 id=odometry-and-its-role-in-slam>Odometry and its Role in SLAM<a hidden class=anchor aria-hidden=true href=#odometry-and-its-role-in-slam>#</a></h1><p><strong>Odometry</strong> refers to the estimation of the robot’s incremental motion over time. In SLAM, odometry (whether from wheel encoders, IMU, or LiDAR visual odometry) plays the role of providing a <strong>real-time local pose estimate</strong> that keeps track of where the robot has moved between updates. While odometry on its own will drift, it is indispensable for giving the SLAM system short-term accuracy and a starting guess for alignment.</p><p>Types of odometry:</p><ul><li><strong>Wheel Odometry:</strong> Calculated from wheel rotations. For example, a differential drive robot integrates wheel encoder ticks into an $(x,y,\theta)$ pose. This is straightforward and high-rate, but subject to wheel slip and cannot account for changes in elevation or rough terrain.</li><li><strong>Visual Odometry (VO):</strong> Using camera images to estimate motion (by tracking feature displacements between frames). VO gives 6-DoF motion and works where wheel odometry might not (e.g., legged robots or flying drones). It can drift if features are far or if there’s motion blur.</li><li><strong>IMU integration:</strong> An IMU (accelerometers + gyros) can provide orientation changes and short-term motion with high frequency. Integrating IMU data (which gives velocity and orientation when integrated) yields odometry that is very smooth and low-latency, but drifts over time (especially position due to acceleration bias integration).</li><li><strong>LiDAR Odometry (LO):</strong> This is essentially using the LiDAR itself to compute odometry by scan matching successive frames. It’s what LOAM’s first stage does: it computes a 10 Hz pose from aligning scan k to scan k-1. Many modern SLAM systems (LIO-SAM, FAST-LIO, etc.) also have a “odometry node” that runs ICP or a Kalman Filter on a combination of IMU + LiDAR to output a continuous pose estimate.</li></ul><p>In a SLAM system, odometry is often treated as the <strong>prediction</strong>:</p><ul><li>The odometry gives a quick estimate of the pose change since the last sensor update. This can be used to <strong>propagate the filter</strong> or to initialize ICP for the next scan alignment.</li><li>Even if odometry drifts in the long run, over a short interval (say 0.1s), it’s usually quite accurate. This ensures that when the next sensor reading comes in, we don’t have to search blindly over a huge space to align things; we can start near the odometry predicted position.</li></ul><p>Odometry alone, if one simply dead-reckons, will eventually produce an inaccurate trajectory (e.g., wheel slip might cause a 5% distance error, leading to 5m error after 100m traveled). However, <strong>SLAM’s mapping/loop closure will correct</strong> this drift over time by referencing external landmarks. Essentially, odometry keeps the pose estimate coherent in the short term, and SLAM provides global corrections in the long term.</p><p>Many SLAM architectures split into <strong>front-end and back-end</strong>:</p><ul><li>The <strong>front-end</strong> often includes odometry estimation (e.g., tracking) and loop closure detection.</li><li>The <strong>back-end</strong> does optimization (adjust poses to satisfy constraints).</li></ul><p>For instance, in <strong>Cartographer</strong> (by Google) there is a local SLAM that integrates IMU + scan matching (odometry) to build small submaps, and a global SLAM that performs loop closure optimization on submap poses. The local SLAM acts as odometry, providing immediate pose feedback.</p><p>In <strong>LOAM</strong>, they explicitly break it into two threads:</p><ul><li>A <strong>high-frequency odometry thread</strong> (10 Hz) that uses a subset of points to estimate motion quickly but with some drift.</li><li>A <strong>low-frequency mapping thread</strong> (1 Hz) that uses accumulated data to refine the pose and update the map, and can correct drift over larger windows.</li></ul><p>This design shows how odometry (the fast thread) is used to keep up with motion, while mapping/loop closures (slow thread) correct it gradually.</p><p>Odometry sources can also be fused: <strong>LIO-SAM</strong> tightly fuses IMU pre-integration with LiDAR scan matching to produce a robust odometry estimate that is better than either alone. The IMU helps to deskew and provide initial guess for scan matching, and scan matching corrects IMU drift – resulting in a more stable odometry.</p><p>In summary, odometry is the backbone that carries the pose forward between absolute corrections. Good odometry reduces the burden on the rest of the SLAM system:</p><ul><li>If odometry is very accurate, the mapping just has to fine-tune and mostly handle loop closures occasionally.</li><li>If odometry is poor, the SLAM system has to rely more on matching observations to past map data even for short-term stabilization, which is harder and computationally expensive.</li></ul><p>From an engineering perspective, always leverage any available odometry (wheel, IMU, etc.) because it makes the SLAM problem more tractable. But also be aware that odometry drift is inevitable – thus the SLAM must incorporate loop closures or other global references (like GPS, landmarks, or known map alignment) to remain globally consistent.</p><h1 id=octree-and-octree-of-features>Octree and Octree of Features<a hidden class=anchor aria-hidden=true href=#octree-and-octree-of-features>#</a></h1><p><strong>Octrees</strong> are tree data structures that recursively divide 3D space into eight octants (halving each axis at each level). They are widely used in 3D mapping to manage spatial information efficiently. An <strong>Octree-based map</strong> provides a memory-efficient representation by <strong>adapting resolution</strong>: empty or uniformly occupied regions can be stored coarsely, and complex regions stored at finer granularity. The popular <strong>OctoMap</strong> library represents a probabilistic occupancy grid in an octree format, enabling large maps to be handled without exorbitant memory.</p><p><strong>Using Octrees in SLAM:</strong></p><ul><li><strong>3D Occupancy Mapping:</strong> As the robot moves, LiDAR points can be inserted into an octree. Each node in the octree corresponds to a cubic volume of space. OctoMap maintains a log-odds of occupancy for each node (voxel) based on how many points hit inside it vs pass through it. This naturally handles sensor uncertainty and can merge multiple observations. Octrees allow quick queries like “is there an obstacle at (x,y,z)?” or ray casting for line-of-sight, which are useful for navigation and sensor simulation. They also make it easy to downsample for visualization (since you can display only leaf nodes at appropriate levels).</li><li><strong>Multi-Resolution Maps:</strong> The octree inherently is multi-scale. For example, high detail (small voxels) near the robot or in areas of interest, and large voxels farther or in sparse areas. This can drastically reduce the number of points stored. A corridor with flat walls: all those wall points can be compressed into large planar cells once the wall has been fully observed.</li><li><strong>Dynamic Updates:</strong> Octrees can be updated online – e.g., if something changes or if new space is explored, the tree expands or updates those nodes. Also, sensor range data provides free-space info: every beam not just gives an occupied endpoint but also many empty cells along its path, which OctoMap marks as free. This helps for navigation (identifying free space).</li></ul><p><strong>Octree of Features:</strong> In some SLAM systems, instead of occupancy, the map stores <strong>features in an octree</strong>. For example, LOAM’s mapping module maintains a <strong>voxel grid of feature points</strong>: it divides space into voxels (like 1m^3 or so) and keeps at most one or a few edge/plane feature points per voxel to limit density. This is effectively an octree (or grid) of features – it sparsifies the feature map to ensure computational efficiency (constant time lookup per voxel). When a new feature is extracted from a scan, if its voxel already has a feature, they might average them or skip adding, to keep map size bounded.</p><p>Another use is for <strong>place recognition</strong>: One approach (NoctuSLAM, as hinted by its name) uses an octree structure to store <em>appearance</em> of places for loop closure detection. An octree can store at each node a summary of the shape (e.g., a hash or signature of the submap in that octant). This can speed up matching large point clouds by coarse-to-fine matching.</p><p>In recent research, <strong>Gaussian voxel maps (GVM)</strong> have emerged, where each voxel (often implemented via an octree) stores a Gaussian distribution (mean and covariance of points in that cell) instead of just occupancy or a single point. This is beneficial for scan matching: rather than matching individual points, one can match a point to a <em>Gaussian blob</em> representing a surface. The GVM approach (as in LIO-GVM) drastically compresses the map (only one Gaussian per voxel) and provides analytical derivatives for alignment. Essentially, this is a marriage of octree mapping and ICP (like a continuous occupancy field for matching).</p><p><strong>Benefits of Octrees:</strong></p><ul><li>Efficient memory usage (only subdivide where needed).</li><li>Fast Nearest Neighbor search if done properly (you can traverse the tree following the binary space partition logic).</li><li>Naturally organizes data for multi-scale algorithms (e.g., coarse registration at higher levels, then fine tune at leaves).</li><li>Well-suited for <strong>mapping large 3D environments</strong> like multi-floor buildings, caves, outdoor terrains.</li></ul><p><strong>Usage in SLAM systems:</strong></p><ul><li>Many SLAM pipelines use octree maps for collision avoidance and planning, even if the SLAM algorithm itself uses point clouds for scan matching. For instance, after SLAM produces a point cloud, one might convert it to an OctoMap for a robot to navigate.</li><li>Some SLAM algorithms integrate mapping and pose graph: after optimization, they can update an octree map globally.</li></ul><p><strong>Example:</strong> A drone mapping a forest might use LiDAR SLAM to get its poses and a sparse map of tree trunks (features). Parallelly, it can build an octree occupancy map of the environment for path planning (ensuring it doesn’t hit branches). The octree will compress all those empty spaces and uniformly filled spaces (like ground plane) effectively.</p><p>In conclusion, octrees provide a powerful framework for <strong>managing 3D map data</strong> in SLAM. Whether it’s an occupancy grid or a feature map, octrees help scale SLAM to large environments by reducing memory and allowing quicker queries. Many state-of-the-art LiDAR SLAM systems (Cartographer 3D, BLAM, etc.) leverage octrees in some form for map representation, even if the front-end uses other methods. As 3D SLAM becomes more common (with autonomous cars and robots), octrees or similar spatial structures are becoming a standard component of the SLAM toolkit.</p><h1 id=kalman-filter-dynamic-and-measurement-models>Kalman Filter (Dynamic and Measurement Models)<a hidden class=anchor aria-hidden=true href=#kalman-filter-dynamic-and-measurement-models>#</a></h1><p>The <strong>Kalman Filter (KF)</strong> is a state estimation algorithm that provides an optimal recursive solution (for linear systems with Gaussian noise) to combine a dynamic model (predicting state evolution) and measurement model (relating state to observations). In the context of SLAM, especially early approaches, the Extended Kalman Filter (EKF) was a prominent framework to estimate the joint state of robot pose and map.</p><p><strong>State, Dynamic Model, Measurement Model:</strong></p><ul><li>The filter maintains a <strong>state vector</strong> and a <strong>covariance matrix</strong> $P$ representing uncertainty. In full SLAM EKF, the state might include the robot pose and the positions of all landmarks (this gets very large, which is one EKF-SLAM issue). In a simpler localization EKF, the state is just the robot pose.</li><li>The <strong>dynamic model</strong> (motion model) $x_{t} = f(x_{t-1}, u_t) + \text{noise}$ propagates the state. For a nonlinear $f$, EKF linearizes it around the current estimate using the Jacobian $F$ (also called $A$ sometimes). The process noise covariance $Q$ represents uncertainty in this motion.</li><li>The <strong>measurement model</strong> $z_{t} = h(x_t, m) + \text{noise}$ relates state to expected observation. For example, if a landmark $j$ with position $(x_j, y_j)$ is in state, and the robot pose $(x, y, \theta)$ is part of state, the measurement could be the bearing and range to that landmark: $h(x, m_j) =[ \sqrt{(x_j-x)^2 + (y_j-y)^2}, \text{atan2}(y_j-y, x_j-x) - \theta ]$. This too is linearized (Jacobian $H$) for the EKF update. Measurement noise covariance $R$ is used here.</li></ul><p><strong>EKF SLAM procedure:</strong> Each time step:</p><ul><li><strong>Predict:</strong> Use motion $f$ to predict new state $\hat{x}_{t|t-1}$ and propagate covariance $P_{t|t-1} = F_t P_{t-1|t-1} F_t^T + Q_t$. This expands uncertainty in the direction of motion uncertainty.</li><li><strong>Update:</strong> When a measurement $z_t$ arrives, compute expected measurement from predicted state $\hat{z} = h(\hat{x}_{t|t-1})$. Compute innovation $y = z_t - \hat{z}$ and the Kalman gain $K = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}$. Then correct state: $x_{t|t} = \hat{x}_{t|t-1} + K y$, and update covariance $P_{t|t} = (I - K H) P_{t|t-1}$.</li></ul><p>In <strong>EKF-SLAM</strong>, the state includes landmarks, so when a new landmark is first observed and recognized as new, it gets appended to the state with appropriate initialization and covariance (EKF can augment state). Each observation can update both robot and landmark parts of the state (correlations built up in P ensure a measurement of a landmark adjusts the robot pose and all correlated landmarks).</p><p>The Kalman Filter provides a mathematically elegant way to handle <strong>sensor fusion</strong> too: e.g., one can combine LiDAR and IMU by modeling IMU outputs as part of the measurement update or directly in the motion model. <strong>Unscented Kalman Filter (UKF)</strong> is an alternative to EKF that avoids Jacobians by using sigma points to propagate means and covariances; it’s useful for highly nonlinear systems.</p><p>However, <strong>EKF-SLAM has limitations</strong>:</p><ul><li>Computational complexity is $O(n^2)$ with $n$ being state size (landmarks) due to covariance updates, making it hard to scale beyond a few hundreds of landmarks before it becomes slow.</li><li>It assumes Gaussian distributions; a single mode, which is not always valid if data association is ambiguous.</li><li>Linearization errors can cause inconsistency (over-confident estimates).</li></ul><p>Despite this, EKFs were historically successful on small-scale SLAM (like mapping a single room with some beacons). Many modern SLAM systems don’t use a monolithic EKF for full SLAM, but Kalman Filters are still used in <strong>local pose tracking</strong> and sensor fusion:</p><ul><li><strong>Robot Localization:</strong> Often an EKF fuses wheel odometry, IMU, and maybe GPS to get a robust pose estimate (this is essentially an localization EKF, not mapping). ROS’s <code>robot_localization</code> package does this.</li><li><strong>Filtering-based SLAM:</strong> Some systems use an EKF or UKF for mapping sparse feature maps (e.g., ORB-SLAM3’s inertial version uses a filter for IMU integration between keyframes).</li><li><strong>Kalman Filter for LiDAR Odometry:</strong> One could use a KF to estimate the velocity of the robot such that the predicted motion aligns subsequent scans, instead of pure ICP. For instance, a constant velocity model plus ICP measurements can be fused in a KF framework.</li></ul><p>An example of a filter-based LiDAR SLAM is <strong>Kalman Filter SLAM in mapping with NDT</strong>: The state is the vehicle pose and velocity; motion from IMU is prediction; LiDAR scan matching (NDT providing relative pose) is the measurement update. This yields a smooth state estimate at high rates.</p><p><strong>Summary of dynamic vs measurement models:</strong></p><ul><li>The <strong>dynamic model</strong> introduces <em>temporal coherence</em> (tie between successive poses, as discussed in the Motion Model section). It’s typically relatively high certainty over short term (especially with wheel/IMU).</li><li>The <strong>measurement model</strong> introduces <em>environment feedback</em> (tie between pose and map features, as in measurement association). It often has higher uncertainty initially (if you don’t know map well) but as map converges, measurements provide strong corrections.</li></ul><p>Kalman Filters assume both models are reasonably linear (or linearizable) and that noise is Gaussian. If LiDAR gives you a million points, you can’t put that directly into a KF; instead, you’d extract a few significant features or do a compression (like using submap match yields a relative pose measurement).</p><p>One notable approach is <strong>EKF Localization on an occupancy grid</strong>: known as the AMCL’s analog but in KF form (less common than particle filters, since the observation model of a full scan given a pose is highly nonlinear/non-Gaussian).</p><p>Even though graph-based SLAM is more common now, understanding KF models is useful because:</p><ul><li>It teaches the importance of models and uncertainty in SLAM.</li><li>Many parts of a SLAM system can be <em>designed using Kalman filter principles</em>, then implemented with non-linear optimization. For instance, the information matrix in an EKF corresponds to the Hessian in graph optimization; the math lines up.</li></ul><p>In conclusion, Kalman Filter brings together the <strong>motion prediction</strong> and <strong>measurement update</strong> in a principled way. In SLAM, EKF was historically used to directly estimate map and pose. Now, its role is often in a reduced capacity (e.g., state estimation for odometry or sensor fusion) or conceptually in factor graphs. Nonetheless, the concepts of dynamic and measurement models, propagated and update uncertainties, are integral to all SLAM methods – even if solved via optimization, one can derive them from the same fundamentals that the Kalman Filter uses.</p><h1 id=post-processing-and-loop-closure>Post-Processing and Loop Closure<a hidden class=anchor aria-hidden=true href=#post-processing-and-loop-closure>#</a></h1><p>No matter how good the odometry (front-end) is, <strong>drift accumulates</strong> in any SLAM. <strong>Loop closure</strong> is the process of detecting when the robot has returned to a previously visited area (closing the loop) and correcting the map and trajectory to enforce the consistency of that loop. This is typically handled in the <strong>back-end</strong> optimization.</p><p><strong>Loop Closure Detection:</strong> The first step is to realize “we are in a known place.” There are various approaches:</p><ul><li><strong>Spatial Proximity:</strong> If using GPS or known initial poses, one might guess a loop closure when the robot is physically near the start. But without external references, proximity must be established by the sensor data itself.</li><li><strong>Place Recognition (Scan Matching):</strong> For LiDAR, one way is to match the current scan against a <strong>database of past scans or submaps</strong>. Techniques like <strong>Scan Context</strong> create a descriptor (e.g., a ring image) of a LiDAR scan that is rotation-invariant; new scans are compared to stored descriptors to find potential matches (candidates for loop closure). Another approach is simply trying ICP alignment of the current scan with some previous keyframe scans (not all, but maybe every 10th or so) to see if a good fit (low error) can be found. This can be expensive, so usually a coarse pre-check is used (like comparing histograms or using some semantic clues).</li><li><strong>Visual Place Recognition:</strong> In a multi-sensor SLAM (camera+LiDAR), the camera could detect loop closure (using Bag-of-Words or deep learned features) and then that can trigger a LiDAR loop alignment. LVI-SAM does this: it uses a vision module to detect loops and then incorporate them for LiDAR+visual graph optimization.</li><li><strong>Map Matching:</strong> If a global map is being built incrementally (like a growing point cloud or submap), one can sometimes detect loop closure by aligning the current local map with the global map in a broad sense (e.g., using correlation on a grid map, or recognizing that certain structures line up).</li></ul><p>Once a <strong>loop candidate</strong> is detected, one typically uses <strong>ICP or a similar registration</strong> to get an accurate relative pose between the current pose and the previously visited pose (or map). For example, if we think we are back at the start, align the current scan with the map area around the start position to compute the transform (this gives the loop closure constraint).</p><p><strong>Adding Loop Constraints:</strong> Loop closures are added as additional constraints in the SLAM optimization problem. In a factor graph, this appears as an edge connecting the current pose node to an old pose node, with a certain relative transformation measurement (and uncertainty). For instance: pose_50 and pose_5 might get a loop closure factor saying “according to loop detection, pose_50 is at (dx, dy, dtheta) relative to pose_5 with some covariance”.</p><p><strong>Back-End Optimization (Pose Graph):</strong> Once added, we solve for the new set of poses that satisfies all constraints (odometry + loop). This often means doing a <strong>nonlinear least squares optimization</strong> (if using Gauss-Newton, Levenberg-Marquardt, or iSAM incremental smoothing). The outcome is that the poses will adjust, distributing the loop error across the trajectory:</p><ul><li>Before loop closure, the robot’s estimated trajectory might not line up where it started (there’s a gap if you plot start vs end).</li><li>After adding a loop closure and optimizing, the trajectory will deform so the end meets the start (closing the gap), and this deformation is spread over many poses so that no single jump is huge (minimizing squared error typically leads to distributing error evenly).</li></ul><p>In EKF-based SLAM, loop closure happens implicitly when a landmark observed earlier is seen again – the filter’s update will adjust the pose and landmark positions. In graph-based SLAM, it’s more explicit: you add an edge and solve. Graph-based methods are more flexible to handle many loop closures and large updates.</p><p><strong>Example:</strong> Cartographer builds submaps and when loop closure finds an overlap between a current scan and an older submap, it adds constraints and then runs a sparse pose graph optimization. This process can run in a background thread. Cartographer uses branch-and-bound to efficiently search for loop closure matches in submaps (to avoid brute force).</p><p><strong>Loop Closure Challenges:</strong> As mentioned, false loops are dangerous. If a wrong loop closure is added (thinking two different places are same), optimization can warp the map incorrectly. Many systems use outlier robust cost functions or check consistency of multiple loop closures before trusting them. Some maintain a “whitelist” or “blacklist” of loop closures.</p><p><strong>After Loop Closure – Map Correction:</strong> Once the poses are adjusted, the <em>map</em> built from those poses must also be corrected. If we maintain a global point cloud map, we can transform all points by the corrected poses. In practice, many SLAM systems don’t explicitly rebuild the entire map from scratch after every loop optimization (would be too slow), instead they often keep the map in pose-referenced form:</p><ul><li>In a pose graph SLAM, each scan or submap is associated with a pose node. When that node moves in optimization, effectively the scan is now at a new location in the global frame. If needed, one can regenerate the global point cloud by fusing scans at their new poses.</li><li>Some online SLAM systems apply corrections on the fly to recent poses (sliding window) but wait until the end or sparsely for big global updates.</li></ul><p><strong>Graph SLAM vs EKF in loop closure:</strong> Graph SLAM tends to handle loop closure more gracefully because you’re optimizing a lot of variables together. EKF would just do one huge update that might add a lot of correlation and potentially inconsistency if the loop error was big. Graph methods allow a big residual to spread over the whole loop.</p><p><strong>Loop Closure vs Relocalization:</strong> Loop closure is often when still continuously tracking. If the robot got lost (kidnapped) and then recognizes a known place, that’s more of a <strong>relocalization</strong> event – similar effect (you add a constraint to a past pose or global map alignment), but in real-time system, you might have to essentially “teleport” the pose estimate to the correct one. Many SLAM systems integrate relocalization by detecting known features (e.g., a QR code or a distinctive geometry that was mapped before).</p><p><strong>Post-Processing:</strong> Sometimes, especially in mapping applications, loop closures and full optimizations are done <strong>offline (post-processing)</strong>. The robot logs data, then later a big optimization is performed to build the final consistent map. However, in an online system (like a car driving), you need to close loops on the fly (or within seconds) to correct your path.</p><p>A famous post-processing technique is <strong>Bundle Adjustment</strong> in visual SLAM (which is basically loop closing by optimizing all camera poses and points). In LiDAR SLAM, the analogous step is global pose graph optimization with all loop closures included.</p><p>Finally, <strong>Map Refinement:</strong> After loop closure, maps can be further refined by <strong>dense methods</strong> if needed. For example, one could do a global <strong>ICP alignment</strong> using all poses as initial guess to fine-tune the point cloud map (this is like distributing the error not just on poses but adjusting points too). Some SLAM systems will also cull redundant points or apply consistency checks after big loop closures (e.g., remove duplicate map points that now overlap).</p><p>To illustrate, consider a robot driving around a block and returning close to start:</p><ul><li>Initially, due to drift, its map of the block might not perfectly close (the roads might overlap or gap slightly).</li><li>Loop closure detects it&rsquo;s back at start, adds a constraint to align end pose to start pose.</li><li>Optimization shifts the intermediate poses slightly so that the road aligns perfectly end-to-start.</li><li>The map (street edges, buildings) now aligns with itself; where there were double features (from start and end passes) they now coincide.</li></ul><p>Thus, <strong>post-processing</strong> (loop closure and optimization) turns a locally consistent map into a globally consistent map. It is an essential component for <strong>global accuracy</strong>. Without it, SLAM would degrade to dead-reckoning over long loops. Modern SLAM back-ends like Google’s <strong>Ceres Solver</strong>, GTSAM, or g2o are highly optimized to do these calculations quickly even for hundreds or thousands of pose variables and constraints.</p><h1 id=recent-advances-in-slam>Recent Advances in SLAM<a hidden class=anchor aria-hidden=true href=#recent-advances-in-slam>#</a></h1><p>SLAM is a very active field of research and development. In recent years, several emerging trends and technologies have shaped new SLAM approaches, especially for LiDAR-based SLAM. We highlight a few:</p><h2 id=incorporating-deep-learning>Incorporating Deep Learning<a hidden class=anchor aria-hidden=true href=#incorporating-deep-learning>#</a></h2><p><strong>Deep Learning</strong> has started to influence SLAM in various ways:</p><ul><li><strong>Place Recognition & Loop Closure:</strong> CNN-based descriptors have been developed to recognize places from sensor data. For LiDAR, researchers have created learning-based scan descriptors (e.g., PointNetVLAD, MinkLoc3D) that can tell if two LiDAR scans are from the same place even under different viewpoints. These learned descriptors can improve loop closure detection robustness over handcrafted methods. For instance, a neural network might learn to emphasize stable features (building outlines, road layout) and ignore transient ones (cars, pedestrians) when generating a scan embedding.</li><li><strong>Feature Extraction:</strong> Instead of using fixed edge/plane detection heuristics (like LOAM does), some recent works train neural networks to extract distinctive features or keypoints from point clouds. The idea is similar to learning features in vision (like SuperPoint, etc.), but for 3D data. These learned features could be more reliable in unstructured environments.</li><li><strong>End-to-End SLAM:</strong> There are experimental end-to-end approaches where a deep network is trained to directly estimate odometry from sensor input (learning an approximate localization without explicit mapping). Examples on LiDAR include learning-based odometry networks (often using occupancy grid as input to a CNN that outputs pose change). While not yet as accurate as classical methods for LiDAR, they show potential to handle sensor noise or failures by learned priors.</li><li><strong>Neural Mapping:</strong> A very cutting-edge direction is representing maps with neural networks (implicit representations). For example, <strong>PIN-SLAM</strong> (2021) represents the map as an implicit field (like a neural network that can output if a point is on a surface) instead of point clouds. Each new scan updates a neural map rather than a voxel grid. This is related to Neural Radiance Fields (NeRF) but for LiDAR geometry. The promise is a continuous, compressed map that can be optimized with gradient descent. It’s computationally heavy though.</li><li><strong>Semantic SLAM:</strong> Deep learning is heavily used to add semantic understanding to SLAM. Networks can segment LiDAR scans into car, pedestrian, ground, building, etc. This semantic info can make SLAM more robust: e.g., dynamic objects can be identified and excluded from mapping, or semantic landmarks (like “parking sign on a pole”) can be used for loop closure because they are recognizable. Some SLAM systems now build <strong>semantic maps</strong> (with object labels) alongside geometric maps.</li><li><strong>Learning Localization Models:</strong> Given enough data, a network can learn to regress the pose of the vehicle given sensor input by recognizing the scene (this is like relocalization). Companies working on self-driving often train networks on LiDAR data for localization against a known map – effectively encoding the map in the network weights to get a fast localization system that is robust to partial observations.</li></ul><p>Overall, deep learning is complementing SLAM rather than replacing it. The <strong>strongest trend</strong> is using learning for perception tasks within SLAM: place recognition, feature extraction, segmentation. The core SLAM estimation (pose optimization) still relies on classical optimization or filtering, but learning helps provide better data association and loop candidates (which addresses some long-standing hard parts of SLAM).</p><h2 id=edge-computing-and-resource-constrained-slam>Edge Computing and Resource-Constrained SLAM<a hidden class=anchor aria-hidden=true href=#edge-computing-and-resource-constrained-slam>#</a></h2><p>As SLAM moves onto smaller devices (drones, AR glasses, IoT robots), there is a push for <strong>lightweight SLAM</strong> that can run on edge hardware with limited compute, memory, and power:</p><ul><li><strong>Algorithmic Optimization:</strong> Techniques like keyframe selection (processing only some frames), submap-based mapping, and sparsification of point clouds (like using feature points instead of full cloud) reduce load. KISS-ICP (2022) is an example of a recent LiDAR odometry that is extremely simple and efficient, yet as accurate as heavier methods, suitable for embedded use.</li><li><strong>Hardware Acceleration:</strong> There’s work on FPGA or ASIC implementations of SLAM components (scan matching, filter update). For example, building a dedicated accelerator for occupancy grid update or for nearest neighbor search in ICP can speed up SLAM on a robot with low-power chips.</li><li><strong>Streaming and Communication:</strong> In multi-robot or cloud-connected scenarios, sometimes the heavy lifting can be offloaded: e.g., a robot sends data to a more powerful computer (edge server) that runs the SLAM and sends back the pose/map updates. The term <strong>Edge SLAM</strong> can also imply distributing the SLAM computation between the robot and edge/cloud. This requires dealing with network latency and bandwidth limits (e.g., compressing LiDAR data).</li><li><strong>Map Compression:</strong> Storing a large map on an embedded device is challenging. Octrees help, but also people use <strong>map pruning</strong> (discarding parts of the map not needed or using lower detail for far away areas) or even procedural map generation (store boundaries of free space instead of every cell). Some research uses learning to compress maps (autoencoders for point clouds, etc.).</li><li><strong>Real-time Constraints:</strong> On edge devices, real-time performance is critical. This has led to development of <strong>constant-time SLAM</strong> algorithms (where complexity doesn’t grow over time). For example, some SLAM systems limit the number of active landmarks to a fixed number by replacing older ones, thereby keeping computation bounded. Others use sliding window optimization (only optimize the last N poses, not the whole trajectory, except when closing a loop).</li></ul><p>For industry engineers, an important recent development is that robust open-source SLAM is available that can run on modest hardware (e.g., Cartographer can run on a Raspberry Pi for 2D, and newer 3D methods like LIO-SAM can run on a GPU-equipped laptop in real-time). The emphasis now is on <strong>portability and efficiency</strong> so that SLAM can be deployed on everything from a tiny microcontroller to a large cloud server, depending on use-case.</p><h2 id=multi-agent-and-distributed-slam>Multi-Agent and Distributed SLAM<a hidden class=anchor aria-hidden=true href=#multi-agent-and-distributed-slam>#</a></h2><p>With the rise of multiple robots working together, <strong>Multi-Agent SLAM</strong> has gained attention:</p><ul><li>In multi-agent SLAM, several robots explore an environment and need to build a common map and understand their relative poses. The challenge is that they start with unknown relative positions.</li><li><strong>Map Merging:</strong> When two robots meet or have overlap in view, they must recognize each other’s maps (data association across robots) and then merge the maps into one consistent global map. This is like a loop closure, but between two different maps. Techniques involve finding common landmarks or places (perhaps using global coordinates if available, or by direct communication of scans when in proximity).</li><li><strong>Distributed Optimization:</strong> Instead of sending all data to a central node, robots might each run their own SLAM and only exchange summarized information (like loop closure constraints between their coordinate frames). Distributed pose graph optimization algorithms exist that converge to the same result as a centralized one but through message passing between robots.</li><li><strong>Communication constraints:</strong> Multi-agent SLAM has to deal with limited bandwidth. Robots might share compressed maps or only share when they expect a loop closure. For example, a drone might broadcast a signature of its recent map; if another drone recognizes that signature from its own map, they then perform a more detailed exchange to align and merge.</li><li><strong>Use-cases:</strong> Multi-agent SLAM is crucial in search-and-rescue (teams of drones mapping a collapsed building), warehouse fleets (ensuring all robots agree on the world map), and autonomous driving fleets (sharing map updates). In 2020, for instance, DARPA’s Subterranean Challenge pushed multi-agent SLAM in caves/tunnels; teams developed systems where ground robots and drones would share mapping data to efficiently cover large areas.</li></ul><p>A concrete example: Assume two LiDAR-equipped rovers start at different ends of a mine. They each build a map. When their maps (frontiers) meet in the middle, they need to align. They could use a handshake procedure: when they detect each other via sensors or communication, they exchange some scans of the area. Running ICP between Rover A’s scan and Rover B’s scan finds the transform to align their coordinate frames. Then they merge maps and continue with one common map. This requires careful coordination to avoid double-counting overlapped areas and to handle any slight discrepancies.</p><p>Multi-agent SLAM is essentially adding another layer of loop closure (between maps of different agents). Modern frameworks like LAMP (by NASA) or Cartographer have extensions for multi-robot mapping.</p><h2 id=robustness-and-resilience>Robustness and Resilience<a hidden class=anchor aria-hidden=true href=#robustness-and-resilience>#</a></h2><p>Robustness has been a recurring theme, but new approaches continue to improve SLAM’s ability to handle tough scenarios:</p><ul><li><strong>Robust Estimation:</strong> Using robust loss functions in optimization (Huber loss, etc.) to reduce influence of outliers (like a false loop closure or a mis-matched feature). Some state-of-the-art systems automatically detect inconsistent loops and either reject or down-weight them (e.g., switchable constraints, dynamic covariance scaling).</li><li><strong>Dynamic Environments:</strong> As mentioned, integrating computer vision or AI to identify dynamic objects and remove them from consideration. Some LiDAR SLAM methods now include tracking of moving objects concurrently with SLAM – effectively doing SLAM in dynamic environments by treating moving objects separately (SLAM++ concept for rigid objects).</li><li><strong>Changing Environments (Long-term SLAM):</strong> Over weeks or months, environments change (furniture moves, seasons change foliage, construction happens). Recent research addresses how to update maps over time or localize in a map that’s somewhat outdated. One approach is <strong>meta-SLAM</strong>: maintain multiple map hypotheses or an ever-updating map. Another is to incorporate environment-change detection (if a known wall is suddenly gone, treat it as a change rather than a SLAM error).</li><li><strong>Failure Detection:</strong> New SLAM systems might monitor their own health (e.g., the covariance of the filter or the graph residual errors). If a sudden jump or inconsistency is detected, they can signal an alert or try to re-initialize. For example, if tracking loss occurs (too few features), a visual SLAM might switch to relocalization mode.</li><li><strong>Precision and Calibration:</strong> Advanced methods do <strong>online sensor calibration</strong> (extrinsics/intrinsics) during SLAM. For LiDAR-IMU SLAM, some algorithms can fine-tune the timing/extrinsic offset between LiDAR and IMU in situ to improve consistency. This makes the system more robust to slight miscalibrations that would otherwise degrade accuracy.</li><li><strong>High-Definition Maps & SLAM:</strong> In autonomous driving, often a prior HD map is available. SLAM in that case can be simplified to <strong>localization against a known map</strong>. But if the map is slightly wrong or outdated, the SLAM must be robust to those differences (not get confused by a missing sign or an extra temporary barrier). Combining SLAM (building a local map) with map-matching (aligning to a global map) is an area of development; it provides redundancy and robustness (if one fails, the other might carry on).</li></ul><p>Finally, <strong>benchmarking and datasets</strong> have pushed robustness: New datasets include adverse weather (rain, snow for LiDAR), aggressive motions, and diverse scenes. SLAM algorithms are evolving to handle these: e.g., using LiDAR returns intensity and number of returns to deal with fog, or combining radar with LiDAR for robust sensing in heavy rain (radar SLAM is an emerging field because radar can see through fog/rain better, albeit at lower resolution).</p><p>In summary, the SLAM landscape in 2025 is quite rich. We see classical geometric methods enhanced with learning (for perception), systems getting lighter and more deployable on small devices, multiple agents collaborating in mapping, and a focus on making SLAM work <strong>reliably</strong> in the real world’s messy conditions. Frameworks like <strong>LIO-SAM</strong>demonstrate how integrating IMU, loop closures via scan context, and factor graph optimization yields a system that outperforms its predecessors. As compute power grows and algorithms mature, we can expect SLAM to become a standard component in most autonomous systems – often running in the background, robustly providing localization and mapping without much babysitting. The continued research into SLAM ensures that issues like scalability, robustness, and versatility will be incrementally solved, making truly autonomous exploration and operation feasible in the coming years.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] J. Lee and C. Hwang, &ldquo;Lidar SLAM: The Ultimate Guide to Simultaneous Localization and Mapping,&rdquo; Wevolver, Mar. 2023. [Online]. Available: <a href=https://www.wevolver.com/article/lidar-slam>https://www.wevolver.com/article/lidar-slam</a></p><p>[2] C. Fernández-Caramés, V. Moreno, B. Curto, and F. J. Rodríguez, &ldquo;A Review of Simultaneous Localization and Mapping for the Robotic-Based Nondestructive Evaluation of Infrastructures,&rdquo; Sensors, vol. 25, no. 3, p. 712, Jan. 2025. [Online]. Available: <a href=https://www.mdpi.com/1424-8220/25/3/712>https://www.mdpi.com/1424-8220/25/3/712</a></p><p>[3] L. Tai, J. Zhang, M. Liu, and W. Burgard, &ldquo;A Review of Simultaneous Localization and Mapping Algorithms,&rdquo; World Electric Vehicle Journal, vol. 16, no. 2, p. 56, Feb. 2024. [Online]. Available: <a href=https://www.mdpi.com/2032-6653/16/2/56>https://www.mdpi.com/2032-6653/16/2/56</a></p><p>[4] H. Peng, J. Y. Huang, F. Shen, S. S. Song, and Y. Liu, &ldquo;OMU: A Probabilistic 3D Occupancy Mapping Accelerator for Real-Time Applications,&rdquo; arXiv preprint arXiv:2205.03325, May 2022. [Online]. Available: <a href=https://arxiv.org/pdf/2205.03325>https://arxiv.org/pdf/2205.03325</a></p><p>[5] S. Xu, H. Lin, and H. Zhang, &ldquo;LIO-GVM: an Accurate, Tightly-Coupled Lidar-Inertial Odometry with Gaussian Voxel Map,&rdquo; arXiv preprint arXiv:2306.17436v3, Jun. 2023. [Online]. Available: <a href=https://arxiv.org/html/2306.17436v3>https://arxiv.org/html/2306.17436v3</a></p><p>[6] S. Lee, C. Park, and H. Myung, &ldquo;LiDAR-Based SLAM under Semantic Constraints in Dynamic Environments,&rdquo; Remote Sensing, vol. 13, no. 18, p. 3651, Sep. 2021. [Online]. Available: <a href=https://www.mdpi.com/2072-4292/13/18/3651>https://www.mdpi.com/2072-4292/13/18/3651</a></p><p>[7] X. Chen, A. Milioto, E. Palazzolo, P. Giguère, J. Behley, and C. Stachniss, &ldquo;Stabilize an Unsupervised Feature Learning for LiDAR-Based Place Recognition,&rdquo; Biorobotics Lab, CMU, 2018. [Online]. Available: <a href=http://biorobotics.ri.cmu.edu/papers/paperUploads/Xu_Stabilize_Unsupervised_2018.pdf>http://biorobotics.ri.cmu.edu/papers/paperUploads/Xu_Stabilize_Unsupervised_2018.pdf</a></p><p>[8] L. Pan, R. Hartley, M. Liu, and C. Stachniss, &ldquo;PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation,&rdquo; IEEE Transactions on Robotics, vol. 40, no. 2, pp. 1271-1289, Apr. 2024. [Online]. Available: <a href=https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/pan2024tro.pdf>https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/pan2024tro.pdf</a></p><p>[9] A. Schaefer, L. Luft, and W. Burgard, &ldquo;Robust Factor Graphs for Pose Graph SLAM,&rdquo; Technical University of Chemnitz, 2022. [Online]. Available: <a href=https://www.tu-chemnitz.de/etit/proaut/en/research/robustslam.html>https://www.tu-chemnitz.de/etit/proaut/en/research/robustslam.html</a></p><p>[10] &ldquo;Iterative closest point,&rdquo; Wikipedia, 2024. [Online]. Available: <a href=https://en.wikipedia.org/wiki/Iterative_closest_point>https://en.wikipedia.org/wiki/Iterative_closest_point</a>. [Accessed: Feb. 1, 2025]</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/simultaneous-localization-and-mapping/>Simultaneous Localization and Mapping</a></li><li><a href=https://livey.github.io/tags/slam/>SLAM</a></li><li><a href=https://livey.github.io/tags/lidar/>LiDAR</a></li><li><a href=https://livey.github.io/tags/autonomous-driving/>Autonomous Driving</a></li><li><a href=https://livey.github.io/tags/iterative-closest-point/>Iterative Closest Point</a></li></ul><nav class=paginav><a class=prev href=https://livey.github.io/posts/2025-03-05-reinforcement-learning/><span class=title>« Prev</span><br><span>Understanding Reinforcement Learning: Concepts, Algorithms, and Applications</span>
</a><a class=next href=https://livey.github.io/posts/2025-02-07-coordinate/><span class=title>Next »</span><br><span>Coordinate Systems in Autonomous Driving</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on x" href="https://x.com/intent/tweet/?text=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&amp;hashtags=SimultaneousLocalizationandMapping%2cSLAM%2cLiDAR%2cAutonomousDriving%2cIterativeClosestPoint"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&amp;title=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29&amp;summary=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&title=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on whatsapp" href="https://api.whatsapp.com/send?text=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on telegram" href="https://telegram.me/share/url?text=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Simultaneous Localization and Mapping (SLAM) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
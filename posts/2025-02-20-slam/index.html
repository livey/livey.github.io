<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LiDAR-SLAM Decoded: From Point Clouds to Precision Maps | Fuwei's Tech Notes</title>
<meta name=keywords content="Simultaneous Localization and Mapping,SLAM,LiDAR,Autonomous Driving,Iterative Closest Point,LiDAR-SLAM,Point Cloud,Mapping,Localization"><meta name=description content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-02-20-slam/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-02-20-slam/"><meta property="og:title" content="LiDAR-SLAM Decoded: From Point Clouds to Precision Maps"><meta property="og:description" content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="LiDAR-SLAM Decoded: From Point Clouds to Precision Maps"><meta name=twitter:description content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-02-20-slam/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="LiDAR-SLAM Decoded: From Point Clouds to Precision Maps"><meta property="og:description" content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-19T00:00:00+00:00"><meta property="article:tag" content="Simultaneous Localization and Mapping"><meta property="article:tag" content="SLAM"><meta property="article:tag" content="LiDAR"><meta property="article:tag" content="Autonomous Driving"><meta property="article:tag" content="Iterative Closest Point"><meta property="article:tag" content="LiDAR-SLAM"><meta property="og:image" content="https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E"><meta name=twitter:title content="LiDAR-SLAM Decoded: From Point Clouds to Precision Maps"><meta name=twitter:description content="An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LiDAR-SLAM Decoded: From Point Clouds to Precision Maps","item":"https://livey.github.io/posts/2025-02-20-slam/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LiDAR-SLAM Decoded: From Point Clouds to Precision Maps","name":"LiDAR-SLAM Decoded: From Point Clouds to Precision Maps","description":"An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges.","keywords":["Simultaneous Localization and Mapping","SLAM","LiDAR","Autonomous Driving","Iterative Closest Point","LiDAR-SLAM","Point Cloud","Mapping","Localization"],"articleBody":"What is SLAM? SLAM demo. SLAM stands for Simultaneous Localization and Mapping. It is a computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it. Applications Object Detection Parking Lot Annotation Lane Annotation Lane Reprojection HD Map [source] SLAM has various applications, including:\nObject Detection\nUndistorted LiDAR point clouds are fed into 3D object detection models to identify objects such as vehicles and pedestrians within the environment.\nParking Lot Annotation\nFirst, SLAM is used to acquire a map of the parking lot. A rendering method then generates a ground-level map to allow for more accurate annotation.\nLane Annotation\nSimilar to parking lot annotation, SLAM creates a high-resolution ground map, which is then used to accurately annotate traffic lanes and road markings.\nHigh-Definition (HD) Map Creation\nSLAM is a foundational technology for building HD maps. The dense 3D point cloud of the environment is processed to extract detailed information about road geometry, signs, and other critical features.\nVisual SLAM vs. LiDAR SLAM Visual SLAM (left) vs. LiDAR SLAM (right) Feature Visual SLAM LiDAR SLAM Hardware Cost Cheap Expensive Algorithm Complex Simple Geometry Accuracy No accurate geometry Accurate geometry Basics of LiDAR Mechanical LiDAR Solid-State LiDAR Hybrid LiDAR Different types of LiDAR [source] There are different types of LiDAR technologies:\nMechanic LiDAR\nPros: 360-degree, high performance, and lower cost Cons: Wear and failure over time Use cases: Data collection, testing, Robotaxi Solid-State LiDAR\nPros: Simple, cheap, reliable Cons: Low power density, short detection distance, near-range blind spot LiDAR Hybrid LiDAR\nPros: Low cost, highly reliable, long distance detection Cons: Limited field of view Use cases: Front-facing LiDAR Problem Formulation The core problems in SLAM can be formulated as optimization problems:\nMapping\n$$\\underset{\\mathbf{m}}{\\text{min}} \\sum_{t=1}^{T}||\\mathbf{z}_{t}-h(\\mathbf{x}_{t},\\mathbf{m})||_{2}^{2}$$ Localization\n$$\\underset{\\mathbf{x}_{1:T}}{\\text{min}} \\sum_{t=1}^{T}||\\mathbf{z}_{t}-h(\\mathbf{x}_{t},\\mathbf{m})||_{2}^{2}$$ SLAM\n$$\\underset{\\mathbf{x}_{1:T},\\mathbf{m}}{\\text{min}} \\sum_{t=1}^{T}||\\mathbf{z}_{t}-h(\\mathbf{x}_{t},\\mathbf{m})||_{2}^{2}$$ Where:\n$\\mathbf{x}$ is the pose of the robot. $\\mathbf{z}$ is the observation. $h(.)$ is the observation function. $\\mathbf{m}$ is the map. Undistorting LiDAR Points Distorted LiDAR Points (vehicle in motion) Undistorted LiDAR Points (motion compensated) To accurately process LiDAR data, it’s necessary to undistort the points. This is particularly important when the ego vehicle moves. Undistortion can be achieved using an Inertial Measurement Unit (IMU), or a motion model that can provide short-term accurate movement information.\nIterative Closest Point (ICP) ICP (Iterative Closest Point) aligning two point clouds. Recall that when doing SLAM, the pose of the robot and the map of the environment are unknown and tightly coupled, creating a “chicken-and-egg” problem. However, we observe that the first scan of the LiDAR already provides a rough estimate of the environment. Without loss of generality, we can set the first scan as the rough map and its corresponding pose as the identity matrix. Then, we can match the following scans to the map and update the pose of the robot. The commonly used algorithm for matching is the Iterative Closest Point (ICP).\nICP is a key algorithm used in SLAM. The process involves:\nAssociation: $$j^{*}=\\underset{j\\in\\{1,...,M\\}}{\\text{argmin}}||\\mathbf{R}\\mathbf{x}_{i}+\\mathbf{t}-\\mathbf{y}_{j}||_{2}$$ Minimization: $$\\underset{\\mathbf{R},\\mathbf{t}}{\\text{argmin}}\\quad E(\\mathbf{R},\\mathbf{t})=\\sum_{i=1}^{N}e_{i}(\\mathbf{R},\\mathbf{t})^{2}=\\sum_{i=1}^{N}||\\mathbf{R}\\mathbf{x}_{i}+\\mathbf{t}-\\mathbf{y}_{j}||_{2}^{2}$$ We continue these two steps iteratively until the robot’s pose converges. For a more detailed explanation of ICP, please refer to Iterative Closest Point Uncovered: Mathematical Foundations and Applications.\nMotion Model Because SLAM relies on estimating the relative pose between the current scan and either previous scans or a map, incorporating motion priors can significantly improve performance. A vehicle motion model—leveraging control data such as IMU readings, steering angle, and wheel speed—provides valuable prior information that enhances SLAM accuracy. With a motion model, the SLAM problem can be formulated as follows:\n$$ \\begin{aligned} \\underset{\\mathbf{m},\\mathbf{x}_{1:T}}{\\text{min}}\u0026 \\sum_{t=1}^{T}||\\mathbf{z}_{t}-h(\\mathbf{x}_{t},\\mathbf{m})||_{2}^{2} \\\\ \\text{s.t. } \u0026\\mathbf{x}_{t}=f(\\mathbf{x}_{t-1},\\mathbf{u}_{t})+ \\mathbf{n}_{t} \\text{ for } t=1,...,T \\end{aligned} \\tag{1} $$where $\\mathbf{u}_{t}$ is the control input at time $t$, and $\\mathbf{n}_{t}$ is the noise.\nKalman Filter Problem (1) is a sequential optimization problem. The Kalman Filter provides an efficient way to solve it. It involves two steps:\nPredict: Predict the next pose based on the current pose and control inputs. $$ \\mathbf{x}_{t} = f(\\mathbf{x}_{t-1}, \\mathbf{u}_{t}) + \\mathbf{n}_{t} $$ Update: Update the current pose using the current observation. $$ \\min_{\\mathbf{m},\\mathbf{x}_{t}} \\| \\mathbf{z}_{t} - h(\\mathbf{x}_{t}, \\mathbf{m}) \\|_{2}^{2} $$Please refer to Demystifying Kalman Filters: From Classical Estimation to Bayesian Inference for more details.\nPost-Processing The ground surface before post-processing. SLAM post-processing. Ground surface after post-processing. As we can see, the map is built by appending scans one after another, which leads to an accumulation of errors. Therefore, post-processing is necessary to address the long-term drift. The process of post-processing is to optimize the map and the robot’s pose jointly. First, we must find the points that belong to the same feature (e.g., a surface, a line, etc.). Then, we identify the poses that correspond to the points. Finally, we make the feature conform to its natural shape by optimizing the poses—for example, making a surface feature more like a plane or a line feature more like a straight line.\nSLAM Challenges in Autonomous Driving Key challenges of LiDAR SLAM in autonomous driving include:\nHigh-Speed Scenarios\nWhen a vehicle travels at high speeds, the resulting point clouds become sparse, leading to less overlap between consecutive scans. This sparsity makes it difficult to accurately match scans and can degrade the quality of the localization and mapping.\nLarge-Scale Mapping\nMapping extensive areas, such as entire cities, consumes significant memory resources. Furthermore, performing post-processing and global optimization on such large maps is computationally expensive and challenging to manage.\nHighly Repetitive Environments\nEnvironments like highways and tunnels are often highly repetitive, with few unique features. This geometric similarity makes it difficult for matching algorithms to determine the relative poses, which makes positioning challenging.\nDynamic Environments\nAutonomous driving environments are inherently dynamic, filled with moving objects like other vehicles and pedestrians. Most SLAM algorithms operate on the fundamental assumption that the environment is static. Therefore, additional techniques are required to identify, track, and remove these dynamic elements from the point cloud to prevent them from corrupting the map and vehicle’s localization.\n","wordCount":"985","inLanguage":"en","image":"https://livey.github.io/posts/2025-02-20-slam/%3Cimage%20path/url%3E","datePublished":"2025-07-19T00:00:00Z","dateModified":"2025-07-19T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-02-20-slam/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LiDAR-SLAM Decoded: From Point Clouds to Precision Maps</h1><div class=post-description>An in-depth tutorial on Simultaneous Localization and Mapping (SLAM), covering its fundamental concepts, real-world applications, and key challenges.</div><div class=post-meta><span title='2025-07-19 00:00:00 +0000 UTC'>July 19, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;985 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#what-is-slam aria-label="What is SLAM?">What is SLAM?</a></li><li><a href=#applications aria-label=Applications>Applications</a></li><li><a href=#visual-slam-vs-lidar-slam aria-label="Visual SLAM vs. LiDAR SLAM">Visual SLAM vs. LiDAR SLAM</a></li><li><a href=#basics-of-lidar aria-label="Basics of LiDAR">Basics of LiDAR</a></li><li><a href=#problem-formulation aria-label="Problem Formulation">Problem Formulation</a></li><li><a href=#undistorting-lidar-points aria-label="Undistorting LiDAR Points">Undistorting LiDAR Points</a></li><li><a href=#iterative-closest-point-icp aria-label="Iterative Closest Point (ICP)">Iterative Closest Point (ICP)</a></li><li><a href=#motion-model aria-label="Motion Model">Motion Model</a></li><li><a href=#kalman-filter aria-label="Kalman Filter">Kalman Filter</a></li><li><a href=#post-processing aria-label=Post-Processing>Post-Processing</a></li><li><a href=#slam-challenges-in-autonomous-driving aria-label="SLAM Challenges in Autonomous Driving">SLAM Challenges in Autonomous Driving</a></li></ul></div></details></div><div class=post-content><h1 id=what-is-slam>What is SLAM?<a hidden class=anchor aria-hidden=true href=#what-is-slam>#</a></h1><figure style=text-align:center><img src=./resources/SLAM-intro.gif alt="SLAM introduction animation" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>SLAM demo.</figcaption></figure>SLAM stands for Simultaneous Localization and Mapping. It is a computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.<h1 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h1><figure style=display:flex;flex-wrap:wrap;justify-content:center;gap:16px><div style="flex:1 1 300px;max-width:300px;text-align:center"><img src=./resources/slam-app-object-box.png alt="SLAM Application: Object Detection" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Object Detection</figcaption></div><div style="flex:1 1 300px;max-width:300px;text-align:center"><img src=./resources/slam-app-parking.jpeg alt="SLAM Application: Parking" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Parking Lot Annotation</figcaption></div><div style="flex:1 1 300px;max-width:300px;text-align:center"><img src=./resources/slam-app-lane.jpeg alt="SLAM Application: Lane Annotation" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Lane Annotation</figcaption></div><div style="flex:1 1 300px;max-width:300px;text-align:center"><img src=./resources/slam-app-lane-project.png alt="SLAM Application: Lane Reprojection" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Lane Reprojection</figcaption></div><div style="flex:1 1 300px;max-width:300px;text-align:center"><img src=./resources/slam-app-hdmap.png alt="SLAM Application: HD Map" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>HD Map <a href=https://mscvprojects.ri.cmu.edu/2020teamg/project/ target=_blank rel=noopener>[source]</a></figcaption></div></figure><p>SLAM has various applications, including:</p><p><strong>Object Detection</strong></p><p>Undistorted LiDAR point clouds are fed into 3D object detection models to identify objects such as vehicles and pedestrians within the environment.</p><p><strong>Parking Lot Annotation</strong></p><p>First, SLAM is used to acquire a map of the parking lot. A rendering method then generates a ground-level map to allow for more accurate annotation.</p><p><strong>Lane Annotation</strong></p><p>Similar to parking lot annotation, SLAM creates a high-resolution ground map, which is then used to accurately annotate traffic lanes and road markings.</p><p><strong>High-Definition (HD) Map Creation</strong></p><p>SLAM is a foundational technology for building HD maps. The dense 3D point cloud of the environment is processed to extract detailed information about road geometry, signs, and other critical features.</p><h1 id=visual-slam-vs-lidar-slam>Visual SLAM vs. LiDAR SLAM<a hidden class=anchor aria-hidden=true href=#visual-slam-vs-lidar-slam>#</a></h1><figure style=display:flex;flex-direction:row;justify-content:center;gap:16px;margin-bottom:8px><div style="flex:1 1 45%;max-width:45%;text-align:center"><img src=./resources/visual-slam.png alt="Visual SLAM Example" style=width:100%;border-radius:8px></div><div style="flex:1 1 45%;max-width:45%;text-align:center"><img src=./resources/lidar-slam.png alt="LiDAR SLAM Example" style=width:100%;border-radius:8px></div></figure><div style=text-align:center;font-size:.95em;margin-bottom:24px>Visual SLAM (left) vs. LiDAR SLAM (right)</div><table><thead><tr><th style=text-align:left>Feature</th><th style=text-align:left>Visual SLAM</th><th style=text-align:left>LiDAR SLAM</th></tr></thead><tbody><tr><td style=text-align:left>Hardware Cost</td><td style=text-align:left>Cheap</td><td style=text-align:left>Expensive</td></tr><tr><td style=text-align:left>Algorithm</td><td style=text-align:left>Complex</td><td style=text-align:left>Simple</td></tr><tr><td style=text-align:left>Geometry Accuracy</td><td style=text-align:left>No accurate geometry</td><td style=text-align:left>Accurate geometry</td></tr></tbody></table><h1 id=basics-of-lidar>Basics of LiDAR<a hidden class=anchor aria-hidden=true href=#basics-of-lidar>#</a></h1><figure style=display:flex;flex-direction:row;justify-content:center;gap:16px;margin-bottom:8px><div style="flex:1 1 33%;max-width:33%;text-align:center"><img src=./resources/mechanical-lidar.gif alt="Mechanical LiDAR" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Mechanical LiDAR</figcaption></div><div style="flex:1 1 33%;max-width:33%;text-align:center"><img src=./resources/solid-state-lidar.gif alt="Solid-State LiDAR" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Solid-State LiDAR</figcaption></div><div style="flex:1 1 33%;max-width:33%;text-align:center"><img src=./resources/hybrid-lidar.png alt="Hybrid LiDAR" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Hybrid LiDAR</figcaption></div></figure><div style=text-align:center;font-size:.95em;margin-bottom:24px>Different types of LiDAR <a href=https://www.hesaitech.com/things-you-need-to-know-about-lidar-solid-state-and-hybrid-solid-state-whats-the-difference/ target=_blank rel=noopener>[source]</a></div><p>There are different types of LiDAR technologies:</p><ul><li><p><strong>Mechanic LiDAR</strong></p><ul><li><strong>Pros:</strong> 360-degree, high performance, and lower cost</li><li><strong>Cons:</strong> Wear and failure over time</li><li><strong>Use cases:</strong> Data collection, testing, Robotaxi</li></ul></li><li><p><strong>Solid-State LiDAR</strong></p><ul><li><strong>Pros:</strong> Simple, cheap, reliable</li><li><strong>Cons:</strong> Low power density, short detection distance, near-range blind spot LiDAR</li></ul></li><li><p><strong>Hybrid LiDAR</strong></p><ul><li><strong>Pros:</strong> Low cost, highly reliable, long distance detection</li><li><strong>Cons:</strong> Limited field of view</li><li><strong>Use cases:</strong> Front-facing LiDAR</li></ul></li></ul><h1 id=problem-formulation>Problem Formulation<a hidden class=anchor aria-hidden=true href=#problem-formulation>#</a></h1><p>The core problems in SLAM can be formulated as optimization problems:</p><ul><li><p><strong>Mapping</strong></p>$$\underset{\mathbf{m}}{\text{min}} \sum_{t=1}^{T}||\mathbf{z}_{t}-h(\mathbf{x}_{t},\mathbf{m})||_{2}^{2}$$</li><li><p><strong>Localization</strong></p>$$\underset{\mathbf{x}_{1:T}}{\text{min}} \sum_{t=1}^{T}||\mathbf{z}_{t}-h(\mathbf{x}_{t},\mathbf{m})||_{2}^{2}$$</li><li><p><strong>SLAM</strong></p>$$\underset{\mathbf{x}_{1:T},\mathbf{m}}{\text{min}} \sum_{t=1}^{T}||\mathbf{z}_{t}-h(\mathbf{x}_{t},\mathbf{m})||_{2}^{2}$$</li></ul><p>Where:</p><ul><li>$\mathbf{x}$ is the pose of the robot.</li><li>$\mathbf{z}$ is the observation.</li><li>$h(.)$ is the observation function.</li><li>$\mathbf{m}$ is the map.</li></ul><h1 id=undistorting-lidar-points>Undistorting LiDAR Points<a hidden class=anchor aria-hidden=true href=#undistorting-lidar-points>#</a></h1><figure style=display:flex;justify-content:center;gap:32px;margin-bottom:12px><div style="flex:1 1 45%;max-width:45%;text-align:center"><img src=./resources/lidar-points-distorted.png alt="Distorted LiDAR Points" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Distorted LiDAR Points (vehicle in motion)</figcaption></div><div style="flex:1 1 45%;max-width:45%;text-align:center"><img src=./resources/lidar-points-undistorted.png alt="Undistorted LiDAR Points" style=width:100%;border-radius:8px><figcaption style=font-size:.95em;margin-top:4px>Undistorted LiDAR Points (motion compensated)</figcaption></div></figure><p>To accurately process LiDAR data, it&rsquo;s necessary to undistort the points. This is particularly important when the ego vehicle moves. Undistortion can be achieved using an Inertial Measurement Unit (IMU), or a motion model that can provide short-term accurate movement information.</p><h1 id=iterative-closest-point-icp>Iterative Closest Point (ICP)<a hidden class=anchor aria-hidden=true href=#iterative-closest-point-icp>#</a></h1><figure style=text-align:center;margin-bottom:16px><img src=./resources/icp-ani.gif alt="ICP Animation" style=width:60%;max-width:400px;border-radius:8px;display:block;margin-left:auto;margin-right:auto><figcaption style=font-size:.95em;margin-top:4px>ICP (Iterative Closest Point) aligning two point clouds.</figcaption></figure><p>Recall that when doing SLAM, the pose of the robot and the map of the environment are unknown and tightly coupled, creating a &ldquo;chicken-and-egg&rdquo; problem. However, we observe that the first scan of the LiDAR already provides a rough estimate of the environment. Without loss of generality, we can set the first scan as the rough map and its corresponding pose as the identity matrix. Then, we can match the following scans to the map and update the pose of the robot. The commonly used algorithm for matching is the Iterative Closest Point (ICP).</p><p>ICP is a key algorithm used in SLAM. The process involves:</p><ol><li><strong>Association</strong>:
$$j^{*}=\underset{j\in\{1,...,M\}}{\text{argmin}}||\mathbf{R}\mathbf{x}_{i}+\mathbf{t}-\mathbf{y}_{j}||_{2}$$</li><li><strong>Minimization</strong>:
$$\underset{\mathbf{R},\mathbf{t}}{\text{argmin}}\quad E(\mathbf{R},\mathbf{t})=\sum_{i=1}^{N}e_{i}(\mathbf{R},\mathbf{t})^{2}=\sum_{i=1}^{N}||\mathbf{R}\mathbf{x}_{i}+\mathbf{t}-\mathbf{y}_{j}||_{2}^{2}$$</li></ol><p>We continue these two steps iteratively until the robot&rsquo;s pose converges. For a more detailed explanation of ICP, please refer to <a href=https://livey.github.io/posts/2024-12-icp/>Iterative Closest Point Uncovered: Mathematical Foundations and Applications</a>.</p><h1 id=motion-model>Motion Model<a hidden class=anchor aria-hidden=true href=#motion-model>#</a></h1><p>Because SLAM relies on estimating the relative pose between the current scan and either previous scans or a map, incorporating motion priors can significantly improve performance. A vehicle motion model—leveraging control data such as IMU readings, steering angle, and wheel speed—provides valuable prior information that enhances SLAM accuracy. With a motion model, the SLAM problem can be formulated as follows:</p>$$
\begin{aligned}
\underset{\mathbf{m},\mathbf{x}_{1:T}}{\text{min}}& \sum_{t=1}^{T}||\mathbf{z}_{t}-h(\mathbf{x}_{t},\mathbf{m})||_{2}^{2} \\
\text{s.t. } &\mathbf{x}_{t}=f(\mathbf{x}_{t-1},\mathbf{u}_{t})+ \mathbf{n}_{t} \text{ for } t=1,...,T
\end{aligned}
\tag{1}
$$<p>where $\mathbf{u}_{t}$ is the control input at time $t$, and $\mathbf{n}_{t}$ is the noise.</p><h1 id=kalman-filter>Kalman Filter<a hidden class=anchor aria-hidden=true href=#kalman-filter>#</a></h1><p>Problem (1) is a sequential optimization problem. The Kalman Filter provides an efficient way to solve it.
It involves two steps:</p><ul><li><strong>Predict:</strong> Predict the next pose based on the current pose and control inputs.</li></ul>$$
\mathbf{x}_{t} = f(\mathbf{x}_{t-1}, \mathbf{u}_{t}) + \mathbf{n}_{t}
$$<ul><li><strong>Update:</strong> Update the current pose using the current observation.</li></ul>$$
\min_{\mathbf{m},\mathbf{x}_{t}} \| \mathbf{z}_{t} - h(\mathbf{x}_{t}, \mathbf{m}) \|_{2}^{2}
$$<p>Please refer to <a href=https://livey.github.io/posts/2025-03-22-kalman-filters/>Demystifying Kalman Filters: From Classical Estimation to Bayesian Inference</a> for more details.</p><h1 id=post-processing>Post-Processing<a hidden class=anchor aria-hidden=true href=#post-processing>#</a></h1><figure style=text-align:center><img src=./resources/slam-post-before.png alt="Ground surface before post-processing" style="width:100%;margin:0 auto;display:block"><figcaption style=font-size:.95em;margin-top:4px>The ground surface before post-processing.</figcaption></figure><figure style=text-align:center><img src=./resources/slam-post.png alt="SLAM post-processing" style="width:100%;margin:0 auto;display:block"><figcaption style=font-size:.95em;margin-top:4px>SLAM post-processing.</figcaption></figure><figure style=text-align:center><img src=./resources/slam-post-after.png alt="Ground surface after post-processing" style="width:100%;margin:0 auto;display:block"><figcaption style=font-size:.95em;margin-top:4px>Ground surface after post-processing.</figcaption></figure><p>As we can see, the map is built by appending scans one after another, which leads to an accumulation of errors. Therefore, post-processing is necessary to address the long-term drift. The process of post-processing is to optimize the map and the robot&rsquo;s pose jointly. First, we must find the points that belong to the same feature (e.g., a surface, a line, etc.). Then, we identify the poses that correspond to the points. Finally, we make the feature conform to its natural shape by optimizing the poses—for example, making a surface feature more like a plane or a line feature more like a straight line.</p><h1 id=slam-challenges-in-autonomous-driving>SLAM Challenges in Autonomous Driving<a hidden class=anchor aria-hidden=true href=#slam-challenges-in-autonomous-driving>#</a></h1><p>Key challenges of LiDAR SLAM in autonomous driving include:</p><p><strong>High-Speed Scenarios</strong></p><p>When a vehicle travels at high speeds, the resulting point clouds become sparse, leading to less overlap between consecutive scans. This sparsity makes it difficult to accurately match scans and can degrade the quality of the localization and mapping.</p><p><strong>Large-Scale Mapping</strong></p><p>Mapping extensive areas, such as entire cities, consumes significant memory resources. Furthermore, performing post-processing and global optimization on such large maps is computationally expensive and challenging to manage.</p><p><strong>Highly Repetitive Environments</strong></p><p>Environments like highways and tunnels are often highly repetitive, with few unique features. This geometric similarity makes it difficult for matching algorithms to determine the relative poses, which makes positioning challenging.</p><p><strong>Dynamic Environments</strong></p><p>Autonomous driving environments are inherently dynamic, filled with moving objects like other vehicles and pedestrians. Most SLAM algorithms operate on the fundamental assumption that the environment is static. Therefore, additional techniques are required to identify, track, and remove these dynamic elements from the point cloud to prevent them from corrupting the map and vehicle&rsquo;s localization.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/simultaneous-localization-and-mapping/>Simultaneous Localization and Mapping</a></li><li><a href=https://livey.github.io/tags/slam/>SLAM</a></li><li><a href=https://livey.github.io/tags/lidar/>LiDAR</a></li><li><a href=https://livey.github.io/tags/autonomous-driving/>Autonomous Driving</a></li><li><a href=https://livey.github.io/tags/iterative-closest-point/>Iterative Closest Point</a></li><li><a href=https://livey.github.io/tags/lidar-slam/>LiDAR-SLAM</a></li><li><a href=https://livey.github.io/tags/point-cloud/>Point Cloud</a></li><li><a href=https://livey.github.io/tags/mapping/>Mapping</a></li><li><a href=https://livey.github.io/tags/localization/>Localization</a></li></ul><nav class=paginav><a class=next href=https://livey.github.io/posts/2025-0515-pnp/><span class=title>Next »</span><br><span>Perspective and Point (PnP) Problem</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on x" href="https://x.com/intent/tweet/?text=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&amp;hashtags=SimultaneousLocalizationandMapping%2cSLAM%2cLiDAR%2cAutonomousDriving%2cIterativeClosestPoint%2cLiDAR-SLAM%2cPointCloud%2cMapping%2cLocalization"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&amp;title=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps&amp;summary=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f&title=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on whatsapp" href="https://api.whatsapp.com/send?text=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on telegram" href="https://telegram.me/share/url?text=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LiDAR-SLAM Decoded: From Point Clouds to Precision Maps on ycombinator" href="https://news.ycombinator.com/submitlink?t=LiDAR-SLAM%20Decoded%3a%20From%20Point%20Clouds%20to%20Precision%20Maps&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-02-20-slam%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning | Fuwei's Tech Notes</title>
<meta name=keywords content="reinforcement learning"><meta name=description content="This is an introduction to reinforcement learning."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-03-05-reinforcement-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="This is an introduction to reinforcement learning."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-03-05-reinforcement-learning/"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="This is an introduction to reinforcement learning."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="This is an introduction to reinforcement learning."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-03-05-reinforcement-learning/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="This is an introduction to reinforcement learning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-05T00:00:00+00:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="og:image" content="https://livey.github.io/posts/2025-03-05-reinforcement-learning/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-03-05-reinforcement-learning/%3Cimage%20path/url%3E"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="This is an introduction to reinforcement learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning","item":"https://livey.github.io/posts/2025-03-05-reinforcement-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning","name":"Reinforcement Learning","description":"This is an introduction to reinforcement learning.","keywords":["reinforcement learning"],"articleBody":"I have taken some reinforcement learning related courses during my Ph.D. study. However, I have not touched it for a long time. Recently, I am working on end-to-end autonomous driving and the success of reinforcement learning in large language models brings me back to this topic. In this post, we will introduce the basic concepts and commonly used algorithms in reinforcement learning. More detailed information can be found in the reference book [1] and OpenAI RL page.\nThe interaction between environment and agent. [1] The Environment and Agent The environment is the world in which the agent operates. The agent is the entity that makes decisions and takes actions. We denote the core components of reinforcement learning as follows (following the notations in [1] with some abused notations between scalar and vector):\n$S$: environment state $A$: agent action $R$: environment reward after taking an action under certain state, it should reflect the agent’s goal; $p(S_{t+1}, R_{t+1}|S_{0:t}, A_{0:t})$: the transition probability that given the history states and actions, the reward $R_{t+1}$ and the next state $S_{t+1}$ $\\pi(A|S)$: the policy that the agent follows given the state $S$, it can be random or deterministic $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the return, where $\\gamma$ is the discount factor. The discount is not necessary, for example, in the episodic case. The return can also be defined by the average of the rewards. $v(s) = \\mathbb{E}[G_t|S_t = s]$ is the value function of state $s$. It is the expected return from state $s$. $q(s, a) = \\mathbb{E}[G_t|S_t = s, A_t = a]$ is the value function of state-action pair $(s, a)$. It is the expected return from state-action pair $(s, a)$. $A(s, a) = Q(s, a) - V(s)$ is the advantage function of state-action pair $(s, a)$. It is the difference between the value function of state-action pair $(s, a)$ and the value function of state $s$. Prediction: we care more about the state value function $v(s)$; (In prediction, we need to predict how well the state is.) Control: we care more about the state-action value function $q(s, a)$. (In control, we need to choose the best action in a state.) When we write the expectation, there are usually only two sources of randomness: the environment transition and the policy. Sometimes, we may omit them for simplicity.\nMarkov Decision Process (MDP) In reinforcement learning, we usually assume the environment is a Markov Decision Process, i.e., mathematically,\n$$ p(S_{t+1}, R_{t+1}| S_{0:t}, A_{0:t}) = p(S_{t+1}, R_{t+1}| S_t, A_t). $$Bellman Equation Recap the definition of the value function,\n$$ \\begin{aligned} v(s) \u0026= \\mathbb{E}[G_t|S_t = s] \\\\ \u0026= \\mathbb{E}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} |S_t = s]\\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} |S_t = s] \\\\ \u0026= \\mathbb{E}_{S_{t+1}|S_t}\\mathbb{E}_{\\cdot|S_{t+1}, S_t}[R_{t+1} + \\gamma G_{t+1}] \\\\ \u0026=\\mathbb{E}_{S_{t+1}|S_t}\\mathbb{E}_{\\cdot|S_{t+1}}[R_{t+1} + \\gamma G_{t+1}]\\\\ \u0026= \\sum_{a, s', r}\\pi(a|s) p(s', r|s, a) [r + \\gamma\\mathbb{E}_{\\cdot|S_{t+1}=s'} (G_{t+1}|S_{t+1}=s')]\\\\ \u0026 = \\sum_{a, s', r}\\pi(a|s) p(s', r|s, a) [r + \\gamma v(s')]\\\\ \\end{aligned} \\tag{1} $$Similarly, the Bellman equation for the action-value function is\n$$ \\begin{aligned} q(s, a) \u0026= \\mathbb{E}[G_t|S_t = s, A_t = a]\\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a]\\\\ \u0026= \\mathbb{E}_{S_{t+1}|S_t, A_t}\\mathbb{E}_{\\cdot|S_{t+1}, S_t, A_t}[R_{t+1} + \\gamma G_{t+1}]\\\\ \u0026=\\mathbb{E}_{S_{t+1}|S_t, A_t}\\mathbb{E}_{\\cdot|S_{t+1}}[R_{t+1} + \\gamma G_{t+1}]\\\\ \u0026=\\sum_{r, s'}p(s', r|s, a)[r + \\gamma\\mathbb{E}_{\\cdot|S_{t+1}=s'}[G_{t+1}|S_{t+1}=s']]\\\\ \u0026 = \\sum_{r, s'}p(s', r|s, a)[r + \\gamma\\mathbb{E}_{A_{t+1}=a'|S_k=s}\\mathbb{E}_{\\cdot|S_{k=1}, A_{k+1}}G_{k+1}]\\\\ \u0026 = \\sum_{r, s'}p(s', r|s, a)[r + \\gamma\\mathbb{E}_{A_{t+1}=a'|S_k=s}q(s', a')]\\\\ \u0026= \\sum_{r, s'}p(s', r|s, a)[r + \\gamma\\sum_{a'}\\pi(a'|s')q(s', a')] \\end{aligned}\\tag{2} $$From the above derivation, we can also see the relationship between the value function and the action-value function,\n$$ v(s) = \\sum_{a}\\pi(a|s)q(s, a) $$$$ q(s, a) = \\sum_{s', r}p(s', r|s, a)[r + \\gamma v(s')] $$Solving the Bellman Equation Let’s write the Bellman value function as\n$$ \\begin{aligned} \\mathcal{H} \\circ v(s) \u0026= \\sum_{a}\\pi(a|s)\\sum_{s', r} p(s', r|s, a)[r + \\gamma v(s')]\\\\ \u0026= \\sum_{r} p(r|s) r + \\gamma \\sum_{s'} p(s'|s) v(s') \\\\ \u0026= \\bar{R}_s + \\gamma P_s \\vec{v}(s) \\end{aligned} $$where $\\bar{R}_s$ is the expected reward from state $s$, $P_s$ is the transition probability from state $s$ to all other states. Writing above in the matrix form, we have\n$$ \\mathcal{H} \\circ \\vec{v} = \\bar{\\mathbf{R}} + \\gamma \\mathbf{P} \\vec{v} $$We can then solve the Bellman equation by solving the linear system,\n$$ \\vec{v} = \\bar{\\mathbf{R}} + \\gamma \\mathbf{P} \\vec{v} $$We can also resort to the iterative method by observing that the $\\mathcal{H}$ is a contraction mapping, for any two vectors $\\mathbf{x}$ and $\\mathbf{y}$,\n$$ \\|\\mathcal{H} \\circ \\mathbf{x} - \\mathcal{H} \\circ \\mathbf{y}\\| = \\|\\gamma \\mathbf{P} (\\mathbf{x} - \\mathbf{y})\\| \\le \\gamma \\|\\mathbf{P} \\|\\|\\mathbf{x} - \\mathbf{y}\\| \\le \\gamma \\|\\mathbf{x} - \\mathbf{y}\\| \u003c \\|\\mathbf{x} - \\mathbf{y}\\| $$ This allows us to use the fixed point method to solve the Bellman equation.\nSimilarly, we can also prove that the operation in the Bellman equation for the action-value function is a contraction mapping.\nOptimal Policy The optimal policy, $\\pi_*$, satisfies\n$$\\forall s, \\quad v_{\\pi_*}(s) \\ge v_\\pi(s)$$That is to say, the optimal policy will yield a greater or equal value function than any other policy for any state. The optimal policy may not be unique, but all optimal policies share the same optimal value function, $v_*$.\nOptimal policy Bellman equation $$ \\begin{aligned} v_*(s) \u0026= \\max_{a} q_*(s, a) \\\\ \u0026= \\max_{a} \\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\ \u0026= \\max_{a} \\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = a] \\\\ \u0026= \\max_{a} \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_*(s')] \\end{aligned} $$ and the state-action value function of the optimal policy satisfies,\n$$ \\begin{aligned} q_*(s, a) \u0026= \\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma v_*(s')|S_t = s, A_t = a] \\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q_*(s', a')|S_t = s, A_t = a] \\\\ \u0026= \\sum_{s', r} p(s', r|s, a)[r + \\gamma \\max_{a'} q_*(s', a')] \\end{aligned} $$Similar to the ordinary Bellman equation, we can also prove it is a contraction mapping, and then use the fixed point method to solve it.\nSuppose there are two optimal value function $v_1$ and $v_2$, the Bellman operator is $\\mathcal{H}$. Then,\n$$ \\begin{aligned} \\|\\mathcal{H}v_1 - \\mathcal{H}v_2 \\|_\\infty \u0026= \\max_{s} | \\mathcal{H}v_1(s) - \\mathcal{H}v_2(s) | \\\\ \u0026= \\max_{s} | \\max_{a} \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_1(s')] - \\max_{a} \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_2(s')] | \\\\ \u0026\\underset{\\le}{(a)} \\max_{s, a} | \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_1(s')] - \\sum_{s', r} p(s', r|s, a)[r + \\gamma v_2(s')] | \\\\ \u0026= \\max_{s, a} \\gamma |\\sum_{s'} p(s'|s, a)[v_1(s') - v_2(s')] | \\\\ \u0026\\le \\gamma \\max_{s, a} \\sum_{s'} p(s'|s, a) \\cdot \\max_{s'} \\{|v_1(s') - v_2(s')|\\} \\\\ \u0026= \\gamma \\|\\vec{v_1} - \\vec{v_2}\\|_\\infty \\end{aligned} $$ where (a) holds because of triangle inequality, $|\\|x\\| - \\|y\\| | \\le \\|x - y\\| \\le \\|x\\| + \\|y\\|$. Similarly, for the optimal action-value Bellman equation, we have\n$$ \\begin{aligned} \\|\\mathcal{H}q_1 - \\mathcal{H}q_2 \\|_\\infty \u0026= \\max_{s, a} | \\mathcal{H}q_1(s, a) - \\mathcal{H}q_2(s, a) | \\\\ \u0026= \\max_{s, a} | \\sum_{s', r} p(s', r|s, a')[r + \\gamma \\max_{a'} q_1(s', a')] - \\sum_{s', r} p(s', r|s, a')[r + \\gamma \\max_{a'} q_2(s', a')] | \\\\ \u0026\\le \\gamma\\max_{s, a} \\sum_{s'} p(s'| s, a)|\\max_{a'}q_1(s', a') - \\max_{a'}q_2(s', a')| \\| \\\\ \u0026\\le \\gamma \\max_{s, a} \\sum_{s'} p(s'| s, a) \\cdot \\max_{a'} \\{|q_1(s', a') - q_2(s', a')|\\} \\\\ \u0026\\le \\gamma \\max_{s', a'} \\{|q_1(s', a') - q_2(s', a')|\\} \\\\ \u0026= \\gamma \\|\\vec{q_1} - \\vec{q_2}\\|_\\infty \\end{aligned} $$Thus we prove that Bellman operator is a contraction mapping for both state and state-action value functions.\nDynamic Programming Policy evaluation Using the fixed point method, we can iteratively\n$$ v_{k+1}(s) = \\sum_{a}\\pi(a|s)\\sum_{s', r} p(s', r|s, a)[r + \\gamma v_k(s')] $$By running this several times, we get the state/state-action value function under policy $\\pi$. However, we should note that this method requires full knowledge of the environment, i.e. the transition probability $p(s', r|s, a)$.\nPolicy improvement Suppose we have two deterministic policies, $\\pi$ and $\\pi'$, such that\n$$ q(s, \\pi'(s)) \\ge v(s) \\quad \\forall s \\in \\mathcal{S} $$Then, the policy $\\pi'$ is as good as or better than $\\pi$, i.e.\n$$ v_{\\pi'}(s) \\ge v_\\pi(s) \\quad \\forall s \\in \\mathcal{S} $$This can be proved below\n$$ \\begin{aligned} v_{\\pi}(s) \u0026\\leq q_{\\pi}(s, \\pi'(s)) \\\\ \u0026= \\mathbb{E} [ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s, A_t = \\pi'(s)] \\\\ \u0026= \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s] \\\\ \u0026\\leq \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1})) \\mid S_t = s] \\\\ \u0026= \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma \\mathbb{E} [ R_{t+2} + \\gamma v_{\\pi}(S_{t+2}) \\mid S_{t+1}, A_{t+1} = \\pi'(S_{t+1})] \\mid S_t = s] \\\\ \u0026= \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_{\\pi}(S_{t+2}) \\mid S_t = s] \\\\ \u0026\\leq \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 v_{\\pi}(S_{t+3}) \\mid S_t = s] \\\\ \u0026\\vdots \\\\ \u0026\\leq \\mathbb{E}_{\\pi'} [ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\cdots \\mid S_t = s] \\\\ \u0026= v_{\\pi'}(s). \\end{aligned} $$The third equation holds because $\\pi'$ is a deterministic policy.\nPolicy improvement takes a greedy step on the original policy with respect to the value function of the original policy,\n$$ \\begin{aligned} \\pi'(s) \u0026= \\underset{a}{\\text{argmax}} q_\\pi(s, a) \\\\ \u0026= \\underset{a}{\\text{argmax}} \\sum_{s', r} p(s', r| s, a) [r + \\gamma v_\\pi(s')] \\end{aligned} $$If the new policy $\\pi'$ is as good as but not better than the old policy $\\pi$, according to the optimal Bellman equation, it is optimal.\nPolicy and value iteration $$ \\pi_0 \\xrightarrow{\\mathbf{E}} v_{\\pi_0} \\xrightarrow{\\mathbf{I}} \\pi_1 \\xrightarrow{\\mathbf{E}} v_{\\pi_1} \\xrightarrow{\\mathbf{I}} \\pi_2 \\xrightarrow{\\mathbf{E}} \\cdots \\xrightarrow{\\mathbf{I}} \\pi_* \\xrightarrow{\\mathbf{E}} v_* $$Monte Carlo Methods According to the definition of the value function, if we sample the whole trajectory, we can estimate the value function by the average of the returns.\nThe first visit Monte Carlo method. [1] We can also evaluate the state-action value function and improve the policy as:\nMonte Carlo Exploring Starts. [1] Off-policy prediction via importance sampling Suppose there are two policies, $b$ is for behavior policy, we simulate the process according to $b$ and generate the episodes. $\\pi$ is the target policy, we want to learn the state or state-action value function under $\\pi$. To use the importance sampling method, we need follow the coverage assumption, i.e. every action taken under $\\pi$ is also taken, at least occasionally, under $b$, which requires $\\pi(a|s) \u003e 0$ implies $b(a|s) \u003e 0$.\nImportance sampling is a general technique used to estimate expected values under one distribution while sampling from another. In reinforcement learning, we use it to estimate values under the target policy $\\pi$ while sampling trajectories from the behavior policy $b$.\nLet’s consider a trajectory $\\tau = (S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T)$ of length $T$. The probability of this trajectory under policy $\\pi$ is:\n$$ p_{\\pi}(\\tau) = p(S_0) \\prod_{t=0}^{T-1} \\pi(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t) $$Similarly, the probability under the behavior policy $b$ is:\n$$ p_{b}(\\tau) = p(S_0) \\prod_{t=0}^{T-1} b(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t) $$The importance sampling ratio is:\n$$ \\rho_{0:T-1} = \\frac{p_{\\pi}(\\tau)}{p_{b}(\\tau)} = \\frac{p(S_0) \\prod_{t=0}^{T-1} \\pi(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)}{p(S_0) \\prod_{t=0}^{T-1} b(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)} = \\prod_{t=0}^{T-1}\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} $$The environment dynamics terms cancel out, leaving only the ratio of policy probabilities. We can use this importance sampling ratio to correct the returns when estimating value functions:\n$$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] = \\mathbb{E}_{b}[\\rho_{t:T-1} G_t|S_t=s] $$Temporal-Difference Learning $$ V(S_t) \\gets V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right] $$which uses the currently sampled reward $R_{t+1}$ and current next state value $V(S_{t+1})$ to update the current state value $V(S_t)$. This represents a moving average of the current state value $V(S_t)$ and the estimated state value, $R_{t+1} + \\gamma V(S_{t+1})$.\nThis quantity,\n$$ \\delta_t \\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), $$called the TD error arises in various forms throughout reinforcement learning.\nTabular TD(0) for estimation state value function. (Chapter 6 of [1]) Convergent conditions Ergodicity of the environment’s Markov chain For TD(0) to work, the environment’s Markov chain, defined by the policy, must be ergodic. This means:\nThe environment is stationary; You can reach any state from any other state eventually; (imagine what happens if there is a dead cycle) All states are visited infinitely often, which is crucial for the algorithm to learn about every state. step size and Robbins-Monro Framework The step size $\\alpha_t$ plays a pivotal role in ensuring convergence, and research aligns with the stochastic approximation theory, particularly the Robbins-Monro conditions. These conditions are:\n$\\sum_{t=1}^{\\infty} \\alpha_t = \\infty$, ensuring that the updates are significant enough initially to overcome random fluctuations and explore the value space;\n$\\sum_{t=1}^{\\infty} \\alpha_t^2 \u003c \\infty$, ensuring that the updates eventually become small enough to allow convergence without excessive oscillation.\nA typical choice is $\\alpha_t = \\frac{1}{t}$. Though we set the step size to satisfy the Robbins-Monro conditions, other choices are beneficial for non-stationary environments.\nPlease find the full proof in [2].\nAdvantages of TD methods “TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions. The next most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an online, fully incremental fashion.” [1]\nSarsa: On-policy TD control $$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]. $$ Sarsa algorithm. (Chapter 6 of [1]) Q-learning: Off-policy TD control $$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right]. $$ Q-learning algorithm. (Chapter 6 of [1]) The expected Sarsa algorithm is a variant of the Q-learning algorithm, which uses the expected value of the next state-action value function instead of the maximum value.\n$$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_{a}\\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t, A_t) \\right]. $$Bootstrapping Unified view of the TD, Monte Carlo, and DP methods Let’s define the n-step return as\n$$ G_{t:t+n} \\triangleq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}), $$Then, you can pick any $n$ step return as the estimation of the value function. The following shows the backup diagram of the TD, Monte Carlo, and DP methods.\nBackup diagrams of TD, Monte Carlo, and DP methods. [fromDavid Silver's RL course lecture 4, \"Model-Free Prediction] Policy Gradient Methods In this section, we will introduce the policy gradient methods. These methods directly learn a parameterized policy and select actions directly from the policy. We may still need to learn the value function. Let’s denote $\\pi(a|s;\\theta)= P(A_t=a | S_t =s; \\boldsymbol{\\theta})$ as the policy, where $\\boldsymbol{\\theta}$ is the parameter. Let $v(s; \\mathbf{w})$ be the value function, where $\\mathbf{w}$ is the parameter.\nPolicy Gradient Theorem In this section, we consider the episodic case, where the process starts with some initial state $s_0$ (not random). Our goal is to maximize the expected return starting from $s_0$, i.e. $J(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\pi}[G_0|S_0=s_0]$.\nFor a state value function $v(s)$, it is a function of the policy. So, we can compute the gradient of the policy with respect to policy parameter, $\\boldsymbol{\\theta}$. The gradient can be computed by\n$$ \\begin{aligned} \\nabla v(s) \u0026= \\nabla \\sum_{a} \\pi(a|s) q(s, a) \\\\ \u0026= \\sum_{a} \\nabla \\pi(a|s)q(s, a) + \\sum_a \\pi(a|s) \\nabla q(s, a) \\\\ \u0026=\\sum_a \\nabla \\pi(a|s)q(s, a) + \\sum_a \\pi(a|s) \\nabla \\sum_{s', r} p(s', r|s, a)[r + v(s')] \\\\ \u0026= \\sum_a \\nabla \\pi(a|s)q(s, a) + \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s, a) \\nabla v(s') \\\\ \u0026\\underset{=}{(a)}\\sum_a \\nabla \\pi(a|s)q(s, a) + \\sum_{s'} p(s'|s) \\nabla v(s')\\\\ \u0026= \\sum_a \\nabla \\pi(a|s)q(s, a) + \\sum_{s'} p(s'|s)\\big[ \\\\ \u0026\\quad\\sum_{a'} \\nabla \\pi(a'|s')q(s', a') + \\sum_{s''}p(s''|s')\\nabla v(s'')\\big] \\text{\\qquad (by rollout equation (a))}\\\\ \u0026= \\sum_a \\nabla \\pi(a|s)q(s, a) + \\sum_{s'}\\sum_a p(s'|s)\\nabla \\pi(a|s')q(s', a) +\\sum_{s''}p(s''|s)\\nabla v(s'') \\\\ \u0026= \\sum_{x \\in \\mathcal{S}} \\sum_{K=0}^\\infty P(s\\to x, K, \\pi) \\sum_a \\nabla \\pi(a|x)q(x, a) \\end{aligned} $$ where $P(s\\to x, K, \\pi)$ is the probability of starting from state $s$ and reaching state $x$ in $K$ steps under policy $\\pi$.\n$$ \\begin{aligned} \\nabla J(\\boldsymbol{\\theta}) \u0026= \\sum_{s\\in \\mathcal{S}} \\sum_{K=0}^\\infty P(s_0\\to x, K, \\pi) \\sum_a \\nabla \\pi(a|x)q(x, a) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} \\left(\\sum_{K=0}^\\infty P(s_0\\to s, K, \\pi)\\right) \\sum_a \\nabla \\pi(a|s)q(s, a) \\\\ \u0026=\\sum_{s \\in \\mathcal{S}} \\eta(s) \\sum_a\\nabla \\pi(a|s)q(s, a) \\\\ \u0026=\\sum_{s'}\\eta(s') \\sum_s \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a\\nabla \\pi(a|s)q(s, a) \\\\ \u0026= \\sum_{s'}\\eta(s') \\sum_s \\mu(s) \\sum_a\\nabla \\pi(a|s)q(s, a) \\\\ \u0026\\propto \\sum_s \\mu(s) \\sum_a\\nabla \\pi(a|s)q(s, a) \\end{aligned} $$where $\\eta(s)$ only depends on the policy and the environment, which can be seen the probability of visiting state $s$. The above equation is derived by letting the discount factor $\\gamma = 1$. If it is the continuous case, the proportional is replaced by equality, for episodic case, the constant term is the average length of visit state $s$. Note that, if we consider the discounted reward, the general conclusion does not change. In this case, $P(s\\to x, K, \\pi)$ will means the discounted probability of visiting state $x$ in $K$ steps under policy $\\pi$.\nThen, if we follow the policy $\\pi$, the gradient can be computed by\n$$ \\nabla J(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\pi}[\\sum_a q(S, a; \\mathbf{w}) \\nabla \\pi(a|S;\\boldsymbol{\\theta})] $$Then, we can instantiate the stochastic gradient ascent algorithm to update the policy parameter:\n$$ \\boldsymbol{\\theta}_{t+1} \\gets \\boldsymbol{\\theta}_t + \\alpha \\sum_a q(S_t, a; \\mathbf{w}_t) \\nabla \\pi(a|S_t;\\boldsymbol{\\theta}_t) $$REINFORCE algorithm $$ \\begin{aligned} \\nabla J(\\boldsymbol{\\theta}) \u0026= \\mathbb{E}_{\\pi}[\\sum_a q(S, a; \\mathbf{w}) \\nabla \\pi(a|S;\\boldsymbol{\\theta})]\\\\ \u0026= \\mathbb{E}_{\\pi}[\\sum_a \\pi(a|S; \\boldsymbol{\\theta}) q(S, a; \\mathbf{w})\\frac{\\nabla \\pi(a|S;\\boldsymbol{\\theta})}{\\pi(a|S; \\boldsymbol{\\theta})}]\\\\ \u0026= \\mathbb{E}_{\\pi}[q(S, A; \\mathbf{w})\\frac{\\nabla \\pi(a|S;\\boldsymbol{\\theta})}{\\pi(a|S; \\boldsymbol{\\theta})}] \\\\ \u0026= \\mathbb{E}_{\\pi}[q(S, A; \\mathbf{w})\\nabla \\ln \\pi(A|S; \\boldsymbol{\\theta})] \\\\ \u0026= \\mathbb{E}_{\\pi}[G_t \\nabla \\ln \\pi(A_t|S_t; \\boldsymbol{\\theta})] \\quad (\\text{because } \\mathbb{E} [q(S, A; \\mathbf{w})|S, A] = G_t) \\end{aligned} $$$$ \\boldsymbol{\\theta}_{t+1} \\gets \\boldsymbol{\\theta}_t + \\alpha G_t \\nabla \\ln \\pi(A_t|S_t; \\boldsymbol{\\theta}_t) $$REINFORCE with baseline We can also add a baseline to the REINFORCE algorithm to reduce the variance. The baseline can be any random variable as long as it is independent with the action.\n$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) (q(s, a; \\mathbf{w}) - b(s)) $$This is verified by\n$$ \\begin{aligned} \\nabla J(\\boldsymbol{\\theta}) \u0026\\propto \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a|s) (q(s, a; \\mathbf{w}) - b(s)) \\\\ \u0026\\propto \\sum_s \\mu(s) [\\sum_a \\nabla \\pi(a|s)q(s, a; \\mathbf{w}) - \\sum_a \\nabla \\pi(a|s)b(s)] \\\\ \u0026= \\sum_s \\mu(s) [\\sum_a \\nabla \\pi(a|s)q(s, a; \\mathbf{w}) - b(s)\\nabla\\sum_a \\pi(a|s)] \\\\ \u0026= \\sum_s \\mu(s) \\sum_a q(s, a)\\nabla \\pi(a|s) \\end{aligned} $$A natural choice of the baseline is the state value function. So, the REINFORCE with baseline algorithm is:\nREINFORCE with baseline. (Chapter 13 of [1]) Actor-Critic Methods Actor-critic method use one-step TD to estimate the full return in the REINFORCE algorithm. In actor-critic method, we need estimate two functions: the advantage function and the policy.\nActor-Critic algorithm. (Chapter 13 of [1]) Trust Region Policy Optimization $$ \\begin{aligned} L_{\\pi}(\\tilde{\\pi}) \u0026= v(\\pi) + \\sum_{t=0}^\\infty \\sum_s P(s_t =s | \\pi) \\sum_a \\tilde{\\pi}(a|s) A_\\pi(s, a) \\\\ \u0026= v_\\pi + \\sum_{s} \\rho_{\\pi}(s) \\sum_{a} \\tilde{\\pi}(a \\mid s) A_{\\pi}(s, a). \\end{aligned} $$ where $\\rho_{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s \\mid \\pi)$ is the sum of the discounted visitation probability of state $s$ under policy $\\pi$.\nLet $D_{KL}^{\\max}(\\pi, \\tilde{\\pi}) = \\max_s D_{KL}(\\pi(\\cdot|s), \\tilde{\\pi}(\\cdot|s))$ be the maximum KL divergence between the two policies.\nTheorem: Let $\\pi$ and $\\tilde{\\pi}$ be two policies. Then the following bound holds:\n$$ v_{\\tilde{\\pi}} \\geq L_{\\pi}(\\tilde{\\pi}) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2 $$where $\\alpha = D_{KL}^{\\max}(\\pi, \\tilde{\\pi})$ and $\\epsilon = \\max_{s,a}|A_{\\pi}(s,a)|$.\nLet the right hand side of the above inequality be $M_{\\pi}(\\tilde{\\pi})$. We have $v_{\\tilde{\\pi}} \\geq M_{\\pi}(\\tilde{\\pi})$ and $v_{\\pi} = M_{\\pi}(\\pi)$. So, we have\n$$ v_{\\tilde{\\pi}} - v_\\pi = M_{\\pi}(\\tilde{\\pi}) - M_{\\pi}(\\pi). $$If we make an ascent step of $M_\\pi(\\pi)$, i.e., $M_\\pi(\\tilde{\\pi}) \u003e M_\\pi(\\pi)$, we will get a new policy $\\tilde{\\pi}$ that has a higher value function. Then it guarantees that we get a monotonic improved policies.\nIf we follow the theoretic objective $M_\\pi(\\tilde{\\pi})$, the constant term $\\frac{4\\epsilon \\gamma}{(1-\\gamma)^2}$, will be very large. Therefore, we place a significant constraint on the KL divergence. Thus, the new policy will likely follow the old policy. So, the author proposed solving\n$$ \\begin{aligned} \u0026\\underset{\\theta}{\\text{maximize}} \\quad L_{\\theta_{\\text{old}}}(\\theta) \\\\ \u0026\\text{subject to} \\quad \\overline{D}_{\\text{KL}}^{\\rho_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}}, \\theta) \\leq \\delta. \\end{aligned} $$where\n$$ \\begin{aligned} \\overline{D}_{\\text{KL}}^{\\rho}(\\theta_1, \\theta_2) \u0026= \\mathbb{E}_{s \\sim \\rho} \\left[ D_{\\text{KL}} \\big( \\pi_{\\theta_1}(\\cdot \\mid s) \\,\\big\\|\\, \\pi_{\\theta_2}(\\cdot \\mid s) \\big) \\right]. \\end{aligned} $$Sample-based estimation Let us write the full optimization problem as\n$$ \\begin{aligned} \u0026\\max_\\theta \\sum_s \\rho_{\\theta_{old}}(s)\\sum_a\\pi_\\theta(a|s)A_{\\theta_{old}}(s, a) \\\\ \u0026\\text{subject to} \\quad \\overline{D}_{\\text{KL}}^{\\rho_{\\theta_{old}}}(\\theta_{old}, \\theta) \\leq \\delta. \\end{aligned} $$If we use importance sampling to estimate the objective, we have\n$$ \\sum_a\\pi_\\theta(a|s)A_{\\theta_{old}}(s, a) = \\mathbb{E}_{a\\sim q}[\\frac{\\pi_\\theta(a|s)}{q(a|s)} A_{\\theta_{old}}(s, a)]. $$Then our optimization problem becomes\n$$ \\begin{aligned} \u0026\\max_\\theta \\mathbb{E}_{s\\sim\\rho_{\\theta_{old}}, a\\sim q}[\\frac{\\pi_\\theta(a|s)}{q(a|s)} A_{\\theta_{old}}(s, a)] \\\\ \u0026\\text{subject to} \\quad \\mathbb{E}_{s \\sim \\rho_{\\theta_{old}}} D_{\\text{KL}}^{\\rho_{\\theta_{old}}}(\\theta_{old}, \\theta) \\leq \\delta. \\end{aligned} $$Sampling methods The author proposed the uniform sampling method for discrete action space and $\\pi_{\\theta_{old}}(a|s)$ for continuous action space. It also suggested using the weighted importance sampling for large and continuous action spaces.\nProximal Policy Optimization This section is based on [4].\nThe objective of the PPO is\n$$ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_{t} \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t, \\text{clip}(\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]. $$ where $\\epsilon$ is a hyperparameter, say $\\epsilon = 0.2$, $\\hat{\\mathbb{E}}_{t}$ means we are doing mean over the older policy trajectory. The following figure shows the value of the objective value with respect to the ratio between the new policy and the older one. This clipping mechanism prevents excessive large policy updates.\nFunction value with respect to the ratio[4] Group relative policy optimization The method was proposed in [5].\nThe objective of the Group relative policy optimization (GRPO) is\n$$ \\begin{aligned} \u0026\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_{\\text{old}}} (O | q) } \\Bigg[ \\\\ \u0026\\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\Bigg\\{ \\min \\Bigg[ \\frac{\\pi_{\\theta} (o_{i,t} | q, o_{i, \u003c t})}{\\pi_{\\theta_{\\text{old}}} (o_{i,t} | q, o_{i, \u003c t})} \\hat{A}_{i,t}, \\operatorname{clip} \\Bigg( \\frac{\\pi_{\\theta} (o_{i,t} | q, o_{i, \u003c t})}{\\pi_{\\theta_{\\text{old}}} (o_{i,t} | q, o_{i, \u003c t})}, 1 - \\epsilon, 1 + \\epsilon \\Bigg) \\hat{A}_{i,t} \\Bigg] \\\\ \u0026- \\beta \\mathbb{D}_{KL} \\big[\\pi_{\\theta} \\| \\pi_{\\text{ref}} \\big] \\Bigg\\} \\Bigg], \\end{aligned} $$where the advantage $\\hat{A}_{i,t}$ is calculated by\n$$ \\hat{A}_{i,t} = \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}, $$where $r_i$ is the (i)th return in the group. $\\pi_{ref}$ is the policy continuing learned from the replay buffer.\nThe KL divergence is estimated by\n$$ \\mathbb{D}_{KL} \\big[\\pi_{\\theta} \\| \\pi_{\\text{ref}} \\big]=\\frac{\\pi_{\\text{ref}}}{\\pi_\\theta} - \\ln \\frac{\\pi_{\\text{ref}}}{\\pi_\\theta} - 1. $$This estimation is unbiased due to\n$$ \\begin{aligned} \\mathbb{E}_{a \\sim \\pi_\\theta}[\\frac{\\pi_{ref}(a)}{\\pi_\\theta(a)} - \\ln \\frac{\\pi_{ref}(a)}{\\pi_\\theta(a)} - 1] \u0026= \\mathbb{E}_{a} \\pi_{ref}(a) \\left( \\frac{\\pi_{ref}(a)}{\\pi_\\theta(a)} - \\ln \\frac{\\pi_{ref}(a)}{\\pi_\\theta(a)} - 1 \\right) \\\\ \u0026= -\\mathbb{E}_{a \\sim \\pi_\\theta} \\frac{\\pi_{ref}(a)}{\\pi_\\theta(a)} \\\\ \u0026= \\mathbb{D}_{KL} \\big[\\pi_{\\theta} \\| \\pi_{\\text{ref}} \\big]. \\end{aligned} $$It is also positive by observing the function $f(x) = x - \\ln x -1$.\nBy computing the relative advantage in the group, we can alleviate the need for using the state value function. Since a state value function for large language model might be as large as the model itself, this method dramatically reduces the computational cost.\nAppendix Why baseline can reduce the variance This proof was generated by ChatGPT and subsequently reviewed and revised by myself.\n1. Policy Gradient (REINFORCE): Let $\\pi_\\theta(a|s)$ be a parameterized policy giving the probability (or density) of action $a$ in state $s$. We seek to maximize the expected return $J(\\theta)$ (the cumulative reward). The policy gradient theorem (REINFORCE) gives the gradient of the objective as an expectation over the policy’s randomness. Formally, one version of the REINFORCE gradient is:\n$$ \\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\pi_\\theta}\\!\\Big[\\, G_t \\,\\nabla_{\\theta} \\ln \\pi_\\theta(A_t \\mid S_t)\\Big]\\,, $$where $G_t$ is the total return (cumulative reward) following time $t$, and the expectation $\\mathbb{E}_{\\pi_\\theta}[\\,\\cdot\\,]$ is taken over the trajectory distribution induced by $\\pi_\\theta$. In words, an unbiased stochastic estimator for the policy gradient is\n$$ \\hat{g} \\;=\\; G_t \\,\\nabla_{\\theta} \\ln \\pi_\\theta(A_t \\mid S_t)\\,, $$since $\\mathbb{E}[\\hat{g}] = \\nabla_\\theta J(\\theta)$ by the policy gradient theorem. This is the basis of the REINFORCE algorithm: adjust $\\theta$ in the direction of $G_t \\nabla_\\theta \\ln\\pi_\\theta(A_t|S_t)$ on each sampled trajectory.\n2. Adding a Baseline to the Gradient Estimator: While $\\hat{g}$ is an unbiased gradient estimator, it can have high variance, slowing convergence. A common technique to reduce variance is to subtract a baseline $b(s)$ from the return. The modified update uses\n$$ \\hat{g}_b \\;=\\; \\big(G_t - b(S_t)\\big)\\,\\nabla_{\\theta} \\ln \\pi_\\theta(A_t \\mid S_t)\\,, $$where $b(s)$ is any function of state (it can even be a constant). Importantly, $b(s)$ must not depend on the action $A_t$ (or any random outcome correlated with the action) – it should be independent of the sampling of $A_t$. This ensures the estimator remains unbiased. Intuitively, $b(s)$ is a baseline level of expected return against which the actual return $G_t$ is compared; $G_t - b(S_t)$ is often called the advantage. Choosing a good baseline (e.g. an estimate of the state’s value $V^\\pi(s)$) can greatly reduce variance without changing the expected value ().\n3. Unbiasedness with a Baseline: We now show that $\\hat{g}_b$ is an unbiased estimator of the true gradient $\\nabla_\\theta J(\\theta)$. Taking expectation (over all randomness in $S_t, A_t,$ and rewards):\n$$ \\begin{aligned} \\mathbb{E}\\!\\big[(G_t - b(S_t))\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)\\big] \u0026= \\mathbb{E}_{S_t}\\!\\Big[\\,\\mathbb{E}_{A_t\\sim\\pi}[\\, (G_t - b(S_t))\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t) \\mid S_t \\,] \\Big] \\\\ \u0026= \\mathbb{E}_{S_t}\\!\\Big[\\, (G_t - b(S_t)) \\,\\mathbb{E}_{A_t\\sim\\pi}[\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t) \\mid S_t\\,] \\Big]\\,, \\end{aligned} $$because $G_t$ and $b(S_t)$ are constant with respect to the inner expectation (conditional on $S_t$). Now, note the key identity for any fixed state $s$:\n$$ \\mathbb{E}_{A\\sim\\pi_\\theta(\\cdot|s)}\\!\\big[\\nabla_\\theta \\ln \\pi_\\theta(A|s)\\big] \\;=\\; \\nabla_\\theta \\int_a \\pi_\\theta(a|s)\\, da \\;=\\; \\nabla_\\theta\\,1 \\;=\\; \\mathbf{0}\\,. $$In other words, $\\mathbb{E}[\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t) \\mid S_t=s] = 0$ for all $s$. This follows from the fact that $\\int \\pi_\\theta(a|s)da=1$ and differentiating yields zero. Using this result above:\n$$ \\mathbb{E}\\!\\big[(G_t - b(S_t))\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)\\big] = \\mathbb{E}_{S_t}\\!\\Big[\\, (G_t - b(S_t)) \\cdot \\mathbf{0} \\Big] = 0 + \\mathbb{E}[G_t\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)]\\,, $$since the $b(S_t)$ term drops out. But $\\mathbb{E}[G_t\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)] = \\nabla_\\theta J(\\theta)$ by the REINFORCE formula. Therefore,\n$$ \\mathbb{E}[\\hat{g}_b] \\;=\\; \\mathbb{E}[(G_t - b(S_t))\\,\\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)] \\;=\\; \\nabla_\\theta J(\\theta)\\,. $$Conclusion: Adding the baseline does not change the expected gradient. The baseline term is an expectation-zero control variate – we’ve added a quantity whose mean is zero, so it doesn’t bias the estimator (machine learning - Advantage Function - Variance Reduction - Data Science Stack Exchange). Thus, $\\hat{g}_b$ is an unbiased gradient estimator, just like the original $\\hat{g}$ ().\n4. Variance Reduction via the Baseline: Although $\\hat{g}_b$ has the same mean as $\\hat{g}$, its variance can be much lower. We will compare $\\mathrm{Var}(\\hat{g}_b)$ to $\\mathrm{Var}(\\hat{g})$ and show the former is smaller (for an appropriate choice of $b$). For simplicity, consider one component of the gradient (one parameter dimension) and write $X = \\nabla_\\theta \\ln \\pi_\\theta(A_t|S_t)$ and $Y = G_t$ for shorthand. Then our original estimator is $U = X Y$ and the baseline estimator is $U_b = X (Y - b(S_t))$. We analyze the variance of these scalar random variables (the argument can be applied to each component of the gradient vector separately () ()):\nVariance without baseline: $\\displaystyle\\mathrm{Var}(U) \\;=\\; \\mathbb{E}[X^2 Y^2] \\;-\\; (\\mathbb{E}[X\\,Y])^2.$\nVariance with baseline: $\\displaystyle\\mathrm{Var}(U_b) \\;=\\; \\mathbb{E}[X^2 (Y - b(S_t))^2] \\;-\\; (\\mathbb{E}[X\\,(Y - b(S_t))])^2.$\nWe already know $\\mathbb{E}[X\\,(Y - b(S_t))] = \\mathbb{E}[X Y]$ (unbiasedness proved above). So:\n$$ \\begin{aligned} \\mathrm{Var}(U_b) \u0026= \\mathbb{E}[X^2 (Y - b(S_t))^2] - (\\mathbb{E}[X Y])^2, \\\\ \\text{where } \\mathbb{E}[X^2 (Y - b)^2] \u0026= \\mathbb{E}[X^2 (Y^2 - 2\\,bY + b^2)] \\\\ \u0026= \\mathbb{E}[X^2 Y^2] \\;-\\; 2\\,b\\,\\mathbb{E}[X^2 Y] \\;+\\; b^2\\,\\mathbb{E}[X^2]\\,. \\end{aligned} $$Now, the difference in variances is:\n$$ \\mathrm{Var}(U) - \\mathrm{Var}(U_b) \\;=\\; \\mathbb{E}[X^2 Y^2] - (\\mathbb{E}[XY])^2 \\;-\\; \\Big(\\mathbb{E}[X^2 (Y - b)^2] - (\\mathbb{E}[XY])^2\\Big)\\,. $$Cancelling the $(\\mathbb{E}[XY])^2$ terms and substituting the expanded form, we get:\n$$ \\mathrm{Var}(U) - \\mathrm{Var}(U_b) \\;=\\; \\mathbb{E}[X^2 Y^2] - \\Big(\\mathbb{E}[X^2 Y^2] - 2\\,b\\,\\mathbb{E}[X^2 Y] + b^2\\,\\mathbb{E}[X^2]\\Big) = 2\\,b\\,\\mathbb{E}[X^2 Y] \\;-\\; b^2\\,\\mathbb{E}[X^2]\\,. $$This expression is a quadratic function of $b$. To maximize the variance reduction, we can choose an optimal $b$. Setting the derivative with respect to $b$ to zero:\n$$ \\frac{d}{db}\\big(2\\,b\\,\\mathbb{E}[X^2 Y] - b^2\\,\\mathbb{E}[X^2]\\big) = 2\\,\\mathbb{E}[X^2 Y] - 2\\,b\\,\\mathbb{E}[X^2] = 0 \\implies b^* \\;=\\; \\frac{\\mathbb{E}[X^2 Y]}{\\mathbb{E}[X^2]}\\,. $$Plugging $b^*$ back in, the minimum achievable variance with an optimal baseline is:\n$$ \\mathrm{Var}(U_{b^*}) \\;=\\; \\mathbb{E}[X^2 Y^2] - (\\mathbb{E}[X^2 Y])^2/\\mathbb{E}[X^2] \\;-\\; (\\mathbb{E}[XY])^2\\,. $$Compare this to $\\mathrm{Var}(U) = \\mathbb{E}[X^2 Y^2] - (\\mathbb{E}[XY])^2$. The difference is\n$$ \\mathrm{Var}(U) - \\mathrm{Var}(U_{b^*}) \\;=\\; \\frac{\\big(\\mathbb{E}[X^2 Y]\\big)^2}{\\mathbb{E}[X^2]}\\,, $$which is nonnegative (indeed, a perfect square divided by $\\mathbb{E}[X^2]\u003e0$). Thus $\\mathrm{Var}(U_{b^*}) \\le \\mathrm{Var}(U)$. In other words, by choosing an appropriate baseline $b$, we can only decrease (or at worst, leave unchanged) the variance of the gradient estimator. Generally, a well-chosen baseline will significantly reduce variance. In fact, in policy gradient theory it is known that the variance is minimized when the baseline $b(s)$ equals the expected return from state $s$, i.e. the state-value $V^\\pi(s)$ (). In that case $Y - b(S_t) = G_t - V^\\pi(S_t)$ is the advantage $A^\\pi(S_t, A_t)$, which has expectation zero given $S_t$. Using the advantage $A(s,a) = Q^\\pi(s,a) - V^\\pi(s)$ in the update yields the smallest possible variance for an unbiased estimator ().\n5. Summary: We have proven that adding a baseline $b(S_t)$ to the REINFORCE policy gradient estimator leaves its expectation unchanged (unbiased) while reducing its variance. The baseline effectively acts as a control variate – a zero-mean adjustment that removes unnecessary fluctuation (machine learning - Advantage Function - Variance Reduction - Data Science Stack Exchange). By appropriate choice of $b(s)$ (often an estimate of $V^\\pi(s)$), the variance of the gradient estimator is provably lowered without introducing bias (). This leads to more stable and sample-efficient learning in practice. The key results can be summarized as:\nUnbiasedness: $\\mathbb{E}[(G_t - b(S_t))\\,\\nabla_\\theta \\ln\\pi_\\theta(A_t|S_t)] = \\mathbb{E}[G_t\\,\\nabla_\\theta \\ln\\pi_\\theta(A_t|S_t)] = \\nabla_\\theta J(\\theta)$.\nVariance reduction: $\\mathrm{Var}[(G_t - b(S_t))\\,\\nabla_\\theta \\ln\\pi] = \\mathrm{Var}[G_t\\,\\nabla_\\theta \\ln\\pi] - \\frac{(\\mathbb{E}[X^2 Y])^2}{\\mathbb{E}[X^2]}\\,$, which is no greater than the original variance, and is minimized when $b(s)=V^\\pi(s)$ (making $Y - b(s)$ the advantage) ().\nThus, adding a baseline in REINFORCE keeps the gradient estimator unbiased but lowers its variance, fulfilling the proof as required.\nReferences [1] Richard S. Sutton, and Andrew G. Barto. “Reinforcement learning: An introduction”, second edition, 2017.\n[2] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms,” Neural Computation, vol. 6, no. 6, pp. 1185–1201, Nov. 1994, doi: 10.1162/neco.1994.6.6.1185.\n[3] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust Region Policy Optimization,” arXiv:1502.05477 [cs], Feb. 2015, Accessed: Dec. 26, 2017. [Online]. Available: http://arxiv.org/abs/1502.05477\n[4] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv:1707.06347 [cs], Jul. 2017, Accessed: Apr. 10, 2018. [Online]. Available: http://arxiv.org/abs/1707.06347\n[5] Z. Shao et al., “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,” Apr. 27, 2024, arXiv:2402.03300. doi: 10.48550/arXiv.2402.03300.\n","wordCount":"4957","inLanguage":"en","image":"https://livey.github.io/posts/2025-03-05-reinforcement-learning/%3Cimage%20path/url%3E","datePublished":"2025-03-05T00:00:00Z","dateModified":"2025-03-05T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-03-05-reinforcement-learning/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Reinforcement Learning</h1><div class=post-description>This is an introduction to reinforcement learning.</div><div class=post-meta><span title='2025-03-05 00:00:00 +0000 UTC'>March 5, 2025</span>&nbsp;·&nbsp;24 min&nbsp;·&nbsp;4957 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-environment-and-agent aria-label="The Environment and Agent">The Environment and Agent</a></li><li><a href=#markov-decision-process-mdp aria-label="Markov Decision Process (MDP)">Markov Decision Process (MDP)</a></li><li><a href=#bellman-equation aria-label="Bellman Equation">Bellman Equation</a><ul><li><a href=#solving-the-bellman-equation aria-label="Solving the Bellman Equation">Solving the Bellman Equation</a></li></ul></li><li><a href=#optimal-policy aria-label="Optimal Policy">Optimal Policy</a><ul><li><a href=#optimal-policy-bellman-equation aria-label="Optimal policy Bellman equation">Optimal policy Bellman equation</a></li></ul></li><li><a href=#dynamic-programming aria-label="Dynamic Programming">Dynamic Programming</a><ul><li><a href=#policy-evaluation aria-label="Policy evaluation">Policy evaluation</a></li><li><a href=#policy-improvement aria-label="Policy improvement">Policy improvement</a></li><li><a href=#policy-and-value-iteration aria-label="Policy and value iteration">Policy and value iteration</a></li></ul></li><li><a href=#monte-carlo-methods aria-label="Monte Carlo Methods">Monte Carlo Methods</a><ul><li><a href=#off-policy-prediction-via-importance-sampling aria-label="Off-policy prediction via importance sampling">Off-policy prediction via importance sampling</a></li></ul></li><li><a href=#temporal-difference-learning aria-label="Temporal-Difference Learning">Temporal-Difference Learning</a><ul><li><a href=#convergent-conditions aria-label="Convergent conditions">Convergent conditions</a></li><li><a href=#advantages-of-td-methods aria-label="Advantages of TD methods">Advantages of TD methods</a></li><li><a href=#sarsa-on-policy-td-control aria-label="Sarsa: On-policy TD control">Sarsa: On-policy TD control</a></li><li><a href=#q-learning-off-policy-td-control aria-label="Q-learning: Off-policy TD control">Q-learning: Off-policy TD control</a></li></ul></li><li><a href=#bootstrapping aria-label=Bootstrapping>Bootstrapping</a><ul><li><a href=#unified-view-of-the-td-monte-carlo-and-dp-methods aria-label="Unified view of the TD, Monte Carlo, and DP methods">Unified view of the TD, Monte Carlo, and DP methods</a></li></ul></li><li><a href=#policy-gradient-methods aria-label="Policy Gradient Methods">Policy Gradient Methods</a><ul><li><a href=#policy-gradient-theorem aria-label="Policy Gradient Theorem">Policy Gradient Theorem</a></li><li><a href=#reinforce-algorithm aria-label="REINFORCE algorithm">REINFORCE algorithm</a><ul><li><a href=#reinforce-with-baseline aria-label="REINFORCE with baseline">REINFORCE with baseline</a></li></ul></li><li><a href=#actor-critic-methods aria-label="Actor-Critic Methods">Actor-Critic Methods</a></li><li><a href=#trust-region-policy-optimization aria-label="Trust Region Policy Optimization">Trust Region Policy Optimization</a><ul><li><a href=#sample-based-estimation aria-label="Sample-based estimation">Sample-based estimation</a></li><li><a href=#sampling-methods aria-label="Sampling methods">Sampling methods</a></li></ul></li><li><a href=#proximal-policy-optimization aria-label="Proximal Policy Optimization">Proximal Policy Optimization</a></li><li><a href=#group-relative-policy-optimization aria-label="Group relative policy optimization">Group relative policy optimization</a></li></ul></li><li><a href=#appendix aria-label=Appendix>Appendix</a><ul><li><a href=#why-baseline-can-reduce-the-variance aria-label="Why baseline can reduce the variance">Why baseline can reduce the variance</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>I have taken some reinforcement learning related courses during my Ph.D. study. However, I have not touched it for a long time. Recently, I am working on end-to-end autonomous driving and the success of reinforcement learning in large language models brings me back to this topic.
In this post, we will introduce the basic concepts and commonly used algorithms in reinforcement learning. More detailed information can be found in the reference book [1] and <a href=https://spinningup.openai.com/en/latest/index.html>OpenAI RL page</a>.</p><figure style=text-align:center><img src=./resources/environment-agent-interaction.png alt="The interaction between environment and agent" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>The interaction between environment and agent. [1]</figcaption></figure><h1 id=the-environment-and-agent>The Environment and Agent<a hidden class=anchor aria-hidden=true href=#the-environment-and-agent>#</a></h1><p>The environment is the world in which the agent operates. The agent is the entity that makes decisions and takes actions. We denote the core components of reinforcement learning as follows (following the notations in [1] with some abused notations between scalar and vector):</p><ul><li>$S$: environment state</li><li>$A$: agent action</li><li>$R$: environment reward after taking an action under certain state, it should reflect the agent&rsquo;s goal;</li><li>$p(S_{t+1}, R_{t+1}|S_{0:t}, A_{0:t})$: the transition probability that given the history states and actions, the reward $R_{t+1}$ and the next state $S_{t+1}$</li><li>$\pi(A|S)$: the policy that the agent follows given the state $S$, it can be random or deterministic</li><li>$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ is the return, where $\gamma$ is the discount factor. The discount is not necessary, for example, in the episodic case. The return can also be defined by the average of the rewards.</li><li>$v(s) = \mathbb{E}[G_t|S_t = s]$ is the value function of state $s$. It is the expected return from state $s$.</li><li>$q(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]$ is the value function of state-action pair $(s, a)$. It is the expected return from state-action pair $(s, a)$.</li><li>$A(s, a) = Q(s, a) - V(s)$ is the advantage function of state-action pair $(s, a)$. It is the difference between the value function of state-action pair $(s, a)$ and the value function of state $s$.</li><li>Prediction: we care more about the state value function $v(s)$; (In prediction, we need to predict how well the state is.)</li><li>Control: we care more about the state-action value function $q(s, a)$. (In control, we need to choose the best action in a state.)</li></ul><p>When we write the expectation, there are usually only two sources of randomness: the environment transition and the policy. Sometimes, we may omit them for simplicity.</p><h1 id=markov-decision-process-mdp>Markov Decision Process (MDP)<a hidden class=anchor aria-hidden=true href=#markov-decision-process-mdp>#</a></h1><p>In reinforcement learning, we usually assume the environment is a Markov Decision Process, i.e., mathematically,</p>$$
p(S_{t+1}, R_{t+1}| S_{0:t}, A_{0:t}) = p(S_{t+1}, R_{t+1}| S_t, A_t).
$$<h1 id=bellman-equation>Bellman Equation<a hidden class=anchor aria-hidden=true href=#bellman-equation>#</a></h1><p>Recap the definition of the value function,</p>$$
\begin{aligned}
v(s)
&= \mathbb{E}[G_t|S_t = s] \\
&= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} |S_t = s]\\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} |S_t = s] \\
&= \mathbb{E}_{S_{t+1}|S_t}\mathbb{E}_{\cdot|S_{t+1}, S_t}[R_{t+1} + \gamma G_{t+1}] \\
&=\mathbb{E}_{S_{t+1}|S_t}\mathbb{E}_{\cdot|S_{t+1}}[R_{t+1} + \gamma G_{t+1}]\\
&= \sum_{a, s', r}\pi(a|s) p(s', r|s, a) [r + \gamma\mathbb{E}_{\cdot|S_{t+1}=s'} (G_{t+1}|S_{t+1}=s')]\\
& = \sum_{a, s', r}\pi(a|s) p(s', r|s, a) [r + \gamma v(s')]\\
\end{aligned} \tag{1}
$$<p>Similarly, the Bellman equation for the action-value function is</p>$$
\begin{aligned}
q(s, a)
&= \mathbb{E}[G_t|S_t = s, A_t = a]\\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1}|S_t = s, A_t = a]\\
&= \mathbb{E}_{S_{t+1}|S_t, A_t}\mathbb{E}_{\cdot|S_{t+1}, S_t, A_t}[R_{t+1} + \gamma G_{t+1}]\\
&=\mathbb{E}_{S_{t+1}|S_t, A_t}\mathbb{E}_{\cdot|S_{t+1}}[R_{t+1} + \gamma G_{t+1}]\\
&=\sum_{r, s'}p(s', r|s, a)[r + \gamma\mathbb{E}_{\cdot|S_{t+1}=s'}[G_{t+1}|S_{t+1}=s']]\\
& = \sum_{r, s'}p(s', r|s, a)[r + \gamma\mathbb{E}_{A_{t+1}=a'|S_k=s}\mathbb{E}_{\cdot|S_{k=1}, A_{k+1}}G_{k+1}]\\
& = \sum_{r, s'}p(s', r|s, a)[r + \gamma\mathbb{E}_{A_{t+1}=a'|S_k=s}q(s', a')]\\
&= \sum_{r, s'}p(s', r|s, a)[r + \gamma\sum_{a'}\pi(a'|s')q(s', a')]
\end{aligned}\tag{2}
$$<p>From the above derivation, we can also see the relationship between the value function and the action-value function,</p>$$
v(s) = \sum_{a}\pi(a|s)q(s, a)
$$$$
q(s, a) = \sum_{s', r}p(s', r|s, a)[r + \gamma v(s')]
$$<h2 id=solving-the-bellman-equation>Solving the Bellman Equation<a hidden class=anchor aria-hidden=true href=#solving-the-bellman-equation>#</a></h2><p>Let&rsquo;s write the Bellman value function as</p>$$
\begin{aligned}
\mathcal{H} \circ v(s)
&= \sum_{a}\pi(a|s)\sum_{s', r} p(s', r|s, a)[r + \gamma v(s')]\\
&= \sum_{r} p(r|s) r + \gamma \sum_{s'} p(s'|s) v(s') \\
&= \bar{R}_s + \gamma P_s \vec{v}(s)
\end{aligned}
$$<p>where $\bar{R}_s$ is the expected reward from state $s$, $P_s$ is the transition probability from state $s$ to all other states. Writing above in the matrix form, we have</p>$$
\mathcal{H} \circ \vec{v} = \bar{\mathbf{R}} + \gamma \mathbf{P} \vec{v}
$$<p>We can then solve the Bellman equation by solving the linear system,</p>$$
\vec{v} = \bar{\mathbf{R}} + \gamma \mathbf{P} \vec{v}
$$<p>We can also resort to the iterative method by observing that the $\mathcal{H}$ is a contraction mapping, for any two vectors $\mathbf{x}$ and $\mathbf{y}$,</p>$$
\|\mathcal{H} \circ \mathbf{x} - \mathcal{H} \circ \mathbf{y}\| = \|\gamma \mathbf{P} (\mathbf{x} - \mathbf{y})\| \le \gamma \|\mathbf{P} \|\|\mathbf{x} - \mathbf{y}\| \le \gamma \|\mathbf{x} - \mathbf{y}\| < \|\mathbf{x} - \mathbf{y}\|
$$<p>This allows us to use the fixed point method to solve the Bellman equation.</p><p>Similarly, we can also prove that the operation in the Bellman equation for the action-value function is a contraction mapping.</p><h1 id=optimal-policy>Optimal Policy<a hidden class=anchor aria-hidden=true href=#optimal-policy>#</a></h1><p>The optimal policy, $\pi_*$, satisfies</p>$$\forall s, \quad v_{\pi_*}(s) \ge v_\pi(s)$$<p>That is to say, the optimal policy will yield a greater or equal value function than any other policy for any state. The optimal policy may not be unique, but all optimal policies share the same optimal value function, $v_*$.</p><h2 id=optimal-policy-bellman-equation>Optimal policy Bellman equation<a hidden class=anchor aria-hidden=true href=#optimal-policy-bellman-equation>#</a></h2>$$
\begin{aligned}
v_*(s)
&= \max_{a} q_*(s, a) \\
&= \max_{a} \mathbb{E}_{\pi_*}[R_{t+1} + \gamma G_{t+1}|S_t = s, A_t = a] \\
&= \max_{a} \mathbb{E}_{\pi_*}[R_{t+1} + \gamma v_*(S_{t+1})|S_t = s, A_t = a] \\
&= \max_{a} \sum_{s', r} p(s', r|s, a)[r + \gamma v_*(s')]
\end{aligned}
$$<p>and the state-action value function of the optimal policy satisfies,</p>$$
\begin{aligned}
q_*(s, a)
&= \mathbb{E}_{\pi_*}[R_{t+1} + \gamma G_{t+1}|S_t = s, A_t = a] \\
&= \mathbb{E}[R_{t+1} + \gamma v_*(s')|S_t = s, A_t = a] \\
&= \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(s', a')|S_t = s, A_t = a] \\
&= \sum_{s', r} p(s', r|s, a)[r + \gamma \max_{a'} q_*(s', a')]
\end{aligned}
$$<p>Similar to the ordinary Bellman equation, we can also prove it is a contraction mapping, and then use the fixed point method to solve it.</p><p>Suppose there are two optimal value function $v_1$ and $v_2$, the Bellman operator is $\mathcal{H}$. Then,</p>$$
\begin{aligned}
\|\mathcal{H}v_1 - \mathcal{H}v_2 \|_\infty
&= \max_{s} | \mathcal{H}v_1(s) - \mathcal{H}v_2(s) | \\
&= \max_{s} | \max_{a} \sum_{s', r} p(s', r|s, a)[r + \gamma v_1(s')] - \max_{a} \sum_{s', r} p(s', r|s, a)[r + \gamma v_2(s')] | \\
&\underset{\le}{(a)} \max_{s, a} | \sum_{s', r} p(s', r|s, a)[r + \gamma v_1(s')] - \sum_{s', r} p(s', r|s, a)[r + \gamma v_2(s')] | \\
&= \max_{s, a} \gamma |\sum_{s'} p(s'|s, a)[v_1(s') - v_2(s')] | \\
&\le \gamma \max_{s, a} \sum_{s'} p(s'|s, a) \cdot \max_{s'} \{|v_1(s') - v_2(s')|\} \\
&= \gamma \|\vec{v_1} - \vec{v_2}\|_\infty
\end{aligned}
$$<p>where (a) holds because of triangle inequality, $|\|x\| - \|y\| | \le \|x - y\| \le \|x\| + \|y\|$.
Similarly, for the optimal action-value Bellman equation, we have</p>$$
\begin{aligned}
\|\mathcal{H}q_1 - \mathcal{H}q_2 \|_\infty
&= \max_{s, a} | \mathcal{H}q_1(s, a) - \mathcal{H}q_2(s, a) | \\
&= \max_{s, a} | \sum_{s', r} p(s', r|s, a')[r + \gamma \max_{a'} q_1(s', a')] - \sum_{s', r} p(s', r|s, a')[r + \gamma \max_{a'} q_2(s', a')] | \\
&\le \gamma\max_{s, a} \sum_{s'} p(s'| s, a)|\max_{a'}q_1(s', a') - \max_{a'}q_2(s', a')| \| \\
&\le \gamma \max_{s, a} \sum_{s'} p(s'| s, a) \cdot \max_{a'} \{|q_1(s', a') - q_2(s', a')|\} \\
&\le \gamma \max_{s', a'} \{|q_1(s', a') - q_2(s', a')|\} \\
&= \gamma \|\vec{q_1} - \vec{q_2}\|_\infty
\end{aligned}
$$<p>Thus we prove that Bellman operator is a contraction mapping for both state and state-action value functions.</p><h1 id=dynamic-programming>Dynamic Programming<a hidden class=anchor aria-hidden=true href=#dynamic-programming>#</a></h1><h2 id=policy-evaluation>Policy evaluation<a hidden class=anchor aria-hidden=true href=#policy-evaluation>#</a></h2><p>Using the fixed point method, we can iteratively</p>$$
v_{k+1}(s) = \sum_{a}\pi(a|s)\sum_{s', r} p(s', r|s, a)[r + \gamma v_k(s')]
$$<p>By running this several times, we get the state/state-action value function under policy $\pi$. However, we should note that this method requires full knowledge of the environment, i.e. the transition probability $p(s', r|s, a)$.</p><h2 id=policy-improvement>Policy improvement<a hidden class=anchor aria-hidden=true href=#policy-improvement>#</a></h2><p>Suppose we have two deterministic policies, $\pi$ and $\pi'$, such that</p>$$
q(s, \pi'(s)) \ge v(s) \quad \forall s \in \mathcal{S}
$$<p>Then, the policy $\pi'$ is as good as or better than $\pi$, i.e.</p>$$
v_{\pi'}(s) \ge v_\pi(s) \quad \forall s \in \mathcal{S}
$$<p>This can be proved below</p>$$
\begin{aligned}
v_{\pi}(s) &\leq q_{\pi}(s, \pi'(s)) \\
    &= \mathbb{E} [ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = \pi'(s)] \\
&= \mathbb{E}_{\pi'} [ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s] \\
&\leq \mathbb{E}_{\pi'} [ R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1})) \mid S_t = s] \\
&= \mathbb{E}_{\pi'} [ R_{t+1} + \gamma \mathbb{E} [ R_{t+2} + \gamma v_{\pi}(S_{t+2}) \mid S_{t+1}, A_{t+1} = \pi'(S_{t+1})] \mid S_t = s] \\
&= \mathbb{E}_{\pi'} [ R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) \mid S_t = s] \\
&\leq \mathbb{E}_{\pi'} [ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) \mid S_t = s] \\
&\vdots \\
    &\leq \mathbb{E}_{\pi'} [ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots \mid S_t = s] \\
&= v_{\pi'}(s).
\end{aligned}
$$<p>The third equation holds because $\pi'$ is a deterministic policy.</p><p>Policy improvement takes a greedy step on the original policy with respect to the value function of the original policy,</p>$$
\begin{aligned}
\pi'(s)
&= \underset{a}{\text{argmax}} q_\pi(s, a) \\
&= \underset{a}{\text{argmax}} \sum_{s', r} p(s', r| s, a) [r + \gamma v_\pi(s')]
\end{aligned}
$$<p>If the new policy $\pi'$ is as good as but not better than the old policy $\pi$, according to the optimal Bellman equation, it is optimal.</p><h2 id=policy-and-value-iteration>Policy and value iteration<a hidden class=anchor aria-hidden=true href=#policy-and-value-iteration>#</a></h2>$$
\pi_0 \xrightarrow{\mathbf{E}} v_{\pi_0} \xrightarrow{\mathbf{I}} \pi_1 \xrightarrow{\mathbf{E}} v_{\pi_1} \xrightarrow{\mathbf{I}} \pi_2 \xrightarrow{\mathbf{E}} \cdots \xrightarrow{\mathbf{I}} \pi_* \xrightarrow{\mathbf{E}} v_*
$$<h1 id=monte-carlo-methods>Monte Carlo Methods<a hidden class=anchor aria-hidden=true href=#monte-carlo-methods>#</a></h1><p>According to the definition of the value function, if we sample the whole trajectory, we can estimate the value function by the average of the returns.</p><figure style=text-align:center><img src=./resources/first-visit-monte-carlo.png alt="the first visit Monte Carlo method" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>The first visit Monte Carlo method. [1]</figcaption></figure><p>We can also evaluate the state-action value function and improve the policy as:</p><figure style=text-align:center><img src=./resources/mc-es.png alt="Monte Carlo Exploring Starts" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Monte Carlo Exploring Starts. [1]</figcaption></figure><h2 id=off-policy-prediction-via-importance-sampling>Off-policy prediction via importance sampling<a hidden class=anchor aria-hidden=true href=#off-policy-prediction-via-importance-sampling>#</a></h2><p>Suppose there are two policies, $b$ is for behavior policy, we simulate the process according to $b$ and generate the episodes. $\pi$ is the target policy, we want to learn the state or state-action value function under $\pi$. To use the importance sampling method, we need follow the coverage assumption, i.e. every action taken under $\pi$ is also taken, at least occasionally, under $b$, which requires $\pi(a|s) > 0$ implies $b(a|s) > 0$.</p><p>Importance sampling is a general technique used to estimate expected values under one distribution while sampling from another. In reinforcement learning, we use it to estimate values under the target policy $\pi$ while sampling trajectories from the behavior policy $b$.</p><p>Let&rsquo;s consider a trajectory $\tau = (S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T)$ of length $T$. The probability of this trajectory under policy $\pi$ is:</p>$$
p_{\pi}(\tau) = p(S_0) \prod_{t=0}^{T-1} \pi(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)
$$<p>Similarly, the probability under the behavior policy $b$ is:</p>$$
p_{b}(\tau) = p(S_0) \prod_{t=0}^{T-1} b(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)
$$<p>The importance sampling ratio is:</p>$$
\rho_{0:T-1} = \frac{p_{\pi}(\tau)}{p_{b}(\tau)} = \frac{p(S_0) \prod_{t=0}^{T-1} \pi(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)}{p(S_0) \prod_{t=0}^{T-1} b(A_t|S_t)p(S_{t+1}, R_{t+1}|S_t, A_t)} = \prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
$$<p>The environment dynamics terms cancel out, leaving only the ratio of policy probabilities. We can use this importance sampling ratio to correct the returns when estimating value functions:</p>$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s] = \mathbb{E}_{b}[\rho_{t:T-1} G_t|S_t=s]
$$<h1 id=temporal-difference-learning>Temporal-Difference Learning<a hidden class=anchor aria-hidden=true href=#temporal-difference-learning>#</a></h1>$$
V(S_t) \gets V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]
$$<p>which uses the currently sampled reward $R_{t+1}$ and current next state value $V(S_{t+1})$ to update the current state value $V(S_t)$. This represents a moving average of the current state value $V(S_t)$ and the estimated state value, $R_{t+1} + \gamma V(S_{t+1})$.</p><p>This quantity,</p>$$
\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t),
$$<p>called the TD error arises in various forms throughout reinforcement learning.</p><figure style=text-align:center><img src=./resources/td0.png alt="tabular TD(0) for estimation state value function" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Tabular TD(0) for estimation state value function. (Chapter 6 of [1])</figcaption></figure><h2 id=convergent-conditions>Convergent conditions<a hidden class=anchor aria-hidden=true href=#convergent-conditions>#</a></h2><ol><li><p>Ergodicity of the environment&rsquo;s Markov chain
For TD(0) to work, the environment&rsquo;s Markov chain, defined by the policy, must be ergodic. This means:</p><ul><li>The environment is stationary;</li><li>You can reach any state from any other state eventually; (imagine what happens if there is a dead cycle)</li><li>All states are visited infinitely often, which is crucial for the algorithm to learn about every state.</li></ul></li><li><p>step size and Robbins-Monro Framework
The step size $\alpha_t$ plays a pivotal role in ensuring convergence, and research aligns with the stochastic approximation theory, particularly the Robbins-Monro conditions. These conditions are:</p><ul><li><p>$\sum_{t=1}^{\infty} \alpha_t = \infty$, ensuring that the updates are significant enough initially to overcome random fluctuations and explore the value space;</p></li><li><p>$\sum_{t=1}^{\infty} \alpha_t^2 < \infty$, ensuring that the updates eventually become small enough to allow convergence without excessive oscillation.</p></li></ul><p>A typical choice is $\alpha_t = \frac{1}{t}$. Though we set the step size to satisfy the Robbins-Monro conditions, other choices are beneficial for non-stationary environments.</p></li></ol><p>Please find the full proof in [2].</p><h2 id=advantages-of-td-methods>Advantages of TD methods<a hidden class=anchor aria-hidden=true href=#advantages-of-td-methods>#</a></h2><p>&ldquo;TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions. The next most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an online, fully incremental fashion.&rdquo; [1]</p><h2 id=sarsa-on-policy-td-control>Sarsa: On-policy TD control<a hidden class=anchor aria-hidden=true href=#sarsa-on-policy-td-control>#</a></h2>$$
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right].
$$<figure style=text-align:center><img src=./resources/sarsa.png alt="Sarsa algorithm" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Sarsa algorithm. (Chapter 6 of [1])</figcaption></figure><h2 id=q-learning-off-policy-td-control>Q-learning: Off-policy TD control<a hidden class=anchor aria-hidden=true href=#q-learning-off-policy-td-control>#</a></h2>$$
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \right].
$$<figure style=text-align:center><img src=./resources/q-learning.png alt="Q-learning algorithm" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Q-learning algorithm. (Chapter 6 of [1])</figcaption></figure><p>The expected Sarsa algorithm is a variant of the Q-learning algorithm, which uses the expected value of the next state-action value function instead of the maximum value.</p>$$
Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_{a}\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t, A_t) \right].
$$<h1 id=bootstrapping>Bootstrapping<a hidden class=anchor aria-hidden=true href=#bootstrapping>#</a></h1><h2 id=unified-view-of-the-td-monte-carlo-and-dp-methods>Unified view of the TD, Monte Carlo, and DP methods<a hidden class=anchor aria-hidden=true href=#unified-view-of-the-td-monte-carlo-and-dp-methods>#</a></h2><p>Let&rsquo;s define the n-step return as</p>$$
G_{t:t+n} \triangleq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}),
$$<p>Then, you can pick any $n$ step return as the estimation of the value function. The following shows the backup diagram of the TD, Monte Carlo, and DP methods.</p><figure style=text-align:center><img src=./resources/td-backup.png alt="TD methods" style="width:31%;margin:0 auto;display:inline-block">
<img src=./resources/mc-backup.png alt="Monte Carlo methods" style="width:31%;margin:0 auto;display:inline-block">
<img src=./resources/dp-backup.png alt="DP methods" style="width:30.5%;margin:0 auto;display:inline-block"><figcaption style=font-weight:400>Backup diagrams of TD, Monte Carlo, and DP methods. [fromDavid Silver's RL course lecture 4, "Model-Free Prediction]</figcaption></figure><h1 id=policy-gradient-methods>Policy Gradient Methods<a hidden class=anchor aria-hidden=true href=#policy-gradient-methods>#</a></h1><p>In this section, we will introduce the policy gradient methods. These methods directly learn a parameterized policy and select actions directly from the policy. We may still need to learn the value function. Let&rsquo;s denote $\pi(a|s;\theta)= P(A_t=a | S_t =s; \boldsymbol{\theta})$ as the policy, where $\boldsymbol{\theta}$ is the parameter. Let $v(s; \mathbf{w})$ be the value function, where $\mathbf{w}$ is the parameter.</p><h2 id=policy-gradient-theorem>Policy Gradient Theorem<a hidden class=anchor aria-hidden=true href=#policy-gradient-theorem>#</a></h2><p>In this section, we consider the episodic case, where the process starts with some initial state $s_0$ (not random). Our goal is to maximize the expected return starting from $s_0$, i.e. $J(\boldsymbol{\theta}) = \mathbb{E}_{\pi}[G_0|S_0=s_0]$.</p><p>For a state value function $v(s)$, it is a function of the policy. So, we can compute the gradient of the policy with respect to policy parameter, $\boldsymbol{\theta}$. The gradient can be computed by</p>$$
\begin{aligned}
\nabla v(s)
&= \nabla \sum_{a} \pi(a|s) q(s, a) \\
&= \sum_{a} \nabla \pi(a|s)q(s, a) + \sum_a \pi(a|s) \nabla q(s, a) \\
&=\sum_a \nabla \pi(a|s)q(s, a) + \sum_a \pi(a|s) \nabla \sum_{s', r} p(s', r|s, a)[r + v(s')] \\
&= \sum_a \nabla \pi(a|s)q(s, a) + \sum_a \pi(a|s) \sum_{s'} p(s'|s, a) \nabla v(s') \\
&\underset{=}{(a)}\sum_a \nabla \pi(a|s)q(s, a) + \sum_{s'} p(s'|s) \nabla v(s')\\
&= \sum_a \nabla \pi(a|s)q(s, a) + \sum_{s'} p(s'|s)\big[ \\
&\quad\sum_{a'} \nabla \pi(a'|s')q(s', a') + \sum_{s''}p(s''|s')\nabla v(s'')\big] \text{\qquad (by rollout equation (a))}\\
&= \sum_a \nabla \pi(a|s)q(s, a) + \sum_{s'}\sum_a p(s'|s)\nabla \pi(a|s')q(s', a) +\sum_{s''}p(s''|s)\nabla v(s'') \\
&= \sum_{x \in \mathcal{S}} \sum_{K=0}^\infty P(s\to x, K, \pi) \sum_a \nabla \pi(a|x)q(x, a)
\end{aligned}
$$<p>where $P(s\to x, K, \pi)$ is the probability of starting from state $s$ and reaching state $x$ in $K$ steps under policy $\pi$.</p>$$
\begin{aligned}
\nabla J(\boldsymbol{\theta})
&= \sum_{s\in \mathcal{S}} \sum_{K=0}^\infty P(s_0\to x, K, \pi) \sum_a \nabla \pi(a|x)q(x, a) \\
&= \sum_{s \in \mathcal{S}} \left(\sum_{K=0}^\infty P(s_0\to s, K, \pi)\right) \sum_a \nabla \pi(a|s)q(s, a) \\
&=\sum_{s \in \mathcal{S}} \eta(s) \sum_a\nabla \pi(a|s)q(s, a) \\
&=\sum_{s'}\eta(s') \sum_s \frac{\eta(s)}{\sum_{s'}\eta(s')} \sum_a\nabla \pi(a|s)q(s, a) \\
&= \sum_{s'}\eta(s') \sum_s \mu(s) \sum_a\nabla \pi(a|s)q(s, a) \\
&\propto \sum_s \mu(s) \sum_a\nabla \pi(a|s)q(s, a)
\end{aligned}
$$<p>where $\eta(s)$ only depends on the policy and the environment, which can be seen the probability of visiting state $s$. The above equation is derived by letting the discount factor $\gamma = 1$. If it is the continuous case, the proportional is replaced by equality, for episodic case, the constant term is the average length of visit state $s$. Note that, if we consider the discounted reward, the general conclusion does not change. In this case, $P(s\to x, K, \pi)$ will means the discounted probability of visiting state $x$ in $K$ steps under policy $\pi$.</p><p>Then, if we follow the policy $\pi$, the gradient can be computed by</p>$$
\nabla J(\boldsymbol{\theta}) = \mathbb{E}_{\pi}[\sum_a q(S, a; \mathbf{w}) \nabla \pi(a|S;\boldsymbol{\theta})]
$$<p>Then, we can instantiate the stochastic gradient ascent algorithm to update the policy parameter:</p>$$
\boldsymbol{\theta}_{t+1} \gets \boldsymbol{\theta}_t + \alpha \sum_a q(S_t, a; \mathbf{w}_t) \nabla \pi(a|S_t;\boldsymbol{\theta}_t)
$$<h2 id=reinforce-algorithm>REINFORCE algorithm<a hidden class=anchor aria-hidden=true href=#reinforce-algorithm>#</a></h2>$$
\begin{aligned}
\nabla J(\boldsymbol{\theta})
&= \mathbb{E}_{\pi}[\sum_a q(S, a; \mathbf{w}) \nabla \pi(a|S;\boldsymbol{\theta})]\\
&= \mathbb{E}_{\pi}[\sum_a \pi(a|S; \boldsymbol{\theta}) q(S, a; \mathbf{w})\frac{\nabla \pi(a|S;\boldsymbol{\theta})}{\pi(a|S; \boldsymbol{\theta})}]\\
&= \mathbb{E}_{\pi}[q(S, A; \mathbf{w})\frac{\nabla \pi(a|S;\boldsymbol{\theta})}{\pi(a|S; \boldsymbol{\theta})}] \\
&= \mathbb{E}_{\pi}[q(S, A; \mathbf{w})\nabla \ln \pi(A|S; \boldsymbol{\theta})] \\
&= \mathbb{E}_{\pi}[G_t \nabla \ln \pi(A_t|S_t; \boldsymbol{\theta})] \quad (\text{because } \mathbb{E} [q(S, A; \mathbf{w})|S, A] = G_t)
\end{aligned}
$$$$
\boldsymbol{\theta}_{t+1} \gets \boldsymbol{\theta}_t + \alpha G_t \nabla \ln \pi(A_t|S_t; \boldsymbol{\theta}_t)
$$<h3 id=reinforce-with-baseline>REINFORCE with baseline<a hidden class=anchor aria-hidden=true href=#reinforce-with-baseline>#</a></h3><p>We can also add a baseline to the REINFORCE algorithm to reduce the variance. The baseline can be any random variable as long as it is independent with the action.</p>$$
\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu(s) \sum_a \nabla \pi(a|s) (q(s, a; \mathbf{w}) - b(s))
$$<p>This is verified by</p>$$
\begin{aligned}
\nabla J(\boldsymbol{\theta})
&\propto \sum_s \mu(s) \sum_a \nabla \pi(a|s) (q(s, a; \mathbf{w}) - b(s)) \\
&\propto \sum_s \mu(s) [\sum_a \nabla \pi(a|s)q(s, a; \mathbf{w}) - \sum_a \nabla \pi(a|s)b(s)] \\
&= \sum_s \mu(s) [\sum_a \nabla \pi(a|s)q(s, a; \mathbf{w}) - b(s)\nabla\sum_a \pi(a|s)] \\
&= \sum_s \mu(s) \sum_a q(s, a)\nabla \pi(a|s)
\end{aligned}
$$<p>A natural choice of the baseline is the state value function. So, the REINFORCE with baseline algorithm is:</p><figure style=text-align:center><img src=./resources/reinforce.png alt="REINFORCE with baseline algorithm" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>REINFORCE with baseline. (Chapter 13 of [1])</figcaption></figure><h2 id=actor-critic-methods>Actor-Critic Methods<a hidden class=anchor aria-hidden=true href=#actor-critic-methods>#</a></h2><p>Actor-critic method use one-step TD to estimate the full return in the REINFORCE algorithm. In actor-critic method, we need estimate two functions: the advantage function and the policy.</p><figure style=text-align:center><img src=./resources/actor-critic.png alt="Actor-Critic algorithm" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Actor-Critic algorithm. (Chapter 13 of [1])</figcaption></figure><h2 id=trust-region-policy-optimization>Trust Region Policy Optimization<a hidden class=anchor aria-hidden=true href=#trust-region-policy-optimization>#</a></h2>$$
\begin{aligned}
L_{\pi}(\tilde{\pi})
&= v(\pi) + \sum_{t=0}^\infty \sum_s P(s_t =s | \pi) \sum_a \tilde{\pi}(a|s) A_\pi(s, a) \\
&= v_\pi + \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a \mid s) A_{\pi}(s, a).
\end{aligned}
$$<p>where $\rho_{\pi}(s) = \sum_{t=0}^{\infty} \gamma^t P(s_t = s \mid \pi)$ is the sum of the discounted visitation probability of state $s$ under policy $\pi$.</p><p>Let $D_{KL}^{\max}(\pi, \tilde{\pi}) = \max_s D_{KL}(\pi(\cdot|s), \tilde{\pi}(\cdot|s))$ be the maximum KL divergence between the two policies.</p><p><strong>Theorem</strong>: Let $\pi$ and $\tilde{\pi}$ be two policies. Then the following bound holds:</p>$$
v_{\tilde{\pi}} \geq L_{\pi}(\tilde{\pi}) - \frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2
$$<p>where $\alpha = D_{KL}^{\max}(\pi, \tilde{\pi})$ and $\epsilon = \max_{s,a}|A_{\pi}(s,a)|$.</p><p>Let the right hand side of the above inequality be $M_{\pi}(\tilde{\pi})$. We have $v_{\tilde{\pi}} \geq M_{\pi}(\tilde{\pi})$ and $v_{\pi} = M_{\pi}(\pi)$. So, we have</p>$$
v_{\tilde{\pi}} - v_\pi = M_{\pi}(\tilde{\pi}) - M_{\pi}(\pi).
$$<p>If we make an ascent step of $M_\pi(\pi)$, i.e., $M_\pi(\tilde{\pi}) > M_\pi(\pi)$, we will get a new policy $\tilde{\pi}$ that has a higher value function. Then it guarantees that we get a monotonic improved policies.</p><p>If we follow the theoretic objective $M_\pi(\tilde{\pi})$, the constant term $\frac{4\epsilon \gamma}{(1-\gamma)^2}$, will be very large. Therefore, we place a significant constraint on the KL divergence. Thus, the new policy will likely follow the old policy. So, the author proposed solving</p>$$
\begin{aligned}
&\underset{\theta}{\text{maximize}} \quad L_{\theta_{\text{old}}}(\theta) \\
&\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{\text{old}}}}(\theta_{\text{old}}, \theta) \leq \delta.
\end{aligned}
$$<p>where</p>$$
\begin{aligned}
\overline{D}_{\text{KL}}^{\rho}(\theta_1, \theta_2) &= \mathbb{E}_{s \sim \rho} \left[ D_{\text{KL}} \big( \pi_{\theta_1}(\cdot \mid s) \,\big\|\, \pi_{\theta_2}(\cdot \mid s) \big) \right].
\end{aligned}
$$<h3 id=sample-based-estimation>Sample-based estimation<a hidden class=anchor aria-hidden=true href=#sample-based-estimation>#</a></h3><p>Let us write the full optimization problem as</p>$$
\begin{aligned}
&\max_\theta \sum_s \rho_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s, a) \\
&\text{subject to} \quad \overline{D}_{\text{KL}}^{\rho_{\theta_{old}}}(\theta_{old}, \theta) \leq \delta.
\end{aligned}
$$<p>If we use importance sampling to estimate the objective, we have</p>$$
\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s, a) = \mathbb{E}_{a\sim q}[\frac{\pi_\theta(a|s)}{q(a|s)} A_{\theta_{old}}(s, a)].
$$<p>Then our optimization problem becomes</p>$$
\begin{aligned}
&\max_\theta \mathbb{E}_{s\sim\rho_{\theta_{old}}, a\sim q}[\frac{\pi_\theta(a|s)}{q(a|s)} A_{\theta_{old}}(s, a)] \\
&\text{subject to} \quad \mathbb{E}_{s \sim \rho_{\theta_{old}}} D_{\text{KL}}^{\rho_{\theta_{old}}}(\theta_{old}, \theta) \leq \delta.
\end{aligned}
$$<h3 id=sampling-methods>Sampling methods<a hidden class=anchor aria-hidden=true href=#sampling-methods>#</a></h3><p>The author proposed the uniform sampling method for discrete action space and $\pi_{\theta_{old}}(a|s)$ for continuous action space. It also suggested using the weighted importance sampling for large and continuous action spaces.</p><h2 id=proximal-policy-optimization>Proximal Policy Optimization<a hidden class=anchor aria-hidden=true href=#proximal-policy-optimization>#</a></h2><p>This section is based on [4].</p><p>The objective of the PPO is</p>$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_{t} \left[ \min \left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} A_t, \text{clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon) A_t \right) \right].
$$<p>where $\epsilon$ is a hyperparameter, say $\epsilon = 0.2$, $\hat{\mathbb{E}}_{t}$ means we are doing mean over the older policy trajectory. The following figure shows the value of the objective value with respect to the ratio between the new policy and the older one. This clipping mechanism prevents excessive large policy updates.</p><figure style=text-align:center><img src=./resources/ppo-clip.png alt="Function value with respect to the ratio" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Function value with respect to the ratio[4]</figcaption></figure><h2 id=group-relative-policy-optimization>Group relative policy optimization<a hidden class=anchor aria-hidden=true href=#group-relative-policy-optimization>#</a></h2><p>The method was proposed in [5].</p><p>The objective of the Group relative policy optimization (GRPO) is</p>$$
\begin{aligned}
&\mathcal{J}_{GRPO}(\theta) 
= \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}} (O | q) } \Bigg[ \\
&\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \Bigg\{
\min \Bigg[
\frac{\pi_{\theta} (o_{i,t} | q, o_{i, < t})}{\pi_{\theta_{\text{old}}} (o_{i,t} | q, o_{i, < t})} \hat{A}_{i,t},
\operatorname{clip} \Bigg(
\frac{\pi_{\theta} (o_{i,t} | q, o_{i, < t})}{\pi_{\theta_{\text{old}}} (o_{i,t} | q, o_{i, < t})}, 1 - \epsilon, 1 + \epsilon
\Bigg) \hat{A}_{i,t}
\Bigg] \\
&- \beta \mathbb{D}_{KL} \big[\pi_{\theta} \| \pi_{\text{ref}} \big] \Bigg\}
\Bigg],
\end{aligned}
$$<p>where the advantage $\hat{A}_{i,t}$ is calculated by</p>$$
\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)},
$$<p>where $r_i$ is the (i)th return in the group. $\pi_{ref}$ is the policy continuing learned from the replay buffer.</p><p>The KL divergence is estimated by</p>$$
\mathbb{D}_{KL} \big[\pi_{\theta} \| \pi_{\text{ref}} \big]=\frac{\pi_{\text{ref}}}{\pi_\theta} - \ln \frac{\pi_{\text{ref}}}{\pi_\theta} - 1.
$$<p>This estimation is unbiased due to</p>$$
\begin{aligned}
\mathbb{E}_{a \sim \pi_\theta}[\frac{\pi_{ref}(a)}{\pi_\theta(a)} - \ln \frac{\pi_{ref}(a)}{\pi_\theta(a)} - 1]
&= \mathbb{E}_{a} \pi_{ref}(a) \left( \frac{\pi_{ref}(a)}{\pi_\theta(a)} - \ln \frac{\pi_{ref}(a)}{\pi_\theta(a)} - 1 \right) \\
&= -\mathbb{E}_{a \sim \pi_\theta} \frac{\pi_{ref}(a)}{\pi_\theta(a)} \\
&= \mathbb{D}_{KL} \big[\pi_{\theta} \| \pi_{\text{ref}} \big].
\end{aligned}
$$<p>It is also positive by observing the function $f(x) = x - \ln x -1$.</p><p>By computing the relative advantage in the group, we can alleviate the need for using the state value function. Since a state value function for large language model might be as large as the model itself, this method dramatically reduces the computational cost.</p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><h2 id=why-baseline-can-reduce-the-variance>Why baseline can reduce the variance<a hidden class=anchor aria-hidden=true href=#why-baseline-can-reduce-the-variance>#</a></h2><p>This proof was generated by ChatGPT and subsequently reviewed and revised by myself.</p><p><strong>1. Policy Gradient (REINFORCE):</strong> Let $\pi_\theta(a|s)$ be a parameterized policy giving the probability (or density) of action $a$ in state $s$. We seek to maximize the expected return $J(\theta)$ (the cumulative reward). The <strong>policy gradient theorem</strong> (REINFORCE) gives the gradient of the objective as an expectation over the policy&rsquo;s randomness. Formally, one version of the REINFORCE gradient is:</p>$$
\nabla_{\theta} J(\theta) \;=\; \mathbb{E}_{\pi_\theta}\!\Big[\, G_t \,\nabla_{\theta} \ln \pi_\theta(A_t \mid S_t)\Big]\,,
$$<p>where $G_t$ is the total return (cumulative reward) following time $t$, and the expectation $\mathbb{E}_{\pi_\theta}[\,\cdot\,]$ is taken over the trajectory distribution induced by $\pi_\theta$. In words, an <em>unbiased stochastic estimator</em> for the policy gradient is</p>$$
\hat{g} \;=\; G_t \,\nabla_{\theta} \ln \pi_\theta(A_t \mid S_t)\,,
$$<p>since $\mathbb{E}[\hat{g}] = \nabla_\theta J(\theta)$ by the policy gradient theorem. This is the basis of the REINFORCE algorithm: adjust $\theta$ in the direction of $G_t \nabla_\theta \ln\pi_\theta(A_t|S_t)$ on each sampled trajectory.</p><p><strong>2. Adding a Baseline to the Gradient Estimator:</strong> While $\hat{g}$ is an unbiased gradient estimator, it can have high variance, slowing convergence. A common technique to reduce variance is to <strong>subtract a baseline</strong> $b(s)$ from the return. The modified update uses</p>$$
\hat{g}_b \;=\; \big(G_t - b(S_t)\big)\,\nabla_{\theta} \ln \pi_\theta(A_t \mid S_t)\,,
$$<p>where $b(s)$ is any function of state (it can even be a constant). Importantly, $b(s)$ <strong>must not depend on the action $A_t$</strong> (or any random outcome correlated with the action) – it should be independent of the sampling of $A_t$. This ensures the estimator remains unbiased. Intuitively, $b(s)$ is a baseline level of expected return against which the actual return $G_t$ is compared; $G_t - b(S_t)$ is often called the <strong>advantage</strong>. Choosing a good baseline (e.g. an estimate of the state’s value $V^\pi(s)$) can greatly reduce variance without changing the expected value (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>).</p><p><strong>3. Unbiasedness with a Baseline:</strong> We now show that $\hat{g}_b$ is an <strong>unbiased</strong> estimator of the true gradient $\nabla_\theta J(\theta)$. Taking expectation (over all randomness in $S_t, A_t,$ and rewards):</p>$$
\begin{aligned}
\mathbb{E}\!\big[(G_t - b(S_t))\,\nabla_\theta \ln \pi_\theta(A_t|S_t)\big]
&= \mathbb{E}_{S_t}\!\Big[\,\mathbb{E}_{A_t\sim\pi}[\, (G_t - b(S_t))\,\nabla_\theta \ln \pi_\theta(A_t|S_t) \mid S_t \,] \Big] \\
&= \mathbb{E}_{S_t}\!\Big[\, (G_t - b(S_t)) \,\mathbb{E}_{A_t\sim\pi}[\,\nabla_\theta \ln \pi_\theta(A_t|S_t) \mid S_t\,] \Big]\,,
\end{aligned}
$$<p>because $G_t$ and $b(S_t)$ are constant with respect to the inner expectation (conditional on $S_t$). Now, note the key identity for any fixed state $s$:</p>$$
\mathbb{E}_{A\sim\pi_\theta(\cdot|s)}\!\big[\nabla_\theta \ln \pi_\theta(A|s)\big] \;=\; \nabla_\theta \int_a \pi_\theta(a|s)\, da \;=\; \nabla_\theta\,1 \;=\; \mathbf{0}\,.
$$<p>In other words, $\mathbb{E}[\nabla_\theta \ln \pi_\theta(A_t|S_t) \mid S_t=s] = 0$ for all $s$. This follows from the fact that $\int \pi_\theta(a|s)da=1$ and differentiating yields zero. Using this result above:</p>$$
\mathbb{E}\!\big[(G_t - b(S_t))\,\nabla_\theta \ln \pi_\theta(A_t|S_t)\big]
= \mathbb{E}_{S_t}\!\Big[\, (G_t - b(S_t)) \cdot \mathbf{0} \Big]
= 0 + \mathbb{E}[G_t\,\nabla_\theta \ln \pi_\theta(A_t|S_t)]\,,
$$<p>since the $b(S_t)$ term drops out. But $\mathbb{E}[G_t\,\nabla_\theta \ln \pi_\theta(A_t|S_t)] = \nabla_\theta J(\theta)$ by the REINFORCE formula. Therefore,</p>$$
\mathbb{E}[\hat{g}_b] \;=\; \mathbb{E}[(G_t - b(S_t))\,\nabla_\theta \ln \pi_\theta(A_t|S_t)] \;=\; \nabla_\theta J(\theta)\,.
$$<p><strong>Conclusion:</strong> Adding the baseline <em>does not change</em> the expected gradient. The baseline term is an <strong>expectation-zero control variate</strong> – we’ve added a quantity whose mean is zero, so it doesn’t bias the estimator (<a href="https://datascience.stackexchange.com/questions/15878/advantage-function-variance-reduction#:~:text=As%20you%20mention%20it%20can,the%20variance%20of%20your%20estimate">machine learning - Advantage Function - Variance Reduction - Data Science Stack Exchange</a>). Thus, $\hat{g}_b$ is an unbiased gradient estimator, just like the original $\hat{g}$ (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>).</p><p><strong>4. Variance Reduction via the Baseline:</strong> Although $\hat{g}_b$ has the same mean as $\hat{g}$, its variance can be much <strong>lower</strong>. We will compare $\mathrm{Var}(\hat{g}_b)$ to $\mathrm{Var}(\hat{g})$ and show the former is smaller (for an appropriate choice of $b$). For simplicity, consider one component of the gradient (one parameter dimension) and write $X = \nabla_\theta \ln \pi_\theta(A_t|S_t)$ and $Y = G_t$ for shorthand. Then our original estimator is $U = X Y$ and the baseline estimator is $U_b = X (Y - b(S_t))$. We analyze the variance of these scalar random variables (the argument can be applied to each component of the gradient vector separately (<a href="https://davidrosenberg.github.io/ttml2021fall/bandits/6.PG-variance-reduction.pdf#:~:text=Let%E2%80%99s%20consider%20the%20variance%20of,At%29%20%E2%88%92b%29G%20j%20t"></a>) (<a href="https://davidrosenberg.github.io/ttml2021fall/bandits/6.PG-variance-reduction.pdf#:~:text=%3D%20E%20%14%20,%10%20G%20j%20t%20%112"></a>)):</p><ul><li><p><strong>Variance without baseline:</strong> $\displaystyle\mathrm{Var}(U) \;=\; \mathbb{E}[X^2 Y^2] \;-\; (\mathbb{E}[X\,Y])^2.$</p></li><li><p><strong>Variance with baseline:</strong> $\displaystyle\mathrm{Var}(U_b) \;=\; \mathbb{E}[X^2 (Y - b(S_t))^2] \;-\; (\mathbb{E}[X\,(Y - b(S_t))])^2.$</p></li></ul><p>We already know $\mathbb{E}[X\,(Y - b(S_t))] = \mathbb{E}[X Y]$ (unbiasedness proved above). So:</p>$$
\begin{aligned}
\mathrm{Var}(U_b) &= \mathbb{E}[X^2 (Y - b(S_t))^2] - (\mathbb{E}[X Y])^2, \\
\text{where } \mathbb{E}[X^2 (Y - b)^2] &= \mathbb{E}[X^2 (Y^2 - 2\,bY + b^2)] \\
&= \mathbb{E}[X^2 Y^2] \;-\; 2\,b\,\mathbb{E}[X^2 Y] \;+\; b^2\,\mathbb{E}[X^2]\,.
\end{aligned}
$$<p>Now, the difference in variances is:</p>$$
\mathrm{Var}(U) - \mathrm{Var}(U_b) \;=\; \mathbb{E}[X^2 Y^2] - (\mathbb{E}[XY])^2 \;-\; \Big(\mathbb{E}[X^2 (Y - b)^2] - (\mathbb{E}[XY])^2\Big)\,.
$$<p>Cancelling the $(\mathbb{E}[XY])^2$ terms and substituting the expanded form, we get:</p>$$
\mathrm{Var}(U) - \mathrm{Var}(U_b) \;=\; \mathbb{E}[X^2 Y^2] - \Big(\mathbb{E}[X^2 Y^2] - 2\,b\,\mathbb{E}[X^2 Y] + b^2\,\mathbb{E}[X^2]\Big)
= 2\,b\,\mathbb{E}[X^2 Y] \;-\; b^2\,\mathbb{E}[X^2]\,.
$$<p>This expression is a quadratic function of $b$. To <strong>maximize</strong> the variance reduction, we can choose an optimal $b$. Setting the derivative with respect to $b$ to zero:</p>$$
\frac{d}{db}\big(2\,b\,\mathbb{E}[X^2 Y] - b^2\,\mathbb{E}[X^2]\big) = 2\,\mathbb{E}[X^2 Y] - 2\,b\,\mathbb{E}[X^2] = 0 \implies b^* \;=\; \frac{\mathbb{E}[X^2 Y]}{\mathbb{E}[X^2]}\,.
$$<p>Plugging $b^*$ back in, the <strong>minimum achievable variance</strong> with an optimal baseline is:</p>$$
\mathrm{Var}(U_{b^*}) \;=\; \mathbb{E}[X^2 Y^2] - (\mathbb{E}[X^2 Y])^2/\mathbb{E}[X^2] \;-\; (\mathbb{E}[XY])^2\,.
$$<p>Compare this to $\mathrm{Var}(U) = \mathbb{E}[X^2 Y^2] - (\mathbb{E}[XY])^2$. The difference is</p>$$
\mathrm{Var}(U) - \mathrm{Var}(U_{b^*}) \;=\; \frac{\big(\mathbb{E}[X^2 Y]\big)^2}{\mathbb{E}[X^2]}\,,
$$<p>which is <strong>nonnegative</strong> (indeed, a perfect square divided by $\mathbb{E}[X^2]>0$). Thus $\mathrm{Var}(U_{b^*}) \le \mathrm{Var}(U)$. In other words, by choosing an appropriate baseline $b$, we can only decrease (or at worst, leave unchanged) the variance of the gradient estimator. Generally, a well-chosen baseline will <strong>significantly reduce variance</strong>. In fact, in policy gradient theory it is known that the variance is minimized when the baseline $b(s)$ equals the <em>expected return</em> from state $s$, i.e. the state-value $V^\pi(s)$ (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>). In that case $Y - b(S_t) = G_t - V^\pi(S_t)$ is the <strong>advantage</strong> $A^\pi(S_t, A_t)$, which has expectation zero <em>given $S_t$</em>. Using the advantage $A(s,a) = Q^\pi(s,a) - V^\pi(s)$ in the update yields the smallest possible variance for an unbiased estimator (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>).</p><p><strong>5. Summary:</strong> We have proven that adding a baseline $b(S_t)$ to the REINFORCE policy gradient estimator leaves its expectation unchanged (unbiased) while reducing its variance. The baseline effectively acts as a <strong>control variate</strong> – a zero-mean adjustment that removes unnecessary fluctuation (<a href="https://datascience.stackexchange.com/questions/15878/advantage-function-variance-reduction#:~:text=As%20you%20mention%20it%20can,the%20variance%20of%20your%20estimate">machine learning - Advantage Function - Variance Reduction - Data Science Stack Exchange</a>). By appropriate choice of $b(s)$ (often an estimate of $V^\pi(s)$), the variance of the gradient estimator is provably lowered without introducing bias (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>). This leads to more stable and sample-efficient learning in practice. The key results can be summarized as:</p><ul><li><p><em>Unbiasedness:</em> $\mathbb{E}[(G_t - b(S_t))\,\nabla_\theta \ln\pi_\theta(A_t|S_t)] = \mathbb{E}[G_t\,\nabla_\theta \ln\pi_\theta(A_t|S_t)] = \nabla_\theta J(\theta)$.</p></li><li><p><em>Variance reduction:</em> $\mathrm{Var}[(G_t - b(S_t))\,\nabla_\theta \ln\pi] = \mathrm{Var}[G_t\,\nabla_\theta \ln\pi] - \frac{(\mathbb{E}[X^2 Y])^2}{\mathbb{E}[X^2]}\,$, which is no greater than the original variance, and is minimized when $b(s)=V^\pi(s)$ (making $Y - b(s)$ the advantage) (<a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/thesis_compressed.pdf#:~:text=The%20reinforcement%20baseline%20does%20not,s%2C%20a%29%20%E2%88%92%20V%20%CF%80"></a>).</p></li></ul><p>Thus, adding a baseline in REINFORCE <strong>keeps the gradient estimator unbiased but lowers its variance</strong>, fulfilling the proof as required.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] Richard S. Sutton, and Andrew G. Barto. &ldquo;Reinforcement learning: An introduction&rdquo;, second edition, 2017.</p><p>[2] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms,” Neural Computation, vol. 6, no. 6, pp. 1185–1201, Nov. 1994, doi: 10.1162/neco.1994.6.6.1185.</p><p>[3] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust Region Policy Optimization,” arXiv:1502.05477 [cs], Feb. 2015, Accessed: Dec. 26, 2017. [Online]. Available: <a href=http://arxiv.org/abs/1502.05477>http://arxiv.org/abs/1502.05477</a></p><p>[4] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv:1707.06347 [cs], Jul. 2017, Accessed: Apr. 10, 2018. [Online]. Available: <a href=http://arxiv.org/abs/1707.06347>http://arxiv.org/abs/1707.06347</a></p><p>[5] Z. Shao et al., “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,” Apr. 27, 2024, arXiv:2402.03300. doi: 10.48550/arXiv.2402.03300.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li></ul><nav class=paginav><a class=next href=https://livey.github.io/posts/2025-02-07-coordinate/><span class=title>Next »</span><br><span>Coordinate Systems</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on x" href="https://x.com/intent/tweet/?text=Reinforcement%20Learning&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f&amp;hashtags=reinforcementlearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f&amp;title=Reinforcement%20Learning&amp;summary=Reinforcement%20Learning&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f&title=Reinforcement%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on telegram" href="https://telegram.me/share/url?text=Reinforcement%20Learning&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-03-05-reinforcement-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Wayformer Paper Reading | Fuwei's Tech Notes</title>
<meta name=keywords content="Trajectory Prediction,Wayformer,Paper Reading,BEV,End to End Autonomous Driving"><meta name=description content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-07-21-wayformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-07-21-wayformer/"><meta property="og:title" content="Wayformer Paper Reading"><meta property="og:description" content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Wayformer Paper Reading"><meta name=twitter:description content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-07-21-wayformer/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="Wayformer Paper Reading"><meta property="og:description" content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-21T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-21T00:00:00+00:00"><meta property="article:tag" content="Trajectory Prediction"><meta property="article:tag" content="Wayformer"><meta property="article:tag" content="Paper Reading"><meta property="article:tag" content="BEV"><meta property="article:tag" content="End to End Autonomous Driving"><meta property="og:image" content="https://livey.github.io/posts/2025-07-21-wayformer/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-07-21-wayformer/%3Cimage%20path/url%3E"><meta name=twitter:title content="Wayformer Paper Reading"><meta name=twitter:description content="A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Wayformer Paper Reading","item":"https://livey.github.io/posts/2025-07-21-wayformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Wayformer Paper Reading","name":"Wayformer Paper Reading","description":"A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms.","keywords":["Trajectory Prediction","Wayformer","Paper Reading","BEV","End to End Autonomous Driving"],"articleBody":"This post provides a technical deep dive into the Wayformer paper [1], a key publication in the field of motion forecasting.\nTraining Overview An overview of the deep learning training pipeline, illustrating the data flow and key components involved during model training. Model Overview of the One-Stage E2E model One staged E2E model. Overview of the Two-Stage E2E model Two staged E2E model. Details of the Two-Stage E2E Model Overview of the Wayformer model. Model Structure Overview (a) (b) The left figure shows the encoder and decoder of the Wayformer model. The right figure shows the details of the encoder [1]. Feature Embedding/Feature Projection $$\\mathbf{f}\\in \\mathbb{R}^{T \\times N\\times D} \\to \\mathbf{x}_{input} \\in \\mathbb{R}^{(T \\cdot N) \\times d}$$Where $T$ is the number of time history, $N$ is the number of entities, $D$ is the number of features, and $d=256$. Feature projection $$\\mathbf{x}_{in} = \\mathbf{f} \\mathbf{W}$$Where $\\mathbf{W} \\in \\mathbb{R}^{D\\times d}$ and $\\mathbf{x}_{in} \\in \\mathbb{R}^{T\\times N \\times d}$\nAdd time and position embedding $$\\mathbf{x}_{input} = \\mathbf{x}_{in} + \\mathbf{p}_t + \\mathbf{p}_s$$Where time embedding: $\\mathbf{p}_t \\in \\mathbb{R}^{1 \\times N \\times d} $, position embedding: $\\mathbf{p}_s \\in \\mathbb{R}^{T \\times 1 \\times d}$. They will broadcast the time and agent dimension respectively.\nSpatio-Temporal Feature Flattening $$\\mathbb{R}^{T\\times N \\times d} \\to \\mathbb{R}^{(T\\cdot N) \\times d}$$Agent and ego $$\\mathbb{R}^{11\\times 32 \\times 28}$$Features:\nPositions (3): x, y, z Dimensions (3): Length, Width, Height Object type (3): One-hot encoding of type (e.g., car, pedestrian, cyclist) is_tracked (1): A flag indicating if the object is tracked or predicted Ego car indicator (1) Time embedding (11): e.g., [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] indicates the state is from one step before the last time step. Exact time (1) Heading (2): cos(yaw) and sin(yaw) Velocity (2): $v_x$, $v_y$ Validity (1) In the implementation, the dimensions are $T = 11$ (10 history + 1 current), $N = 32$ (31 agents + 1 ego), and $D=28$.\nMap The map is represented as hotmaps of segmentations.\n$$\\mathbf{x}^{mp} \\in \\mathbb{R}^{H\\times W \\times C},$$where $H=560, W=160, C=11$. The channel meanings are: 3 for segmentation type, 6 for lane shape, and 2 for lane color.\nThere are two ways to process the map features:\nPatchify-Based\nDivide H and W into a grid of patches. This results in a tensor with shape $H/N^p \\times W/N^p \\times N^p \\times N^p \\times C$. Flatten each patch into a vector, resulting in a tensor with shape: $1 \\times (H/N^p \\times W/N^p) \\times (N^p \\times N^p \\times C)$. Image-Based (CNN)\nStart with a segmentation map of size: $560 \\times 160 \\times 11$. Process it through a series of CNN layers to get a feature map of size: $70 \\times 20 \\times 256$. Reshape this feature map into a sequence of tokens: $1 \\times 1400 \\times 256$. Navigation $$\\mathbb{R}^{1\\times 200 \\times 2}$$Raw inputs: a list of waypoints from the navigation applications. Requests based on current position from the RTK. Process: find the first point within range of interests; transform waypoints into ego coordinate\nResample to a fixed number of points with fixed spacing: if the path is too short, pad with zeros; if the path is too long, retain the first pre-defined number of points\nChange each point into the ego coordinate\nFeatures: Positions: x, y in ego coordinate Update:\nRequest new navigation from current RTK pose or retain from previously computed ones\nTransform to ego coordinate according to relative pose\nRoute $$\\mathbb{R}^{1\\times1500\\times 2}$$Contents: similar to navigation but are global, longer and has more points. For example, it has 50 points and 30m arc spacing between two points. Comparison between route and navigation\nNavigation Route Points 40 30 Spacing 5m 50m Traffic Light $$\\mathbf{x}\\in \\mathbb{R}^{1\\times 10 \\times 16}$$Features:\nPositions (3): x, y, z\nTypes (9): left turn, right turn, straight, U-turn, etc\nColor (3): red, yellow, green\nConfidence Score Road Sign $$\\mathbb{R}^{1\\times 10 \\times 14}$$Contents: stop, yield, speed limit, no entry, pedestrian crossing, turn restriction, etc. Features: Position (2): x, y Bounding box corners (8)\nTypes (3): crosswalk, bump, no_parking_area\nConfidence score (1)\nRoad Arrow $$\\mathbb{R}^{1\\times 10 \\times 20}$$Contents: directional arrows on the road surface that indicate allowed or recommended driving directions (such as turn left, go straight, turn right, etc.)\nFeatures: Center position (2): x, y Direction (2) : cos, sin\nBounding box corners (8)\nType (7): left, right, straight, U-turn, etc\nScore(1)\nGround Truth Trajectory The next 5 or 8 seconds’ trajectories on the current coordinate:\n$$\\mathbb{R}^{50\\times 2}$$The trajectory is from SLAM, RTK, or cyber pose. So, relative pose accuracy is important.\nPerceivers Standard Transformer Standard Encoder Decoder Structure Illustration of the attention mechanism in the Transformer architecture [2]. Tokenize/Projecting the original input into feature vector: $\\mathbf{x}_{in}$\nAdd position embedding feature to input: $\\mathbf{x}_{input} = \\mathbf{x}_{in} + \\mathbf{x}_{pos}$\nMap input into key, value, query: $\\mathbf{x}_{input} \\to \\mathbf{Q}, \\mathbf{K},\\mathbf{V}$\nSelf attention in encoder\nStarting with a given token, e.g., , self attention, and cross attention with encoded in decoder\nDetails in Attention $$ \\boxed{ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\right) \\mathbf{V} } $$We define:\n$\\mathbf{Q} \\in \\mathbb{R}^{n \\times d_k}$, Query matrix $\\mathbf{Q} = \\begin{bmatrix} \\mathbf{q}_1^\\top \\\\ \\mathbf{q}_2^\\top \\\\ \\mathbf{q}_3^\\top \\\\ \\vdots\\\\ \\mathbf{q}_n^\\top \\\\ \\end{bmatrix} $\n$\\mathbf{K} \\in \\mathbb{R}^{m \\times d_k}$, Key matrix $\\mathbf{K} = \\begin{bmatrix} \\mathbf{k}_1^\\top \\\\ \\mathbf{k}_2^\\top \\\\ \\mathbf{k}_3^\\top \\\\ \\vdots\\\\ \\mathbf{k}_m \\end{bmatrix} $\n$\\mathbf{V} \\in \\mathbb{R}^{m \\times d_v}$, Value matrix $\\mathbf{V}= \\begin{bmatrix} \\mathbf{v}_1^\\top \\\\ \\mathbf{v}_2^\\top \\\\ \\mathbf{v}_3^\\top \\\\ \\vdots\\\\ \\mathbf{v}_m^\\top \\\\ \\end{bmatrix} $\nwhere\n$\\mathbf{q}_i \\in \\mathbb{R}^{d_k}$, $\\mathbf{k}_j \\in \\mathbb{R}^{d_k}$, $\\mathbf{v}_j \\in \\mathbb{R}^{d_v}$ $n$: number of queries $m$: number of keys/values Step 1: Dot Product Between Queries and Keys\n$$\\mathbf{S} = \\mathbf{Q} \\mathbf{K}^\\top \\in \\mathbb{R}^{n \\times m}$$Each element:\n$$S_{ij} = \\mathbf{q}_i^\\top \\mathbf{k}_j$$So:\n$$\\mathbf{S} = \\begin{bmatrix} \\mathbf{q}_1^\\top \\mathbf{k}_1 \u0026 \\cdots \u0026 \\mathbf{q}_1^\\top \\mathbf{k}_m \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\mathbf{q}_n^\\top \\mathbf{k}_1 \u0026 \\cdots \u0026 \\mathbf{q}_n^\\top \\mathbf{k}_m \\end{bmatrix}$$Step 2: Scale the Scores\n$$\\mathbf{S}' = \\frac{\\mathbf{S}}{\\sqrt{d_k}} = \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times m}$$Step 3: Apply Softmax\n$$\\mathbf{A} = \\text{softmax}(\\mathbf{S}') \\in \\mathbb{R}^{n \\times m}$$Each row of $\\mathbf{A}$, denoted $\\mathbf{a}_i \\in \\mathbb{R}^{m}$, contains attention weights for query $\\mathbf{q}_i$:\n$$ \\mathbf{a}_i = \\text{softmax}\\left( \\frac{\\mathbf{q}_i^\\top \\mathbf{K}^\\top}{\\sqrt{d_k}} \\right) $$That is:\n$$a_{ij} = \\frac{\\exp\\left( \\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_k}} \\right)}{\\sum\\limits_{j'=1}^{m} \\exp\\left( \\frac{\\mathbf{q}_i^\\top \\mathbf{k}_{j'}}{\\sqrt{d_k}} \\right)}$$Step 4: Multiply Attention Weights by Value Matrix\n$$\\mathbf{O} = \\mathbf{A} \\mathbf{V} \\in \\mathbb{R}^{n \\times d_v}$$Step 5: Compute Each Output Vector\nEach output vector $\\mathbf{o}_i \\in \\mathbb{R}^{d_v}$ is a weighted sum of all value vectors $\\mathbf{v}_j$, weighted by attention weights $a_{ij}$:\n$$\\mathbf{o}_i = \\sum_{j=1}^{m} a_{ij} \\mathbf{v}_j.$$So the full output is:\n$$ \\mathbf{O} = \\begin{bmatrix} \\mathbf{o}_1^\\top \\\\ \\mathbf{o}_2^\\top \\\\ \\vdots \\\\ \\mathbf{o}_n^\\top \\end{bmatrix} \\in \\mathbb{R}^{n \\times d_v}. $$Clarification of Self-Attention and Cross-Attention Self-Attention Core Mechanism:Operates on a single input sequence. Queries (Q), keys (K), and values (V) are derived from the same source. Purpose:Captures intra-sequence dependencies (e.g., relationships between agents in a scene or words in a sentence). Cross-Attention Core Mechanism:Queries (Q) come from one sequence, while keys (K) and values (V) come from another independent sequence. Purpose:Enables cross-modal integration (e.g., fusing agent dynamics with map semantics or routing data). Key Differences\nAspect Self-Attention Cross-Attention Input Sources Q, K, V from same sequence Q from sequence A; K/V from sequence B Primary Role Intra-sequence relationship modeling Inter-sequence information fusion Perceiver [3] Illustration of the Perceiver architecture, which uses a cross-attention mechanism to handle large and diverse inputs by mapping them to a smaller latent array [3]. Scene Encoder/ Perceiver Encoder A PerceiverEncoder is designed to solve a different problem: handling extremely large inputs (like images, audio, or your map data) that are too big for standard self-attention.Its goal is not to enrich the giant input, but to distill it into a small, manageable, fixed-size latent array. To do this, two different things to interact:\nOur input is too big for standard self-attention\nThe only way for the small latent array to “read” or “query” information from the large input data is through cross-attention.\nTrajectory Decoder/Perceiver Decoder Given learnable output query\nCross attention with the output of the encoder\nSelf attention\nOutput $$\\{\\boldsymbol{\\pi}_i, \\boldsymbol{\\mu}_i, \\boldsymbol{\\sigma}_i\\}$$where $\\boldsymbol{\\pi}_i$ is the mixing coefficient, $\\boldsymbol{\\mu}_i$ is the mean, and $\\boldsymbol{\\sigma}_i$ is the standard deviation.\nGaussian mixture\n$$p(\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma})=\\sum_i \\pi_i\\mathcal{N}(\\boldsymbol{\\mu}^i, \\boldsymbol{\\sigma}^i)$$Or\nMixture of Laplace Distributions\nA Laplace distribution has the probability density function:\n$$f(x \\mid \\mu, b) = \\frac{1}{2b} \\exp\\left( -\\frac{|x - \\mu|}{b} \\right),$$ predicted probability: $N_{mode}$\npredicted trajectory: $N_{mode}\\times N_T \\times 5$, ( $\\mu_x, \\mu_y, s_x, s_y, \\rho$) where the covariance matrix is:\n$$\\mathbf{\\Sigma}= \\begin{bmatrix} s_x^2 \u0026 \\rho s_x s_y \\\\ \\rho s_x s_y \u0026 s_y^2 \\end{bmatrix}$$Loss The classification loss + the regression loss\nClassification loss: select the predicted trajectory with ground truth and compute its corresponding cross entropy\nRegression loss: minimize the negative likelihood loss for the selected Gaussian with ground truth trajectory\nReferences [1] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and B. Sapp, “Wayformer: Motion Forecasting via Simple \u0026 Efficient Attention Networks,” Jul. 12, 2022, arXiv: arXiv:2207.05844. doi: 10.48550/arXiv.2207.05844.\n[2] A. Vaswani et al., “Attention is All you Need,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2017.\n[3] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. Hénaff, M. M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, “Perceiver IO: A General Architecture for Structured Inputs \u0026 Outputs,” CoRR, vol. abs/2107.14795, 2021. [Online]. Available: https://arxiv.org/abs/2107.14795\n","wordCount":"1529","inLanguage":"en","image":"https://livey.github.io/posts/2025-07-21-wayformer/%3Cimage%20path/url%3E","datePublished":"2025-07-21T00:00:00Z","dateModified":"2025-07-21T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-07-21-wayformer/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Wayformer Paper Reading</h1><div class=post-description>A technical breakdown of the Wayformer paper for motion forecasting, focusing on its architecture and attention mechanisms.</div><div class=post-meta><span title='2025-07-21 00:00:00 +0000 UTC'>July 21, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1529 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#training-overview aria-label="Training Overview">Training Overview</a></li></ul><li><a href=#model aria-label=Model>Model</a><ul><ul><li><a href=#overview-of-the-one-stage-e2e-model aria-label="Overview of the One-Stage E2E model">Overview of the One-Stage E2E model</a></li><li><a href=#overview-of-the-two-stage-e2e-model aria-label="Overview of the Two-Stage E2E model">Overview of the Two-Stage E2E model</a></li><li><a href=#details-of-the-two-stage-e2e-model aria-label="Details of the Two-Stage E2E Model">Details of the Two-Stage E2E Model</a></li></ul></ul></li><li><a href=#model-structure-overview aria-label="Model Structure Overview">Model Structure Overview</a><ul><li><a href=#feature-embeddingfeature-projection aria-label="Feature Embedding/Feature Projection">Feature Embedding/Feature Projection</a><ul><li><a href=#agent-and-ego aria-label="Agent and ego">Agent and ego</a></li><li><a href=#map aria-label=Map>Map</a></li><li><a href=#navigation aria-label=Navigation>Navigation</a></li><li><a href=#route aria-label=Route>Route</a></li><li><a href=#traffic-light aria-label="Traffic Light">Traffic Light</a></li><li><a href=#road-sign aria-label="Road Sign">Road Sign</a></li><li><a href=#road-arrow aria-label="Road Arrow">Road Arrow</a></li><li><a href=#ground-truth-trajectory aria-label="Ground Truth Trajectory">Ground Truth Trajectory</a></li></ul></li><li><a href=#perceivers aria-label=Perceivers>Perceivers</a><ul><li><a href=#standard-transformer aria-label="Standard Transformer">Standard Transformer</a><ul><li><a href=#standard-encoder-decoder-structure aria-label="Standard Encoder Decoder Structure">Standard Encoder Decoder Structure</a></li><li><a href=#details-in-attention aria-label="Details in Attention">Details in Attention</a></li><li><a href=#clarification-of-self-attention-and-cross-attention aria-label="Clarification of Self-Attention and Cross-Attention">Clarification of Self-Attention and Cross-Attention</a></li></ul></li><li><a href=#perceiver-3 aria-label="Perceiver [3]">Perceiver [3]</a><ul><li><a href=#scene-encoder-perceiver-encoder aria-label="Scene Encoder/ Perceiver Encoder">Scene Encoder/ Perceiver Encoder</a></li><li><a href=#trajectory-decoderperceiver-decoder aria-label="Trajectory Decoder/Perceiver Decoder">Trajectory Decoder/Perceiver Decoder</a></li></ul></li></ul></li><li><a href=#output aria-label=Output>Output</a></li><li><a href=#loss aria-label=Loss>Loss</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>This post provides a technical deep dive into the Wayformer paper [1], a key publication in the field of motion forecasting.</p><h2 id=training-overview>Training Overview<a hidden class=anchor aria-hidden=true href=#training-overview>#</a></h2><figure style=text-align:center><img src=./resources/training.png alt="Wayformer training pipeline" style="width:80%;margin:0 auto;display:block"><figcaption style=font-weight:400>An overview of the deep learning training pipeline, illustrating the data flow and key components involved during model training.</figcaption></figure><h1 id=model>Model<a hidden class=anchor aria-hidden=true href=#model>#</a></h1><h3 id=overview-of-the-one-stage-e2e-model>Overview of the One-Stage E2E model<a hidden class=anchor aria-hidden=true href=#overview-of-the-one-stage-e2e-model>#</a></h3><figure style=text-align:center><img src=./resources/one-stage-e2e.png alt="One staged E2E model" style="width:80%;margin:0 auto;display:block"><figcaption style=font-weight:400>One staged E2E model.</figcaption></figure><h3 id=overview-of-the-two-stage-e2e-model>Overview of the Two-Stage E2E model<a hidden class=anchor aria-hidden=true href=#overview-of-the-two-stage-e2e-model>#</a></h3><figure style=text-align:center><img src=./resources/two-stage-e2e.png alt="Two staged E2E model" style="width:80%;margin:0 auto;display:block"><figcaption style=font-weight:400>Two staged E2E model.</figcaption></figure><h3 id=details-of-the-two-stage-e2e-model>Details of the Two-Stage E2E Model<a hidden class=anchor aria-hidden=true href=#details-of-the-two-stage-e2e-model>#</a></h3><figure style=text-align:center><img src=./resources/model-struct.png alt="Overview of the Wayformer model" style="width:80%;margin:0 auto;display:block"><figcaption style=font-weight:400>Overview of the Wayformer model.</figcaption></figure><h1 id=model-structure-overview>Model Structure Overview<a hidden class=anchor aria-hidden=true href=#model-structure-overview>#</a></h1><figure style=display:flex;flex-direction:column;align-items:center;margin-bottom:10px><div style=display:flex;justify-content:center;align-items:flex-start;gap:20px;width:100%><div style=text-align:center><img src=./resources/wayformer-fig1.png alt="Wayformer Figure 1" style=width:100%;max-width:400px><figcaption style=font-weight:400>(a)</figcaption></div><div style=text-align:center><img src=./resources/wayformer-fig2.png alt="Wayformer Figure 2" style=width:89%;max-width:300px><figcaption style=font-weight:400>(b)</figcaption></div></div><figcaption style=font-style:normal;margin-top:8px;font-weight:400>The left figure shows the encoder and decoder of the Wayformer model. The right figure shows the details of the encoder [1].</figcaption></figure><h2 id=feature-embeddingfeature-projection>Feature Embedding/Feature Projection<a hidden class=anchor aria-hidden=true href=#feature-embeddingfeature-projection>#</a></h2>$$\mathbf{f}\in \mathbb{R}^{T \times N\times D} \to \mathbf{x}_{input} \in \mathbb{R}^{(T \cdot N) \times d}$$<p>Where $T$ is the number of time history, $N$ is the number of entities, $D$ is the number of features, and $d=256$.</p><ol><li>Feature projection</li></ol>$$\mathbf{x}_{in} = \mathbf{f} \mathbf{W}$$<p>Where $\mathbf{W} \in \mathbb{R}^{D\times d}$ and $\mathbf{x}_{in} \in \mathbb{R}^{T\times N \times d}$</p><ol start=2><li>Add time and position embedding</li></ol>$$\mathbf{x}_{input} = \mathbf{x}_{in} + \mathbf{p}_t + \mathbf{p}_s$$<p>Where time embedding: $\mathbf{p}_t \in \mathbb{R}^{1 \times N \times d} $, position embedding: $\mathbf{p}_s \in \mathbb{R}^{T \times 1 \times d}$. They will broadcast the time and agent dimension respectively.</p><ol start=3><li>Spatio-Temporal Feature Flattening</li></ol>$$\mathbb{R}^{T\times N \times d} \to \mathbb{R}^{(T\cdot N) \times d}$$<h3 id=agent-and-ego>Agent and ego<a hidden class=anchor aria-hidden=true href=#agent-and-ego>#</a></h3>$$\mathbb{R}^{11\times 32 \times 28}$$<p>Features:</p><ol><li>Positions (3): x, y, z</li><li>Dimensions (3): Length, Width, Height</li><li>Object type (3): One-hot encoding of type (e.g., car, pedestrian, cyclist)</li><li><code>is_tracked</code> (1): A flag indicating if the object is tracked or predicted</li><li>Ego car indicator (1)</li><li>Time embedding (11): e.g., <code>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</code> indicates the state is from one step before the last time step.</li><li>Exact time (1)</li><li>Heading (2): <code>cos(yaw)</code> and <code>sin(yaw)</code></li><li>Velocity (2): $v_x$, $v_y$</li><li>Validity (1)</li></ol><p>In the implementation, the dimensions are $T = 11$ (10 history + 1 current), $N = 32$ (31 agents + 1 ego), and $D=28$.</p><h3 id=map>Map<a hidden class=anchor aria-hidden=true href=#map>#</a></h3><p>The map is represented as hotmaps of segmentations.</p>$$\mathbf{x}^{mp} \in \mathbb{R}^{H\times W \times C},$$<p>where $H=560, W=160, C=11$. The channel meanings are: 3 for segmentation type, 6 for lane shape, and 2 for lane color.</p><p>There are two ways to process the map features:</p><p><strong>Patchify-Based</strong></p><ol><li>Divide H and W into a grid of patches. This results in a tensor with shape $H/N^p \times W/N^p \times N^p \times N^p \times C$.</li><li>Flatten each patch into a vector, resulting in a tensor with shape: $1 \times (H/N^p \times W/N^p) \times (N^p \times N^p \times C)$.</li></ol><p><strong>Image-Based (CNN)</strong></p><ol><li>Start with a segmentation map of size: $560 \times 160 \times 11$.</li><li>Process it through a series of CNN layers to get a feature map of size: $70 \times 20 \times 256$.</li><li>Reshape this feature map into a sequence of tokens: $1 \times 1400 \times 256$.</li></ol><h3 id=navigation>Navigation<a hidden class=anchor aria-hidden=true href=#navigation>#</a></h3>$$\mathbb{R}^{1\times 200 \times 2}$$<p>Raw inputs: a list of waypoints from the navigation applications.</p><p>Requests based on current position from the RTK.</p><p>Process: find the first point within range of interests; transform waypoints into ego coordinate</p><ol><li><p>Resample to a fixed number of points with fixed spacing: if the path is too short, pad with zeros; if the path is too long, retain the first pre-defined number of points</p></li><li><p>Change each point into the ego coordinate</p></li></ol><p>Features:</p><ol><li>Positions: x, y in ego coordinate</li></ol><p>Update:</p><ol><li><p>Request new navigation from current RTK pose or retain from previously computed ones</p></li><li><p>Transform to ego coordinate according to relative pose</p></li></ol><h3 id=route>Route<a hidden class=anchor aria-hidden=true href=#route>#</a></h3>$$\mathbb{R}^{1\times1500\times 2}$$<p>Contents: similar to navigation but are global, longer and has more points.</p><p>For example, it has 50 points and 30m arc spacing between two points.</p><p>Comparison between route and navigation</p><table><thead><tr><th></th><th>Navigation</th><th>Route</th></tr></thead><tbody><tr><td>Points</td><td>40</td><td>30</td></tr><tr><td>Spacing</td><td>5m</td><td>50m</td></tr></tbody></table><h3 id=traffic-light>Traffic Light<a hidden class=anchor aria-hidden=true href=#traffic-light>#</a></h3>$$\mathbf{x}\in \mathbb{R}^{1\times 10 \times 16}$$<p>Features:</p><ol><li><p>Positions (3): x, y, z</p></li><li><p>Types (9): left turn, right turn, straight, U-turn, etc</p></li><li><p>Color (3): red, yellow, green</p></li><li><p>Confidence Score</p></li></ol><h3 id=road-sign>Road Sign<a hidden class=anchor aria-hidden=true href=#road-sign>#</a></h3>$$\mathbb{R}^{1\times 10 \times 14}$$<p>Contents: stop, yield, speed limit, no entry, pedestrian crossing, turn restriction, etc.</p><p>Features:</p><ol><li><p>Position (2): x, y</p></li><li><p>Bounding box corners (8)</p></li><li><p>Types (3): crosswalk, bump, no_parking_area</p></li><li><p>Confidence score (1)</p></li></ol><h3 id=road-arrow>Road Arrow<a hidden class=anchor aria-hidden=true href=#road-arrow>#</a></h3>$$\mathbb{R}^{1\times 10 \times 20}$$<p>Contents: directional arrows on the road surface that indicate allowed or recommended driving directions (such as turn left, go straight, turn right, etc.)</p><p>Features:</p><ol><li><p>Center position (2): x, y</p></li><li><p>Direction (2) : cos, sin</p></li><li><p>Bounding box corners (8)</p></li><li><p>Type (7): left, right, straight, U-turn, etc</p></li><li><p>Score(1)</p></li></ol><h3 id=ground-truth-trajectory>Ground Truth Trajectory<a hidden class=anchor aria-hidden=true href=#ground-truth-trajectory>#</a></h3><p>The next 5 or 8 seconds&rsquo; trajectories on the current coordinate:</p>$$\mathbb{R}^{50\times 2}$$<p>The trajectory is from SLAM, RTK, or cyber pose. So, relative pose accuracy is important.</p><h2 id=perceivers>Perceivers<a hidden class=anchor aria-hidden=true href=#perceivers>#</a></h2><h3 id=standard-transformer>Standard Transformer<a hidden class=anchor aria-hidden=true href=#standard-transformer>#</a></h3><h4 id=standard-encoder-decoder-structure>Standard Encoder Decoder Structure<a hidden class=anchor aria-hidden=true href=#standard-encoder-decoder-structure>#</a></h4><figure style=text-align:center><img src=./resources/attention.png alt="Attention mechanism illustration" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>Illustration of the attention mechanism in the Transformer architecture [2].</figcaption></figure><ol><li><p>Tokenize/Projecting the original input into feature vector: $\mathbf{x}_{in}$</p></li><li><p>Add position embedding feature to input: $\mathbf{x}_{input} = \mathbf{x}_{in} + \mathbf{x}_{pos}$</p></li><li><p>Map input into key, value, query: $\mathbf{x}_{input} \to \mathbf{Q}, \mathbf{K},\mathbf{V}$</p></li><li><p>Self attention in encoder</p></li><li><p>Starting with a given token, e.g., &lt;start>, self attention, and cross attention with encoded in decoder</p></li></ol><h4 id=details-in-attention>Details in Attention<a hidden class=anchor aria-hidden=true href=#details-in-attention>#</a></h4>$$
\boxed{
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax} \left( \frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}} \right) \mathbf{V}
}
$$<p>We define:</p><ul><li><p>$\mathbf{Q} \in \mathbb{R}^{n \times d_k}$, Query matrix
$\mathbf{Q} =
\begin{bmatrix}
\mathbf{q}_1^\top \\
\mathbf{q}_2^\top \\
\mathbf{q}_3^\top \\
\vdots\\
\mathbf{q}_n^\top \\
\end{bmatrix}
$</p></li><li><p>$\mathbf{K} \in \mathbb{R}^{m \times d_k}$, Key matrix $\mathbf{K} =
\begin{bmatrix}
\mathbf{k}_1^\top \\
\mathbf{k}_2^\top \\
\mathbf{k}_3^\top \\
\vdots\\
\mathbf{k}_m
\end{bmatrix}
$</p></li><li><p>$\mathbf{V} \in \mathbb{R}^{m \times d_v}$, Value matrix $\mathbf{V}=
\begin{bmatrix}
\mathbf{v}_1^\top \\
\mathbf{v}_2^\top \\
\mathbf{v}_3^\top \\
\vdots\\
\mathbf{v}_m^\top \\
\end{bmatrix}
$</p></li></ul><p>where</p><ul><li>$\mathbf{q}_i \in \mathbb{R}^{d_k}$, $\mathbf{k}_j \in \mathbb{R}^{d_k}$, $\mathbf{v}_j \in \mathbb{R}^{d_v}$</li><li>$n$: number of queries</li><li>$m$: number of keys/values</li></ul><p><strong>Step 1: Dot Product Between Queries and Keys</strong></p>$$\mathbf{S} = \mathbf{Q} \mathbf{K}^\top \in \mathbb{R}^{n \times m}$$<p>Each element:</p>$$S_{ij} = \mathbf{q}_i^\top \mathbf{k}_j$$<p>So:</p>$$\mathbf{S} =
\begin{bmatrix}
\mathbf{q}_1^\top \mathbf{k}_1 & \cdots & \mathbf{q}_1^\top \mathbf{k}_m \\
\vdots & \ddots & \vdots \\
\mathbf{q}_n^\top \mathbf{k}_1 & \cdots & \mathbf{q}_n^\top \mathbf{k}_m
\end{bmatrix}$$<p><strong>Step 2: Scale the Scores</strong></p>$$\mathbf{S}' = \frac{\mathbf{S}}{\sqrt{d_k}} = \frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}} \in \mathbb{R}^{n \times m}$$<p><strong>Step 3: Apply Softmax</strong></p>$$\mathbf{A} = \text{softmax}(\mathbf{S}') \in \mathbb{R}^{n \times m}$$<p>Each row of $\mathbf{A}$, denoted $\mathbf{a}_i \in \mathbb{R}^{m}$, contains attention weights for query $\mathbf{q}_i$:</p>$$
\mathbf{a}_i = \text{softmax}\left( \frac{\mathbf{q}_i^\top \mathbf{K}^\top}{\sqrt{d_k}} \right)
$$<p>That is:</p>$$a_{ij} = \frac{\exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}} \right)}{\sum\limits_{j'=1}^{m} \exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_{j'}}{\sqrt{d_k}} \right)}$$<p><strong>Step 4: Multiply Attention Weights by Value Matrix</strong></p>$$\mathbf{O} = \mathbf{A} \mathbf{V} \in \mathbb{R}^{n \times d_v}$$<p><strong>Step 5: Compute Each Output Vector</strong></p><p>Each output vector $\mathbf{o}_i \in \mathbb{R}^{d_v}$ is a weighted sum of all value vectors $\mathbf{v}_j$, weighted by attention weights $a_{ij}$:</p>$$\mathbf{o}_i = \sum_{j=1}^{m} a_{ij} \mathbf{v}_j.$$<p>So the full output is:</p>$$
\mathbf{O} =
\begin{bmatrix}
\mathbf{o}_1^\top \\
\mathbf{o}_2^\top \\
\vdots \\
\mathbf{o}_n^\top
\end{bmatrix}
\in \mathbb{R}^{n \times d_v}.
$$<h4 id=clarification-of-self-attention-and-cross-attention><strong>Clarification of Self-Attention and Cross-Attention</strong><a hidden class=anchor aria-hidden=true href=#clarification-of-self-attention-and-cross-attention>#</a></h4><ol><li>Self-Attention</li></ol><ul><li><p><strong>Core Mechanism</strong>:Operates on a <strong>single input sequence</strong>. Queries (Q), keys (K), and values (V) are derived from the same source.</p></li><li><p><strong>Purpose</strong>:Captures <strong>intra-sequence dependencies</strong> (e.g., relationships between agents in a scene or words in a sentence).</p></li></ul><ul><li>Cross-Attention</li></ul><ul><li><p><strong>Core Mechanism</strong>:Queries (Q) come from <strong>one sequence</strong>, while keys (K) and values (V) come from <strong>another independent sequence</strong>.</p></li><li><p><strong>Purpose</strong>:Enables <strong>cross-modal integration</strong> (e.g., fusing agent dynamics with map semantics or routing data).</p></li></ul><p><strong>Key Differences</strong></p><table><thead><tr><th><strong>Aspect</strong></th><th>Self-Attention</th><th>Cross-Attention</th></tr></thead><tbody><tr><td><strong>Input Sources</strong></td><td>Q, K, V from same sequence</td><td>Q from sequence A; K/V from sequence B</td></tr><tr><td><strong>Primary Role</strong></td><td>Intra-sequence relationship modeling</td><td>Inter-sequence information fusion</td></tr></tbody></table><h3 id=perceiver-3>Perceiver [3]<a hidden class=anchor aria-hidden=true href=#perceiver-3>#</a></h3><figure style=text-align:center><img src=./resources/perceiver.png alt="Perceiver architecture diagram" style="width:80%;margin:0 auto;display:block"><figcaption style=font-weight:400>Illustration of the Perceiver architecture, which uses a cross-attention mechanism to handle large and diverse inputs by mapping them to a smaller latent array [3].</figcaption></figure><h4 id=scene-encoder-perceiver-encoder>Scene Encoder/ Perceiver Encoder<a hidden class=anchor aria-hidden=true href=#scene-encoder-perceiver-encoder>#</a></h4><p>A PerceiverEncoder is designed to solve a different problem: handling extremely large inputs (like images, audio, or your map data) that are too big for standard self-attention.Its goal is not to enrich the giant input, but to distill it into a small, manageable, fixed-size latent array. To do this, two different things to interact:</p><ol><li><p>Our input is too big for standard self-attention</p></li><li><p>The only way for the small latent array to &ldquo;read&rdquo; or &ldquo;query&rdquo; information from the large input data is through cross-attention.</p></li></ol><h4 id=trajectory-decoderperceiver-decoder>Trajectory Decoder/Perceiver Decoder<a hidden class=anchor aria-hidden=true href=#trajectory-decoderperceiver-decoder>#</a></h4><ol><li><p>Given learnable output query</p></li><li><p>Cross attention with the output of the encoder</p></li><li><p>Self attention</p></li></ol><h2 id=output>Output<a hidden class=anchor aria-hidden=true href=#output>#</a></h2>$$\{\boldsymbol{\pi}_i, \boldsymbol{\mu}_i, \boldsymbol{\sigma}_i\}$$<p>where $\boldsymbol{\pi}_i$ is the mixing coefficient, $\boldsymbol{\mu}_i$ is the mean, and $\boldsymbol{\sigma}_i$ is the standard deviation.</p><p>Gaussian mixture</p>$$p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\sigma})=\sum_i \pi_i\mathcal{N}(\boldsymbol{\mu}^i, \boldsymbol{\sigma}^i)$$<p>Or</p><p>Mixture of Laplace Distributions</p><p>A Laplace distribution has the probability density function:</p>$$f(x \mid \mu, b) = \frac{1}{2b} \exp\left( -\frac{|x - \mu|}{b} \right),$$<ul><li><p>predicted probability: $N_{mode}$</p></li><li><p>predicted trajectory: $N_{mode}\times N_T \times 5$, ( $\mu_x, \mu_y, s_x, s_y, \rho$) where the covariance matrix is:</p></li></ul>$$\mathbf{\Sigma}=
\begin{bmatrix}
s_x^2 & \rho s_x s_y \\
\rho s_x s_y & s_y^2
\end{bmatrix}$$<h2 id=loss>Loss<a hidden class=anchor aria-hidden=true href=#loss>#</a></h2><p>The classification loss + the regression loss</p><ol><li><p>Classification loss: select the predicted trajectory with ground truth and compute its corresponding cross entropy</p></li><li><p>Regression loss: minimize the negative likelihood loss for the selected Gaussian with ground truth trajectory</p></li></ol><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and B. Sapp, “Wayformer: Motion Forecasting via Simple & Efficient Attention Networks,” Jul. 12, 2022, <em>arXiv</em>: arXiv:2207.05844. doi: <a href=https://doi.org/10.48550/arXiv.2207.05844>10.48550/arXiv.2207.05844</a>.</p><p>[2] A. Vaswani et al., “Attention is All you Need,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2017.</p><p>[3] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. Hénaff, M. M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, “Perceiver IO: A General Architecture for Structured Inputs & Outputs,” CoRR, vol. abs/2107.14795, 2021. [Online]. Available: <a href=https://arxiv.org/abs/2107.14795>https://arxiv.org/abs/2107.14795</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/trajectory-prediction/>Trajectory Prediction</a></li><li><a href=https://livey.github.io/tags/wayformer/>Wayformer</a></li><li><a href=https://livey.github.io/tags/paper-reading/>Paper Reading</a></li><li><a href=https://livey.github.io/tags/bev/>BEV</a></li><li><a href=https://livey.github.io/tags/end-to-end-autonomous-driving/>End to End Autonomous Driving</a></li></ul><nav class=paginav><a class=next href=https://livey.github.io/posts/2025-02-20-slam/><span class=title>Next »</span><br><span>LiDAR-SLAM Decoded: From Point Clouds to Precision Maps</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on x" href="https://x.com/intent/tweet/?text=Wayformer%20Paper%20Reading&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f&amp;hashtags=TrajectoryPrediction%2cWayformer%2cPaperReading%2cBEV%2cEndtoEndAutonomousDriving"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f&amp;title=Wayformer%20Paper%20Reading&amp;summary=Wayformer%20Paper%20Reading&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f&title=Wayformer%20Paper%20Reading"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on whatsapp" href="https://api.whatsapp.com/send?text=Wayformer%20Paper%20Reading%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on telegram" href="https://telegram.me/share/url?text=Wayformer%20Paper%20Reading&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Wayformer Paper Reading on ycombinator" href="https://news.ycombinator.com/submitlink?t=Wayformer%20Paper%20Reading&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-07-21-wayformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
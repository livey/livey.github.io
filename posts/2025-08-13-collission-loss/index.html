<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving | Fuwei's Tech Notes</title>
<meta name=keywords content="Autonomous Driving,Collision Loss,End-to-End Planning,Probabilistic Maps,Cost Map,Signed Distance Map"><meta name=description content="Collision loss design in end-to-end autonomous driving model training."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-08-13-collission-loss/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="Collision loss design in end-to-end autonomous driving model training."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-08-13-collission-loss/"><meta property="og:title" content="Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving"><meta property="og:description" content="Collision loss design in end-to-end autonomous driving model training."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving"><meta name=twitter:description content="Collision loss design in end-to-end autonomous driving model training."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-08-13-collission-loss/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving"><meta property="og:description" content="Collision loss design in end-to-end autonomous driving model training."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-14T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-14T00:00:00+00:00"><meta property="article:tag" content="Autonomous-Driving"><meta property="article:tag" content="Collision Loss"><meta property="article:tag" content="End-to-End Planning"><meta property="article:tag" content="Probabilistic Maps"><meta property="article:tag" content="Cost Map"><meta property="article:tag" content="Signed Distance Map"><meta property="og:image" content="https://livey.github.io/posts/2025-08-13-collission-loss/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-08-13-collission-loss/%3Cimage%20path/url%3E"><meta name=twitter:title content="Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving"><meta name=twitter:description content="Collision loss design in end-to-end autonomous driving model training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving","item":"https://livey.github.io/posts/2025-08-13-collission-loss/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving","name":"Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving","description":"Collision loss design in end-to-end autonomous driving model training.","keywords":["Autonomous Driving","Collision Loss","End-to-End Planning","Probabilistic Maps","Cost Map","Signed Distance Map"],"articleBody":"This post explores collision loss design for end-to-end autonomous driving training, focusing on extending PLUTO’s binary occupancy map approach to handle probabilistic maps. The method enables safer autonomous driving by providing smooth, uncertainty-aware collision avoidance while maintaining computational efficiency.\nUsing Signed Distance Map with Binary Occupancy Map How PLUTO Builds the Loss Map [2] Vehicle Model In PLUTO and other planning algorithms, the vehicle is modelded as a series of overlapping discs.\nVehicle model covered by overlapping discs (as used in PLUTO) Distance Map The PLUTO planner for imitation learning-based planning uses auxiliary losses for safety, including a collision loss.\nPLUTO starts with a binary occupancy map (voxel grid):\n1 = obstacle or off-road cell 0 = free/drivable cell\nFrom this map, PLUTO computes an Euclidean Signed Distance Field (ESDF) [1]:\nEach cell stores the shortest Euclidean distance to the nearest obstacle.\nPositive distances indicate free space, zero indicates contact with an obstacle.\nThe distance map $D(f(p))$ is defined as:\n$$ D f (p) = \\min_{q\\in G}\\left (d(p, q) + f (q)\\right) $$Many algorithms for computing the distance transform of point sets use the equivalent definition\n$$ D(p) = \\min_{q\\in G}\\left( d(p, q) + 1(q)\\right) $$where $1(q)$ is an indicator function for membership in $P$:\n$$ 1(q) = \\begin{cases} 0, \\text{ if } q\\in P \\\\ \\infin, \\text{ otherwise } \\end{cases} $$ The distance map can be computed with linear computational complexity [1]. Below shows an example of the computed signed Euclidean distance map.\nExample of computed signed Euclidean distance map Loss Formulation For each predicted trajectory point, PLUTO covers the vehicle with several covering circles (each circle has a center $c$ and radius $R$).\nThe circle’s center is projected into the signed distance map.\nThe signed distance $d(c)$ is obtained via bilinear interpolation.\nA hinge loss penalizes intrusions into obstacles:\n$$ \\ell_\\text{collide}(c) = \\max\\bigl(0, R + \\varepsilon - d(c)\\bigr) $$where:\n$R$ = circle radius\n$\\varepsilon$ = safety margin\nThis loss is summed over all circles and trajectory points to discourage collisions.\nNote: The loss value itself does not provide guidance for avoiding collisions. It is the distance gradients that provide the guidance information.\nWhy Binary Masks Are Limiting Binary masks assume perfect knowledge of which cells are occupied or free. Real mapping systems (e.g., from LiDAR or cameras) usually output a probability of occupancy $p(y) \\in [0,1]$. If you threshold $p(y)$ to make it binary, you lose uncertainty:\nHigh threshold → unsafe (vehicle might hit uncertain obstacles).\nLow threshold → too conservative (avoiding many safe areas).\nWe need a way to create a soft distance field directly from the probability map.\nExtending to Probability Maps Probabilistic / Stochastic Distance Transform We use the log-sum-exp soft-min identity:\n$$ \\min_i x_i = \\lim_{\\alpha \\to \\infty} \\left( -\\frac{1}{\\alpha} \\ln \\sum_i e^{-\\alpha x_i} \\right) $$If we set $x_i = \\frac{ \\|c - y\\|^2 }{ 2 }$ and $\\alpha = 1 / \\sigma^2$:\n$$ d_\\text{soft}(c) = - 2 \\sigma^2 \\ln \\sum_{y} p(y) \\exp \\left( - \\frac{ \\|c-y\\|^2 }{ 2 \\sigma^2 } \\right)$$ $p(y)$ = occupancy probability at voxel $y$\n$\\sigma$ = smoothing parameter\nKey points:\nWhen $p(y)$ is binary and $\\sigma \\to 0$, the Gaussian kernel becomes a Dirac delta (see Appendix A), and $$ d_\\text{soft}(c) \\to \\min_{y:p(y)=1} \\|c-y\\|^2 $$which is exactly the squared distance used in the signed distance map.\nWhen $p(y)$ is probabilistic, $d_\\text{soft}(c)$ smoothly decreases when high-probability obstacles are near. Loss Function This probabilistic signed distance map can be plugged into the same PLUTO hinge loss:\n$$\\ell_\\text{collide}(c) = \\max(0, (R + \\varepsilon)^2 - d_\\text{soft}(c))$$and remains fully differentiable. Here, we use $(R+\\varepsilon)^2$ due to the soft distance converging to the squared minimal distance.\nPractical Considerations $\\sigma$ choice: Small $\\sigma$ ≈ sharp distance field (like ESDF) but noisy. Large $\\sigma$ ≈ smooth gradients but blurrier obstacle boundaries.\nSummation range: The sums are over all voxels $y$ in the grid, but for efficiency you only consider voxels within a radius (e.g., $3\\sigma$ for Gaussian).\nDifferentiability: Both the Gaussian-based and convolution-based methods are differentiable\nThresholding fallback: You can still threshold the probability map to get a binary map and run the normal SDF. But this discards uncertainty and isn’t recommended if you want smooth, safe behavior.\nSummary Binary occupancy map → Signed Distance Map → PLUTO hinge loss $$ \\ell_\\text{collide}(c) = \\max(0, (R + \\varepsilon)^2 - d(c)) $$ Probability map needs a soft distance field:\nProbabilistic Distance Transform: Uses log-sum-exp with Gaussian smoothing, becomes ESDF when $\\sigma\\to0$.\nKernel Convolution: Convolves occupancy probabilities with a vehicle-shaped kernel to measure expected collision.\nReduce Computational Complexity 2D CNN We start by observing the probabilistic formula\n$$ d_\\text{soft}(c) = - 2 \\sigma^2 \\ln \\sum_{y} p(y) \\exp \\left( - \\frac{ \\|c-y\\|^2 }{ 2 \\sigma^2 } \\right)$$Since the exponential term decays very fast as the distance increases. So, we can only compute the points in the vicinity of the current pixel. So, we have the approximation\n$$ d_\\text{soft}(c) = - 2 \\sigma^2 \\ln \\sum_{y\\in V_c} p(y) \\exp \\left( - \\frac{ \\|c-y\\|^2 }{ 2 \\sigma^2 } \\right), $$where we define $V_c = \\{x| \\|x-c\\| \\le r\\}$.\n1D CNN With this new approximation, it is clear that it is equivalent to Gaussian smooth. If we do 2D convolution on the 2D probabilistic map, its complexity is $WHK^2$, where $K$ is the width of the kernel.\nHowever by observing that the Gaussian smoothing is symmetrical:\n$$\\begin{aligned} d_\\text{soft}(c) \u0026= -2 \\sigma^2 \\ln \\sum_{z\\in V_c} p(z) \\exp \\left( - \\frac{ \\|c-z\\|^2 }{ 2 \\sigma^2 } \\right)\\\\ \u0026= -2 \\sigma^2 \\ln \\sum_{z\\in V_c} p(z) \\exp \\left( - \\frac{ (c_x-z_x)^2 + (c_y - z_y)^2 }{ 2 \\sigma^2 } \\right)\\\\ \u0026=-2 \\sigma^2 \\ln \\sum_{z\\in V_c} p(z) \\exp \\left( - \\frac{ (c_x-z_x)^2}{ 2 \\sigma^2 } \\right)\\exp\\left(-\\frac{(c_y-z_y)^2}{2\\sigma^2} \\right)\\\\ \u0026=-2\\sigma^2\\ln \\sum_{z_y \\in V_{c_y}}\\left( \\sum_{z_x \\in V_{c_x}} p(z_x, z_y) \\exp\\left(-\\frac{(c_x-z_x)^2}{2\\sigma^2} \\right)\\right) \\exp\\left(-\\frac{(c_y-z_y)^2}{2\\sigma^2} \\right) \\end{aligned}. $$Then the 2D CNN can be done along the x and y axis separately. Thus, its computational complexity can be reduced to $WHK$.\nPatch Method When the number of the predicted points is much smaller than the original number of pixels. One simple method is cropping the patches around the predicted pose, computing the distance map on the patch, and sampling the value.\nBinarize Probability Map Sometimes, the curb probability map contains too much noise. In such cases, it is better to use a threshold to convert the probability map to binary format. Then we can use the signed Euclidean distance map or follow the CNN method to compute the distance map. The left figure shows the distance map using the original probabilistic map. The right figure shows the distance from the binarized map. Due to noisy curbs, the probabilistic distance map is not reliable, while the binarized map provides a clear and accurate distance map.\nProbabilistic distance map Binary distance map Comparison: probabilistic vs binary distance maps Sampling Both our method and PLUTO rely on sampling the cost map. The cost itself does not provide the guidance on how to avoid the curb, the gradients of the cost map does. Here, we employ bilinear sampling method [5].\nBilinear Interpolation Suppose that we want to find the value of the unknown function $f$ at the point $(x,y)$. It is assumed that we know the value of $f$ at the four points $Q(11) = (x(1), y(1))$, $Q(12) = (x(1), y(2))$, $Q(21) = (x(2), y(1))$, and $Q(22) = (x(2), y(2))$.\nBilinear interpolation Repeated Linear Interpolation $$\\begin{aligned} f(x,y) \u0026= \\frac{y_2 - y}{y_2 - y_1} f(x, y_1) + \\frac{y - y_1}{y_2 - y_1} f(x, y_2)\\\\ \u0026= \\frac{y_2 - y}{y_2 - y_1} \\left( \\frac{x_2 - x}{x_2 - x_1} f(Q_{11}) + \\frac{x - x_1}{x_2 - x_1} f(Q_{21}) \\right) + \\frac{y - y_1}{y_2 - y_1} \\left( \\frac{x_2 - x}{x_2 - x_1} f(Q_{12}) + \\frac{x - x_1}{x_2 - x_1} f(Q_{22}) \\right)\\\\ \u0026= \\frac{1}{(x_2 - x_1)(y_2 - y_1)} \\Bigl( f(Q_{11})(x_2 - x)(y_2 - y) + f(Q_{12})(x_2 - x)(y - y_1) + f(Q_{21})(x - x_1)(y_2 - y) + f(Q_{22})(x - x_1)(y - y_1) \\Bigr)\\\\ \u0026= \\frac{1}{(x_2 - x_1)(y_2 - y_1)} \\begin{bmatrix} x_2 - x \u0026 x - x_1 \\end{bmatrix} \\begin{bmatrix} f(Q_{11}) \u0026 f(Q_{12}) \\\\ f(Q_{21}) \u0026 f(Q_{22}) \\end{bmatrix} \\begin{bmatrix} y_2 - y \\\\ y - y_1 \\end{bmatrix}. \\end{aligned}$$Polynomial Fit An alternative way is to write the solution to the interpolation problem as a multilinear polynomial\n$$ f(x, y) \\approx a_{00} + a_{10}x + a_{01}y + a_{11}xy,$$where the coefficients are found by solving the linear system\n$$ \\begin{bmatrix} 1 \u0026 x_1 \u0026 y_1 \u0026 x_1 y_1 \\\\ 1 \u0026 x_1 \u0026 y_2 \u0026 x_1 y_2 \\\\ 1 \u0026 x_2 \u0026 y_1 \u0026 x_2 y_1 \\\\ 1 \u0026 x_2 \u0026 y_2 \u0026 x_2 y_2 \\end{bmatrix} \\begin{bmatrix} a_{00} \\\\ a_{10} \\\\ a_{01} \\\\ a_{11} \\end{bmatrix} = \\begin{bmatrix} f(Q_{11}) \\\\ f(Q_{12}) \\\\ f(Q_{21}) \\\\ f(Q_{22}) \\end{bmatrix}, $$yielding the result\n$$ \\begin{bmatrix} a_{00} \\\\ a_{10} \\\\ a_{01} \\\\ a_{11} \\end{bmatrix} = \\frac{1}{(x_2 - x_1)(y_2 - y_1)} \\begin{bmatrix} x_2 y_2 \u0026 -x_2 y_1 \u0026 -x_1 y_2 \u0026 x_1 y_1 \\\\ y_2 \u0026 y_1 \u0026 y_2 \u0026 -y_1 \\\\ x_2 \u0026 x_2 \u0026 x_1 \u0026 -x_1 \\\\ 1 \u0026 -1 \u0026 -1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} f(Q_{11}) \\\\ f(Q_{12}) \\\\ f(Q_{21}) \\\\ f(Q_{22}) \\end{bmatrix}. $$For typical interpolation of images/grids, use repeated linear interpolation—it’s simpler and usually faster overall. Switch to the polynomial form only if you’ll reuse the same cell’s coefficients a lot or you specifically want closed-form derivatives.\nSample the Distance Map Next step, we are going to construct our loss function. The PLUTO paper first computes the distance transform map, then does sampling on the centers of the discs. This method is good and simple if we can compute the distance map offline. The mathematical formulation is\n$$ d(x) = \\mathcal{S}(d(y(x))) $$where $\\mathcal{S}$ is the sampling operator. In PLUTO, it uses a bilinear sampling method to get losses at the disc center.\nSample the Probability Map Directly Instead of directly sampling the distance map, we sample the probability of the map.\n$$ d(x) = - 2 \\sigma^2 \\ln \\sum_{y\\in V_y} \\mathcal{S}(p(y(x))) \\exp \\left( - \\frac{ \\|c-y\\|^2 }{ 2 \\sigma^2 } \\right). $$Further, we can predefine some grids around the current pose and pre-compute their corresponding weight,\n$$ d(x) = - 2 \\sigma^2 \\ln \\sum_{i,j} \\mathcal{S}(p(x_u + \\delta x_i, x_v + \\delta y_j))w_{i,j}, $$where $w_{i,j} = \\exp \\left( - \\frac{ \\|[\\delta x_i, \\delta y_j]\\|^2 }{ 2 \\sigma^2 } \\right)$.\nThe drawback of this method is that the original $p(y)$ may not provide enough gradient guidance. This happens particularly when $p(y)$ is almost binary.\nAnother Direct Sampling Method Ego vehicle is represented by 20×10 grid points on a 5m×2.5m rectangle (enough to cover the segmentation resolution)\nRotate and translate ego vehicle according to the positions and headings that calculated from the predicted trajectories\nMap the transformed ego vehicle onto the curbside segmentation map and sample the values of the curbside segmentation map\nAggregate the sampled values and compute the loss\nFrom BEV coordinate to image sampling coordinate From BEV coordinate to image sampling coordinate\nConvert BEV range $x\\in [-32, 32], y\\in [64, 128]$ to the sampling range that $w\\in[-1, 1], h\\in[-1, 1]$\n$$ \\begin{aligned} w \u0026= (-y+32)/(32 + 32)*2-1,\\\\ h \u0026= (-x + 128)/(128+64)*2-1 \\end{aligned} $$ From BEV coordinate to image sampling coordinate The two sampling methods are almost the same.\nChoosing Parameters for a Probabilistic Signed Distance Map In this section, we are using the 1D CNN method to create the distance map. Recall the distance map is defined as\n$$ d_\\text{soft}(c) = - 2 \\sigma^2 \\ln \\sum_{y\\in V_c} p(y) \\exp \\left( - \\frac{ \\|c-y\\|^2 }{ 2 \\sigma^2 } \\right), $$and the loss function is\n$$ \\ell_\\text{collide}(c) = \\max\\bigl(0, (R + \\varepsilon)^2 - d(c)\\bigr). $$The Kernel Size When doing CNN, we need to choose the kernel size, which corresponds to choosing $V_c$. Here we choose the points that are within $4*\\sigma$ distance of $c$, i.e., $V_c = \\{x|\\|x-c\\|\\le 4\\sigma\\}$. So, the kernel size is defined as\n$$ N_k = 2* \\max \\left(\\lceil 4*\\sigma/\\Delta\\rceil, \\lceil \\frac{R+\\varepsilon}{\\Delta}\\rceil\\right) + 1, $$where $\\Delta$ is the resolution of the probabilistic map. The term $\\lceil 4*\\sigma/\\Delta\\rceil$ is used to ensure that the kernel size is large enough to cover the points that are within $4*\\sigma$ distance of $c$. The term $\\lceil \\frac{R+\\varepsilon}{\\Delta}\\rceil$ is used to ensure that the kernel size is large enough to cover the points that are within $R+\\varepsilon$ distance of $c$.\nThe Scale and Soft Margin Parameters We have another two parameters $\\sigma$ and $\\varepsilon$. Our rule of thumb is that when there is only one point mass curb, we should detect loss when the ego vehicle is within $R+\\varepsilon$.\nAccording to inequality (see Appendix A)\n$$ m^2-2*\\sigma^2*\\log n \\le d_{\\text{soft}}(c)\\le m^2, $$where $m$ is the minimal distance to the curb. Assume the nearest curb locates at the origin so that we have\n$$\\|\\mathbf{x}\\|^2 - 2*\\sigma^2* \\log n \\le d_{\\text{soft}}(\\|\\mathbf{x}\\|^2) \\le \\|\\mathbf{x}\\|^2, $$where $d_{\\text{soft}}(\\mathbf{x}) = d_{\\text{soft}}(\\|\\mathbf{x}\\|^2)$.\nSo that the margin violation term\n$$(R+\\varepsilon)^2-\\|\\mathbf{x}\\|^2 \\le (R+\\varepsilon)^2 - d_{\\text{soft}}(\\|\\mathbf{x}\\|^2)\\le (R+\\varepsilon)^2+\\tau\\log n -\\|\\mathbf{x}\\|^2, $$Remember $d_{\\text{soft}}$ can be negative values. Let us draw the hinge loss function of $\\|\\mathbf{x}\\|^2$ as below:\nPython Code: Hinge Loss Visualization (Click to expand) # Fix the plot to ensure there's only ONE y-axis (no duplicate vertical lines) # and make sure the y-axis line itself serves as the main vertical axis. import numpy as np import matplotlib.pyplot as plt # Define parameters again R = 2.0 eps = 0.5 tau = 1.0 n = 10 # Define x values (squared norm) x_squared = np.linspace(-1, (R+eps)**2 + tau*np.log(n) + 1, 400) # Define left and right functions left_func = (R+eps)**2 - x_squared right_func = (R+eps)**2 + tau*np.log(n) - x_squared # Apply hinge loss left_hinge = np.maximum(0, left_func) right_hinge = np.maximum(0, right_func) # Start plotting fig, ax = plt.subplots(figsize=(8,5)) # Plot hinge lines ax.plot(x_squared, left_hinge, label=r'Hinge[$(R+\\varepsilon)^2 - \\|x\\|^2$]', linewidth=2) ax.plot(x_squared, right_hinge, label=r'Hinge[$(R+\\varepsilon)^2 + \\tau \\log n - \\|x\\|^2$]', linewidth=2, linestyle='--') # Shade gap and guaranteed loss areas ax.fill_between(x_squared, left_hinge, right_hinge, color='orange', alpha=0.3, label='Curb-dependent loss') ax.fill_between(x_squared, 0, left_hinge, color='skyblue', alpha=0.4, label='Guaranteed loss') # Remove ticks ax.set_xticks([]) ax.set_yticks([]) # Ensure equal axis scaling ax.set_aspect('equal') # Move left spine (y-axis) to x=0 (the origin) ax.spines['left'].set_position('zero') # Hide top and right spines ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) # Make bottom spine visible and set its position ax.spines['bottom'].set_position(('data',0)) # Label the origin \"0\" ax.text(0, 0, \"0\", fontsize=12, ha='right', va='top', color='black', fontweight='bold') # Intersection points left_intersection = (R+eps)**2 right_intersection = (R+eps)**2 + tau*np.log(n) # Labels at intersection points ax.text(left_intersection, 0, r'$(R+\\varepsilon)^2$', fontsize=12, ha='center', va='top', color='blue', fontweight='bold') ax.text(right_intersection, 0, r'$(R+\\varepsilon)^2 + \\tau \\log n$', fontsize=12, ha='center', va='top', color='orange', fontweight='bold') # Labels ax.set_xlabel(r'$\\|x\\|^2$') ax.set_ylabel('Hinge loss value') ax.legend() ax.grid(False) plt.tight_layout() plt.show() Hinge loss bounds The blue area is the loss that is absolutely guaranteed. The yellow gap will depend on the parameters and the distribution of the probabilistic map.\nSo, the real soft margin is at $(R+\\varepsilon)^2+2*\\sigma^2*\\log n$. If we set $\\varepsilon=0$, the soft margin gap is $2*\\sigma^2\\log n$\nFor example, if we want the soft margin gap to be $1m$. In our case, valid $n$ is approximately $3^2$. So, that $2*\\sigma^2*\\log 3^2 =4*\\sigma^2*\\log 3 \\approx 4.4*\\sigma^2 =1$. Then, we can set $\\sigma=0.47$.\nHinge loss bounds Hinge loss bounds zoomed in Hinge loss bound. A more precise estimation of the loss upper bound can be found in Appendix B.\nAppendix Appendix A Log–Sum–Exp “soft‑min” identity For real numbers $x_1,\\dots,x_n$ and a positive “temperature” (or smoothing) parameter $\\tau \u003e 0$:\n$$ \\text{softmin}_{\\tau}(x_1,\\dots,x_n) = −τ \\log⁡ ⁣\\sum_{i=1}^n e^{-\\frac{x_i}{\\tau}} $$As $\\tau \\to 0^+$,\n$$ \\text{softmin}_{\\tau}(x_1, x_2, \\dots, x_n) = \\min_i x_i $$Why does it converge to min⁡?\nLet $m = \\min_i x_i$. Rewrite the sum by factoring out the smallest term:\n$$ \\sum_{i=1}^n e^{-\\frac{x_i}{\\tau}} = e^{-\\frac{m}{\\tau}}\\sum_{i=1}^n e^{-\\frac{x_i - m}{\\tau}} $$Plug into the soft‑min:\n$$ -\\tau\\log\\sum_{i=1}^n e^{-\\frac{x_i}{\\tau}} = m -\\tau \\log\\sum_{i=1}^n e^{-\\frac{x_i -m}{\\tau}} $$All terms inside the log are $\\ge 1$ (because one of them is $e^0=1$). Thus,\n$$ m-\\tau \\log n \\le -\\tau\\log\\sum_{i=1}^ne^{-\\frac{x_i}{\\tau}} \\le m $$As $\\tau \\to 0$, the left term equals $m$, so that the log sum equals to the minimum.\nIntuition\nLog–sum–exp is a smooth version of “min” (or “max”) The exponential makes smaller $x_i$ dominate the sum; the log and $-\\tau$ rescale back to the original domain.\nTemperature $\\tau$ controls smoothness.\nSmall $\\tau$ → very sharp, almost the true minimum.\nLarge $\\tau$ → averages more of the terms (smoother).\nThe gradient (useful in optimization) of the softmin function is\nDefine $f = -\\tau \\log \\sum_{i=1}^n e^{-\\frac{x_i}{\\tau}}$,\nthen\n$$\\frac{\\partial f}{\\partial x_i} = \\frac{e^{-\\frac{x_i}{\\tau}}}{\\sum_{i=1}^n e^{-\\frac{x_i}{\\tau}}} = \\text{softmax}(-\\frac{x_i}{\\tau}), $$Interpretation:\nThe gradient weights are a soft assignment to the argmin.\nAs $\\tau\\to 0$, the gradient becomes a one‑hot vector at the actual minimum index.\nNumerical stability (log‑sum‑exp trick) To avoid overflow/underflow, subtract the smallest value (or any reference):\nm = x.min() softmin = -tau * torch.log(torch.exp(-(x - m)/tau).sum()) + m This is equivalent to what we did algebraically.\nThere is also continuous version of the softmin function.\nFor a continuous function $f(y)$:\n$$ \\text{softmin}_{\\tau} f = -\\tau \\log e^{-f(y)/\\tau} dy, $$As $\\tau \\to 0$ it converges to $\\inf_y f(y)$\n(assuming the integral is well-defined). This is the Laplace approximation principle.\nMax version of soft function is\n$$ \\text{softmax}_{\\tau}(x) = \\tau \\log\\sum_i^n e^{x_i/\\tau} $$As $\\tau\\to 0$, $\\lim_{\\tau \\to 0} \\text{ softmax}_{\\tau}(x)=\\max_i\\{x_i\\}$.\nAppendix B Refined Lower Bound of the Soft Distance Recall our soft distance map is defined as\n$$d_{\\text{soft}}(c) = -2\\sigma^2\\log\\sum_y f(y)e^{-\\frac{\\|c-y\\|^2}{2\\sigma^2}}$$Assume $y_{\\min}$ is with the minimal distance to $c$ and $f(y_{\\min})=1$. Then we have\n$$\\begin{aligned} d_{\\text{soft}}(c) \u0026= -2\\sigma^2\\log\\sum_y f(y)e^{-\\frac{\\|c-y\\|^2}{2\\sigma^2}}\\\\ \u0026=\\|c-y_{\\min}\\|^2 - 2\\sigma^2\\log\\sum_{y}e^{-\\frac{\\|c-y\\|^2 - \\|c-y_{\\min}\\|^2}{2\\sigma^2}})\\\\ \u0026\\ge \\|c-y_{\\min}\\|^2 - 2\\sigma^2\\log\\sum_{i=-K,\\dots, K, j=-K, \\dots, K}e^{-\\frac{(i\\Delta)^2+(j\\Delta)^2}{2\\sigma^2}} \\end{aligned} $$where $K$ is the half of the kernel size and $\\Delta$ is the resolution of the map.\nThen we need to estimate the summation term. Let us define\n$$S(K,\\Delta, \\sigma) = \\sum_{i=-K}^K\\sum_{j=-K}^K e^{-\\frac{(i\\Delta)^2+(j\\Delta)^2}{2\\sigma^2}}$$We can approximate this with a Gaussian integration on condition that $K$ is large and $\\frac{\\Delta}{\\sigma}$ is small. So we have\n$$ S(K, \\Delta, \\sigma) \\approx \\frac{2\\pi\\sigma^2}{\\Delta^2} $$Thus, the distance is lower bounded by\n$$ d_{\\text{soft}}(c) \\ge \\|c-y_{\\min}\\|^2 - 2\\sigma^2\\log\\frac{2\\pi\\sigma^2}{\\Delta^2} $$If we want a soft distance to be $0.3$, then by solving $2\\sigma^2\\log\\frac{2\\pi\\sigma^2}{\\Delta^2} = 0.3$ and $\\Delta=0.4$, we get $\\sigma\\approx0.32$.\nAppendix C Algorithm for Distance Transform Please refer to [1]. The computational complexity is $\\mathcal{O}(WH)$.\nReferences [1] P. F. Felzenszwalb and D. P. Huttenlocher, “Distance transforms of sampled functions,” Theory of Computing, vol. 8, no. 1, pp. 415–428, Sep. 2012, doi:10.4086/toc.2012.v008a019\n[2] J. Cheng, Y. Chen, and Q. Chen, “PLUTO: Pushing the Limit of Imitation Learning‑based Planning for Autonomous Driving,” arXiv preprint arXiv:2404.14327, Apr. 22, 2024.\n[3] A. Meijster, J. B. T. M. Roerdink, and W. H. Hesselink, “A general algorithm for computing distance transforms in linear time,” in Mathematical Morphology and its Applications to Image and Signal Processing, J. Goutsias, L. Vincent, and D. S. Bloomberg, Eds. Dordrecht, The Netherlands: Kluwer Academic, 2000, pp. 331–340. https://pure.rug.nl/ws/files/14632734/c2.pdf\n[4] T. Strutz, “The Distance Transform and its Computation — An Introduction,” arXiv, vol. arXiv:2106.03503 v2, Feb. 24, 2023, arXiv Working Paper TECHP/2021/06. https://arxiv.org/pdf/2106.03503\n[5] “Bilinear interpolation,” Wikipedia, The Free Encyclopedia, last modified August 6, 2025. [Online]. Available: https://en.wikipedia.org/wiki/Bilinear_interpolation\n","wordCount":"3118","inLanguage":"en","image":"https://livey.github.io/posts/2025-08-13-collission-loss/%3Cimage%20path/url%3E","datePublished":"2025-08-14T00:00:00Z","dateModified":"2025-08-14T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-08-13-collission-loss/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving</h1><div class=post-description>Collision loss design in end-to-end autonomous driving model training.</div><div class=post-meta><span title='2025-08-14 00:00:00 +0000 UTC'>August 14, 2025</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3118 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#using-signed-distance-map-with-binary-occupancy-map aria-label="Using Signed Distance Map with Binary Occupancy Map">Using Signed Distance Map with Binary Occupancy Map</a><ul><li><a href=#how-pluto--builds-the-loss-map-2 aria-label="How PLUTO  Builds the Loss Map [2]">How PLUTO Builds the Loss Map [2]</a><ul><li><a href=#vehicle-model aria-label="Vehicle Model">Vehicle Model</a></li><li><a href=#distance-map aria-label="Distance Map">Distance Map</a></li><li><a href=#loss-formulation aria-label="Loss Formulation">Loss Formulation</a></li></ul></li><li><a href=#why-binary-masks-are-limiting aria-label="Why Binary Masks Are Limiting">Why Binary Masks Are Limiting</a></li></ul></li><li><a href=#extending-to-probability-maps aria-label="Extending to Probability Maps">Extending to Probability Maps</a><ul><li><a href=#probabilistic--stochastic-distance-transform aria-label="Probabilistic / Stochastic Distance Transform">Probabilistic / Stochastic Distance Transform</a></li><li><a href=#loss-function aria-label="Loss Function">Loss Function</a></li><li><a href=#practical-considerations aria-label="Practical Considerations">Practical Considerations</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#reduce-computational-complexity aria-label="Reduce Computational Complexity">Reduce Computational Complexity</a><ul><li><a href=#2d-cnn aria-label="2D CNN">2D CNN</a></li><li><a href=#1d-cnn aria-label="1D CNN">1D CNN</a></li><li><a href=#patch-method aria-label="Patch Method">Patch Method</a></li></ul></li><li><a href=#binarize-probability-map aria-label="Binarize Probability Map">Binarize Probability Map</a></li><li><a href=#sampling aria-label=Sampling>Sampling</a><ul><li><a href=#bilinear-interpolation aria-label="Bilinear Interpolation">Bilinear Interpolation</a><ul><li><a href=#repeated-linear-interpolation aria-label="Repeated Linear Interpolation">Repeated Linear Interpolation</a></li><li><a href=#polynomial-fit aria-label="Polynomial Fit">Polynomial Fit</a></li></ul></li><li><a href=#sample-the-distance-map aria-label="Sample the Distance Map">Sample the Distance Map</a><ul><li><a href=#sample-the-probability-map-directly aria-label="Sample the Probability Map Directly">Sample the Probability Map Directly</a></li><li><a href=#another-direct-sampling-method aria-label="Another Direct Sampling Method">Another Direct Sampling Method</a></li></ul></li></ul></li></ul></li><li><a href=#choosing-parameters-for-a-probabilistic-signed-distance-map aria-label="Choosing Parameters for a Probabilistic Signed Distance Map">Choosing Parameters for a Probabilistic Signed Distance Map</a><ul><li><a href=#the-kernel-size aria-label="The Kernel Size">The Kernel Size</a></li><li><a href=#the-scale-and-soft-margin-parameters aria-label="The Scale and Soft Margin Parameters">The Scale and Soft Margin Parameters</a></li></ul></li><li><a href=#appendix aria-label=Appendix>Appendix</a><ul><li><a href=#appendix-a aria-label="Appendix A">Appendix A</a><ul><li><a href=#logsumexp-softmin-identity aria-label="Log–Sum–Exp “soft‑min” identity">Log–Sum–Exp “soft‑min” identity</a></li></ul></li><li><a href=#appendix-b aria-label="Appendix B">Appendix B</a><ul><li><a href=#refined-lower-bound-of-the-soft-distance aria-label="Refined Lower Bound of the Soft Distance">Refined Lower Bound of the Soft Distance</a></li></ul></li><li><a href=#appendix-c aria-label="Appendix C">Appendix C</a><ul><li><a href=#algorithm-for-distance-transform aria-label="Algorithm for Distance Transform">Algorithm for Distance Transform</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>This post explores collision loss design for end-to-end autonomous driving training, focusing on extending PLUTO&rsquo;s binary occupancy map approach to handle probabilistic maps. The method enables safer autonomous driving by providing smooth, uncertainty-aware collision avoidance while maintaining computational efficiency.</p><h1 id=using-signed-distance-map-with-binary-occupancy-map>Using Signed Distance Map with Binary Occupancy Map<a hidden class=anchor aria-hidden=true href=#using-signed-distance-map-with-binary-occupancy-map>#</a></h1><h2 id=how-pluto--builds-the-loss-map-2>How PLUTO Builds the Loss Map [2]<a hidden class=anchor aria-hidden=true href=#how-pluto--builds-the-loss-map-2>#</a></h2><h3 id=vehicle-model>Vehicle Model<a hidden class=anchor aria-hidden=true href=#vehicle-model>#</a></h3><p>In PLUTO and other planning algorithms, the vehicle is modelded as a series of overlapping discs.</p><figure style=text-align:center><img src=./resources/diagram.png alt="Vehicle model with covering discs" style="width:40%;margin:0 auto;display:block"><figcaption style=font-weight:400>Vehicle model covered by overlapping discs (as used in PLUTO)</figcaption></figure><h3 id=distance-map>Distance Map<a hidden class=anchor aria-hidden=true href=#distance-map>#</a></h3><ul><li><p>The PLUTO planner for imitation learning-based planning uses <strong>auxiliary losses</strong> for safety, including a collision loss.</p></li><li><p>PLUTO starts with a <strong>binary occupancy map</strong> (voxel grid):</p><ul><li><p><code>1</code> = obstacle or off-road cell</p></li><li><p><code>0</code> = free/drivable cell</p></li></ul></li><li><p>From this map, PLUTO computes an Euclidean Signed Distance Field (ESDF) [1]:</p><ul><li><p>Each cell stores the shortest Euclidean distance to the nearest obstacle.</p></li><li><p>Positive distances indicate free space, zero indicates contact with an obstacle.</p></li></ul><p>The distance map $D(f(p))$ is defined as:</p>$$
D f (p) = \min_{q\in G}\left (d(p, q) + f (q)\right)
$$<p>Many algorithms for computing the distance transform of point sets use the equivalent definition</p>$$
D(p) = \min_{q\in G}\left( d(p, q) + 1(q)\right)
$$<p>where $1(q)$ is an indicator function for membership in $P$:</p>$$
1(q) =
\begin{cases}
0, \text{ if } q\in P \\
\infin, \text{ otherwise }
\end{cases}
$$</li></ul><p>The distance map can be computed with linear computational complexity [1]. Below shows an example of the computed signed Euclidean distance map.</p><figure style=text-align:center><img src=./resources/image.png alt="Distance map example" style="width:70%;margin:0 auto;display:block"><figcaption style=font-weight:400>Example of computed signed Euclidean distance map</figcaption></figure><h3 id=loss-formulation>Loss Formulation<a hidden class=anchor aria-hidden=true href=#loss-formulation>#</a></h3><p>For each predicted trajectory point, PLUTO covers the vehicle with several covering circles (each circle has a center $c$ and radius $R$).</p><ol><li><p>The circle&rsquo;s center is projected into the signed distance map.</p></li><li><p>The signed distance $d(c)$ is obtained via bilinear interpolation.</p></li><li><p>A hinge loss penalizes intrusions into obstacles:</p></li></ol>$$
\ell_\text{collide}(c) = \max\bigl(0, R + \varepsilon - d(c)\bigr)
$$<p>where:</p><ul><li><p>$R$ = circle radius</p></li><li><p>$\varepsilon$ = safety margin</p></li></ul><p>This loss is summed over all circles and trajectory points to discourage collisions.</p><p><strong>Note:</strong> The loss value itself does not provide guidance for avoiding collisions. It is the distance gradients that provide the guidance information.</p><h2 id=why-binary-masks-are-limiting>Why Binary Masks Are Limiting<a hidden class=anchor aria-hidden=true href=#why-binary-masks-are-limiting>#</a></h2><p>Binary masks assume perfect knowledge of which cells are occupied or free.
Real mapping systems (e.g., from LiDAR or cameras) usually output a probability of occupancy $p(y) \in [0,1]$.
If you threshold $p(y)$ to make it binary, you lose uncertainty:</p><ul><li><p>High threshold → unsafe (vehicle might hit uncertain obstacles).</p></li><li><p>Low threshold → too conservative (avoiding many safe areas).</p></li></ul><p>We need a way to create a soft distance field directly from the probability map.</p><h1 id=extending-to-probability-maps>Extending to Probability Maps<a hidden class=anchor aria-hidden=true href=#extending-to-probability-maps>#</a></h1><h2 id=probabilistic--stochastic-distance-transform>Probabilistic / Stochastic Distance Transform<a hidden class=anchor aria-hidden=true href=#probabilistic--stochastic-distance-transform>#</a></h2><p>We use the <strong>log-sum-exp soft-min identity</strong>:</p>$$
\min_i x_i = \lim_{\alpha \to \infty} \left( -\frac{1}{\alpha} \ln \sum_i e^{-\alpha x_i} \right)
$$<p>If we set $x_i = \frac{ \|c - y\|^2 }{ 2 }$ and $\alpha = 1 / \sigma^2$:</p>$$
d_\text{soft}(c) = - 2 \sigma^2 \ln \sum_{y} p(y) \exp \left( - \frac{ \|c-y\|^2 }{ 2 \sigma^2 } \right)$$<ul><li><p>$p(y)$ = occupancy probability at voxel $y$</p></li><li><p>$\sigma$ = smoothing parameter</p></li></ul><p><strong>Key points:</strong></p><ul><li>When $p(y)$ is binary and $\sigma \to 0$, the Gaussian kernel becomes a Dirac delta (see Appendix A), and</li></ul>$$
d_\text{soft}(c) \to \min_{y:p(y)=1} \|c-y\|^2
$$<p>which is exactly the squared distance used in the signed distance map.</p><ul><li>When $p(y)$ is probabilistic, $d_\text{soft}(c)$ smoothly decreases when high-probability obstacles are near.</li></ul><h2 id=loss-function>Loss Function<a hidden class=anchor aria-hidden=true href=#loss-function>#</a></h2><p>This probabilistic signed distance map can be plugged into the same PLUTO hinge loss:</p>$$\ell_\text{collide}(c) = \max(0, (R + \varepsilon)^2 - d_\text{soft}(c))$$<p>and remains fully differentiable. Here, we use $(R+\varepsilon)^2$ due to the soft distance converging to the squared minimal distance.</p><h2 id=practical-considerations>Practical Considerations<a hidden class=anchor aria-hidden=true href=#practical-considerations>#</a></h2><ul><li><p>$\sigma$ choice: Small $\sigma$ ≈ sharp distance field (like ESDF) but noisy. Large $\sigma$ ≈ smooth gradients but blurrier obstacle boundaries.</p></li><li><p>Summation range: The sums are over all voxels $y$ in the grid, but for efficiency you only consider voxels within a radius (e.g., $3\sigma$ for Gaussian).</p></li><li><p>Differentiability: Both the Gaussian-based and convolution-based methods are differentiable</p></li><li><p>Thresholding fallback: You <em>can</em> still threshold the probability map to get a binary map and run the normal SDF. But this discards uncertainty and isn’t recommended if you want smooth, safe behavior.</p></li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li>Binary occupancy map → Signed Distance Map → PLUTO hinge loss</li></ul>$$
\ell_\text{collide}(c) = \max(0, (R + \varepsilon)^2 - d(c))
$$<ul><li><p>Probability map needs a soft distance field:</p><ul><li><p>Probabilistic Distance Transform: Uses log-sum-exp with Gaussian smoothing, becomes ESDF when $\sigma\to0$.</p></li><li><p>Kernel Convolution: Convolves occupancy probabilities with a vehicle-shaped kernel to measure expected collision.</p></li></ul></li></ul><h2 id=reduce-computational-complexity>Reduce Computational Complexity<a hidden class=anchor aria-hidden=true href=#reduce-computational-complexity>#</a></h2><h3 id=2d-cnn>2D CNN<a hidden class=anchor aria-hidden=true href=#2d-cnn>#</a></h3><p>We start by observing the probabilistic formula</p>$$
d_\text{soft}(c) = - 2 \sigma^2 \ln \sum_{y} p(y) \exp \left( - \frac{ \|c-y\|^2 }{ 2 \sigma^2 } \right)$$<p>Since the exponential term decays very fast as the distance increases. So, we can only compute the points in the vicinity of the current pixel. So, we have the approximation</p>$$
d_\text{soft}(c) = - 2 \sigma^2 \ln \sum_{y\in V_c} p(y) \exp \left( - \frac{ \|c-y\|^2 }{ 2 \sigma^2 } \right),
$$<p>where we define $V_c = \{x| \|x-c\| \le r\}$.</p><h3 id=1d-cnn>1D CNN<a hidden class=anchor aria-hidden=true href=#1d-cnn>#</a></h3><p>With this new approximation, it is clear that it is equivalent to Gaussian smooth. If we do 2D convolution on the 2D probabilistic map, its complexity is $WHK^2$, where $K$ is the width of the kernel.</p><p>However by observing that
the Gaussian smoothing is symmetrical:</p>$$\begin{aligned}
d_\text{soft}(c)
&= -2 \sigma^2 \ln \sum_{z\in V_c} p(z) \exp \left( - \frac{ \|c-z\|^2 }{ 2 \sigma^2 } \right)\\
&= -2 \sigma^2 \ln \sum_{z\in V_c} p(z) \exp \left( - \frac{ (c_x-z_x)^2 + (c_y - z_y)^2 }{ 2 \sigma^2 } \right)\\
&=-2 \sigma^2 \ln \sum_{z\in V_c} p(z) \exp \left( - \frac{ (c_x-z_x)^2}{ 2 \sigma^2 } \right)\exp\left(-\frac{(c_y-z_y)^2}{2\sigma^2} \right)\\
&=-2\sigma^2\ln \sum_{z_y \in V_{c_y}}\left( \sum_{z_x \in V_{c_x}} p(z_x, z_y) \exp\left(-\frac{(c_x-z_x)^2}{2\sigma^2} \right)\right) \exp\left(-\frac{(c_y-z_y)^2}{2\sigma^2} \right)
\end{aligned}.
$$<p>Then the 2D CNN can be done along the x and y axis separately. Thus, its computational complexity can be reduced to $WHK$.</p><h3 id=patch-method>Patch Method<a hidden class=anchor aria-hidden=true href=#patch-method>#</a></h3><p>When the number of the predicted points is much smaller than the original number of pixels. One simple method is cropping the patches around the predicted pose, computing the distance map on the patch, and sampling the value.</p><h2 id=binarize-probability-map>Binarize Probability Map<a hidden class=anchor aria-hidden=true href=#binarize-probability-map>#</a></h2><p>Sometimes, the curb probability map contains too much noise. In such cases, it is better to use a threshold to convert the probability map to binary format. Then we can use the signed Euclidean distance map or follow the CNN method to compute the distance map. The left figure shows the distance map using the original probabilistic map. The right figure shows the distance from the binarized map. Due to noisy curbs, the probabilistic distance map is not reliable, while the binarized map provides a clear and accurate distance map.</p><figure style=text-align:center><div style=display:flex;justify-content:center;gap:20px;align-items:center><div style=text-align:center><img src=./resources/image-1.png alt="Using the original probability map to construct the distance map." style=width:100%;max-width:350px><figcaption style=font-weight:400;font-size:.9em>Probabilistic distance map</figcaption></div><div style=text-align:center><img src=./resources/image-2.png alt="Using binary map to construct the distance map." style=width:100%;max-width:350px><figcaption style=font-weight:400;font-size:.9em>Binary distance map</figcaption></div></div><figcaption style=font-weight:400;margin-top:10px>Comparison: probabilistic vs binary distance maps</figcaption></figure><h2 id=sampling>Sampling<a hidden class=anchor aria-hidden=true href=#sampling>#</a></h2><p>Both our method and PLUTO rely on sampling the cost map. The cost itself does not provide the guidance on how to avoid the curb, the gradients of the cost map does. Here, we employ bilinear sampling method [5].</p><h3 id=bilinear-interpolation>Bilinear Interpolation<a hidden class=anchor aria-hidden=true href=#bilinear-interpolation>#</a></h3><p>Suppose that we want to find the value of the unknown function $f$ at the point $(x,y)$. It is assumed that we know the value of $f$ at the four points $Q(11) = (x(1), y(1))$, $Q(12) = (x(1), y(2))$, $Q(21) = (x(2), y(1))$, and $Q(22) = (x(2), y(2))$.</p><figure style=text-align:center><img src=./resources/image-3.png alt="Bilinear interpolation" style="width:70%;margin:0 auto;display:block"><figcaption style=font-weight:400>Bilinear interpolation</figcaption></figure><h4 id=repeated-linear-interpolation>Repeated Linear Interpolation<a hidden class=anchor aria-hidden=true href=#repeated-linear-interpolation>#</a></h4>$$\begin{aligned}
f(x,y) &= \frac{y_2 - y}{y_2 - y_1} f(x, y_1) + \frac{y - y_1}{y_2 - y_1} f(x, y_2)\\
&= \frac{y_2 - y}{y_2 - y_1} \left( \frac{x_2 - x}{x_2 - x_1} f(Q_{11}) + \frac{x - x_1}{x_2 - x_1} f(Q_{21}) \right) + \frac{y - y_1}{y_2 - y_1} \left( \frac{x_2 - x}{x_2 - x_1} f(Q_{12}) + \frac{x - x_1}{x_2 - x_1} f(Q_{22}) \right)\\
&= \frac{1}{(x_2 - x_1)(y_2 - y_1)}
\Bigl( f(Q_{11})(x_2 - x)(y_2 - y) + f(Q_{12})(x_2 - x)(y - y_1) + f(Q_{21})(x - x_1)(y_2 - y) + f(Q_{22})(x - x_1)(y - y_1) \Bigr)\\
&= \frac{1}{(x_2 - x_1)(y_2 - y_1)}
\begin{bmatrix} x_2 - x & x - x_1 \end{bmatrix}
\begin{bmatrix} f(Q_{11}) & f(Q_{12}) \\ f(Q_{21}) & f(Q_{22}) \end{bmatrix}
\begin{bmatrix} y_2 - y \\ y - y_1 \end{bmatrix}.
\end{aligned}$$<h4 id=polynomial-fit>Polynomial Fit<a hidden class=anchor aria-hidden=true href=#polynomial-fit>#</a></h4><p>An alternative way is to write the solution to the interpolation problem as a multilinear polynomial</p>$$
f(x, y) \approx a_{00} + a_{10}x + a_{01}y + a_{11}xy,$$<p>where the coefficients are found by solving the linear system</p>$$
\begin{bmatrix}
1 & x_1 & y_1 & x_1 y_1 \\
1 & x_1 & y_2 & x_1 y_2 \\
1 & x_2 & y_1 & x_2 y_1 \\
1 & x_2 & y_2 & x_2 y_2
\end{bmatrix}
\begin{bmatrix}
a_{00} \\
a_{10} \\
a_{01} \\
a_{11}
\end{bmatrix} =
\begin{bmatrix}
f(Q_{11}) \\
f(Q_{12}) \\
f(Q_{21}) \\
f(Q_{22})
\end{bmatrix},
$$<p>yielding the result</p>$$
\begin{bmatrix}
a_{00} \\
a_{10} \\
a_{01} \\
a_{11}
\end{bmatrix} =
\frac{1}{(x_2 - x_1)(y_2 - y_1)}
\begin{bmatrix}
x_2 y_2 & -x_2 y_1 & -x_1 y_2 & x_1 y_1 \\
y_2 & y_1 & y_2 & -y_1 \\
x_2 & x_2 & x_1 & -x_1 \\
1 & -1 & -1 & 1
\end{bmatrix}
\begin{bmatrix}
f(Q_{11}) \\
f(Q_{12}) \\
f(Q_{21}) \\
f(Q_{22})
\end{bmatrix}.
$$<p>For typical interpolation of images/grids, use repeated linear interpolation—it’s simpler and usually faster overall. Switch to the polynomial form only if you’ll reuse the same cell’s coefficients a lot or you specifically want closed-form derivatives.</p><h3 id=sample-the-distance-map>Sample the Distance Map<a hidden class=anchor aria-hidden=true href=#sample-the-distance-map>#</a></h3><p>Next step, we are going to construct our loss function. The PLUTO paper first computes the distance transform map, then does sampling on the centers of the discs. This method is good and simple if we can compute the distance map offline. The mathematical formulation is</p>$$
d(x) = \mathcal{S}(d(y(x)))
$$<p>where $\mathcal{S}$ is the sampling operator. In PLUTO, it uses a bilinear sampling method to get losses at the disc center.</p><h4 id=sample-the-probability-map-directly>Sample the Probability Map Directly<a hidden class=anchor aria-hidden=true href=#sample-the-probability-map-directly>#</a></h4><p>Instead of directly sampling the distance map, we sample the probability of the map.</p>$$
d(x) = - 2 \sigma^2 \ln \sum_{y\in V_y} \mathcal{S}(p(y(x))) \exp \left( - \frac{ \|c-y\|^2 }{ 2 \sigma^2 } \right).
$$<p>Further, we can predefine some grids around the current pose and pre-compute their corresponding weight,</p>$$
d(x) = - 2 \sigma^2 \ln \sum_{i,j} \mathcal{S}(p(x_u + \delta x_i, x_v + \delta y_j))w_{i,j},
$$<p>where $w_{i,j} = \exp \left( - \frac{ \|[\delta x_i, \delta y_j]\|^2 }{ 2 \sigma^2 } \right)$.</p><p>The drawback of this method is that the original $p(y)$ may not provide enough gradient guidance. This happens particularly when $p(y)$ is almost binary.</p><h4 id=another-direct-sampling-method>Another Direct Sampling Method<a hidden class=anchor aria-hidden=true href=#another-direct-sampling-method>#</a></h4><ol><li><p>Ego vehicle is represented by 20×10 grid points on a 5m×2.5m rectangle (enough to cover the segmentation resolution)</p></li><li><p>Rotate and translate ego vehicle according to the positions and headings that calculated from the predicted trajectories</p></li><li><p>Map the transformed ego vehicle onto the curbside segmentation map and sample the values of the curbside segmentation map</p></li><li><p>Aggregate the sampled values and compute the loss</p></li></ol><figure style=text-align:center><img src=./resources/diagram-1.png alt="From BEV coordinate to image sampling coordinate" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>From BEV coordinate to image sampling coordinate</figcaption></figure><p>From BEV coordinate to image sampling coordinate</p><p>Convert BEV range $x\in [-32, 32], y\in [64, 128]$ to the sampling range that $w\in[-1, 1], h\in[-1, 1]$</p>$$
\begin{aligned}
w &= (-y+32)/(32 + 32)*2-1,\\
h &= (-x + 128)/(128+64)*2-1
\end{aligned}
$$<figure style=text-align:center><img src=./resources/diagram-2.png alt="From BEV coordinate to image sampling coordinate" style="width:100%;margin:0 auto;display:block"><figcaption style=font-weight:400>From BEV coordinate to image sampling coordinate</figcaption></figure><p>The two sampling methods are almost the same.</p><h1 id=choosing-parameters-for-a-probabilistic-signed-distance-map>Choosing Parameters for a Probabilistic Signed Distance Map<a hidden class=anchor aria-hidden=true href=#choosing-parameters-for-a-probabilistic-signed-distance-map>#</a></h1><p>In this section, we are using the 1D CNN method to create the distance map. Recall the distance map is defined as</p>$$
d_\text{soft}(c) = - 2 \sigma^2 \ln \sum_{y\in V_c} p(y) \exp \left( - \frac{ \|c-y\|^2 }{ 2 \sigma^2 } \right),
$$<p>and the loss function is</p>$$
\ell_\text{collide}(c) = \max\bigl(0, (R + \varepsilon)^2 - d(c)\bigr).
$$<h2 id=the-kernel-size>The Kernel Size<a hidden class=anchor aria-hidden=true href=#the-kernel-size>#</a></h2><p>When doing CNN, we need to choose the kernel size, which corresponds to choosing $V_c$.
Here we choose the points that are within $4*\sigma$ distance of $c$,
i.e., $V_c = \{x|\|x-c\|\le 4\sigma\}$. So, the kernel size is defined as</p>$$
N_k = 2* \max \left(\lceil 4*\sigma/\Delta\rceil, \lceil \frac{R+\varepsilon}{\Delta}\rceil\right) + 1,
$$<p>where $\Delta$ is the resolution of the probabilistic map. The term $\lceil 4*\sigma/\Delta\rceil$ is used to ensure that the kernel size is large enough to cover the points that are within $4*\sigma$ distance of $c$. The term $\lceil \frac{R+\varepsilon}{\Delta}\rceil$ is used to ensure that the kernel size is large enough to cover the points that are within $R+\varepsilon$ distance of $c$.</p><h2 id=the-scale-and-soft-margin-parameters>The Scale and Soft Margin Parameters<a hidden class=anchor aria-hidden=true href=#the-scale-and-soft-margin-parameters>#</a></h2><p>We have another two parameters $\sigma$ and $\varepsilon$. Our rule of thumb is that when there is only one point mass curb, we should detect loss when the ego vehicle is within $R+\varepsilon$.</p><p>According to inequality (see Appendix A)</p>$$
m^2-2*\sigma^2*\log n \le d_{\text{soft}}(c)\le m^2,
$$<p>where $m$ is the minimal distance to the curb. Assume the nearest curb locates at the origin so that we have</p>$$\|\mathbf{x}\|^2 - 2*\sigma^2* \log n \le d_{\text{soft}}(\|\mathbf{x}\|^2) \le \|\mathbf{x}\|^2,
$$<p>where $d_{\text{soft}}(\mathbf{x}) = d_{\text{soft}}(\|\mathbf{x}\|^2)$.</p><p>So that the margin violation term</p>$$(R+\varepsilon)^2-\|\mathbf{x}\|^2 \le (R+\varepsilon)^2 - d_{\text{soft}}(\|\mathbf{x}\|^2)\le (R+\varepsilon)^2+\tau\log n -\|\mathbf{x}\|^2,
$$<p>Remember $d_{\text{soft}}$ can be negative values. Let us draw the hinge loss function of $\|\mathbf{x}\|^2$ as below:</p><details><summary><strong>Python Code: Hinge Loss Visualization</strong> (Click to expand)</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Fix the plot to ensure there&#39;s only ONE y-axis (no duplicate vertical lines)</span>
</span></span><span class=line><span class=cl><span class=c1># and make sure the y-axis line itself serves as the main vertical axis.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define parameters again</span>
</span></span><span class=line><span class=cl><span class=n>R</span> <span class=o>=</span> <span class=mf>2.0</span>
</span></span><span class=line><span class=cl><span class=n>eps</span> <span class=o>=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl><span class=n>tau</span> <span class=o>=</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define x values (squared norm)</span>
</span></span><span class=line><span class=cl><span class=n>x_squared</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>R</span><span class=o>+</span><span class=n>eps</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=n>tau</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>n</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>400</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define left and right functions</span>
</span></span><span class=line><span class=cl><span class=n>left_func</span> <span class=o>=</span> <span class=p>(</span><span class=n>R</span><span class=o>+</span><span class=n>eps</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>-</span> <span class=n>x_squared</span>
</span></span><span class=line><span class=cl><span class=n>right_func</span> <span class=o>=</span> <span class=p>(</span><span class=n>R</span><span class=o>+</span><span class=n>eps</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=n>tau</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>n</span><span class=p>)</span> <span class=o>-</span> <span class=n>x_squared</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply hinge loss</span>
</span></span><span class=line><span class=cl><span class=n>left_hinge</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>left_func</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>right_hinge</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>right_func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Start plotting</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot hinge lines</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_squared</span><span class=p>,</span> <span class=n>left_hinge</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>r</span><span class=s1>&#39;Hinge[$(R+\varepsilon)^2 - \|x\|^2$]&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_squared</span><span class=p>,</span> <span class=n>right_hinge</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>r</span><span class=s1>&#39;Hinge[$(R+\varepsilon)^2 + \tau \log n - \|x\|^2$]&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Shade gap and guaranteed loss areas</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>fill_between</span><span class=p>(</span><span class=n>x_squared</span><span class=p>,</span> <span class=n>left_hinge</span><span class=p>,</span> <span class=n>right_hinge</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;orange&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Curb-dependent loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>fill_between</span><span class=p>(</span><span class=n>x_squared</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>left_hinge</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;skyblue&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.4</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Guaranteed loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Remove ticks</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>([])</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ensure equal axis scaling</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_aspect</span><span class=p>(</span><span class=s1>&#39;equal&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Move left spine (y-axis) to x=0 (the origin)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>spines</span><span class=p>[</span><span class=s1>&#39;left&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>set_position</span><span class=p>(</span><span class=s1>&#39;zero&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Hide top and right spines</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>spines</span><span class=p>[</span><span class=s1>&#39;top&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>set_visible</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>spines</span><span class=p>[</span><span class=s1>&#39;right&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>set_visible</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Make bottom spine visible and set its position</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>spines</span><span class=p>[</span><span class=s1>&#39;bottom&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>set_position</span><span class=p>((</span><span class=s1>&#39;data&#39;</span><span class=p>,</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Label the origin &#34;0&#34;</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;0&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;right&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;top&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>fontweight</span><span class=o>=</span><span class=s1>&#39;bold&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Intersection points</span>
</span></span><span class=line><span class=cl><span class=n>left_intersection</span> <span class=o>=</span> <span class=p>(</span><span class=n>R</span><span class=o>+</span><span class=n>eps</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>right_intersection</span> <span class=o>=</span> <span class=p>(</span><span class=n>R</span><span class=o>+</span><span class=n>eps</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=n>tau</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Labels at intersection points</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>left_intersection</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>r</span><span class=s1>&#39;$(R+\varepsilon)^2$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;top&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>fontweight</span><span class=o>=</span><span class=s1>&#39;bold&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>right_intersection</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=sa>r</span><span class=s1>&#39;$(R+\varepsilon)^2 + \tau \log n$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;top&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;orange&#39;</span><span class=p>,</span> <span class=n>fontweight</span><span class=o>=</span><span class=s1>&#39;bold&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Labels</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;$\|x\|^2$&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Hinge loss value&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div></details><figure style=text-align:center><img src=./resources/image-4.png alt="Hinge loss bounds" style="width:70%;margin:0 auto;display:block"><figcaption style=font-weight:400>Hinge loss bounds</figcaption></figure><p>The blue area is the loss that is absolutely guaranteed. The yellow gap will depend on the parameters and the distribution of the probabilistic map.</p><p>So, the real soft margin is at $(R+\varepsilon)^2+2*\sigma^2*\log n$. If we set $\varepsilon=0$, the soft margin gap is $2*\sigma^2\log n$</p><p>For example, if we want the soft margin gap to be $1m$. In our case, valid $n$ is approximately $3^2$. So, that $2*\sigma^2*\log 3^2 =4*\sigma^2*\log 3 \approx 4.4*\sigma^2 =1$. Then, we can set $\sigma=0.47$.</p><figure style=text-align:center><div style=display:flex;justify-content:center;gap:20px;align-items:center><div style=text-align:center><img src=./resources/image-5.png alt="Hinge loss bounds" style=width:100%;max-width:300px><figcaption style=font-weight:400;font-size:.9em>Hinge loss bounds</figcaption></div><div style=text-align:center><img src=./resources/image-6.png alt="Hinge loss bounds" style=width:100%;max-width:300px><figcaption style=font-weight:400;font-size:.9em>Hinge loss bounds zoomed in</figcaption></div></div><figcaption style=font-weight:400;margin-top:10px>Hinge loss bound.</figcaption></figure><p>A more precise estimation of the loss upper bound can be found in Appendix B.</p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><h2 id=appendix-a><strong>Appendix A</strong><a hidden class=anchor aria-hidden=true href=#appendix-a>#</a></h2><h3 id=logsumexp-softmin-identity><strong>Log–Sum–Exp “soft‑min” identity</strong><a hidden class=anchor aria-hidden=true href=#logsumexp-softmin-identity>#</a></h3><p>For real numbers $x_1,\dots,x_n$ and a positive “temperature” (or smoothing) parameter $\tau > 0$:</p>$$ \text{softmin}_{\tau}(x_1,\dots,x_n) = −τ \log⁡ ⁣\sum_{i=1}^n e^{-\frac{x_i}{\tau}}
$$<p>As $\tau \to 0^+$,</p>$$
\text{softmin}_{\tau}(x_1, x_2, \dots, x_n) = \min_i x_i
$$<p>Why does it converge to min⁡?</p><p>Let $m = \min_i x_i$. Rewrite the sum by factoring out the smallest term:</p>$$
\sum_{i=1}^n e^{-\frac{x_i}{\tau}} = e^{-\frac{m}{\tau}}\sum_{i=1}^n e^{-\frac{x_i - m}{\tau}}
$$<p>Plug into the soft‑min:</p>$$
-\tau\log\sum_{i=1}^n e^{-\frac{x_i}{\tau}} = m -\tau \log\sum_{i=1}^n e^{-\frac{x_i -m}{\tau}}
$$<p>All terms inside the log are $\ge 1$ (because one of them is $e^0=1$). Thus,</p>$$
m-\tau \log n \le -\tau\log\sum_{i=1}^ne^{-\frac{x_i}{\tau}} \le m
$$<p>As $\tau \to 0$, the left term equals $m$, so that the log sum equals to the minimum.</p><p>Intuition</p><ul><li><p>Log–sum–exp is a smooth version of “min” (or “max”)
The exponential makes smaller $x_i$ dominate the sum; the log and $-\tau$ rescale back to the original domain.</p></li><li><p>Temperature $\tau$ controls smoothness.</p><ul><li><p>Small $\tau$ → very sharp, almost the true minimum.</p></li><li><p>Large $\tau$ → averages more of the terms (smoother).</p></li></ul></li></ul><p>The gradient (useful in optimization) of the softmin function is</p><p>Define $f = -\tau \log \sum_{i=1}^n e^{-\frac{x_i}{\tau}}$,</p><p>then</p>$$\frac{\partial f}{\partial x_i} = \frac{e^{-\frac{x_i}{\tau}}}{\sum_{i=1}^n e^{-\frac{x_i}{\tau}}} = \text{softmax}(-\frac{x_i}{\tau}),
$$<p>Interpretation:</p><ul><li><p>The gradient weights are a soft assignment to the argmin.</p></li><li><p>As $\tau\to 0$, the gradient becomes a one‑hot vector at the actual minimum index.</p></li></ul><ul><li>Numerical stability (log‑sum‑exp trick)</li></ul><p>To avoid overflow/underflow, subtract the smallest value (or any reference):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>min</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>softmin</span> <span class=o>=</span> <span class=o>-</span><span class=n>tau</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>m</span><span class=p>)</span><span class=o>/</span><span class=n>tau</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span> <span class=o>+</span> <span class=n>m</span>
</span></span></code></pre></div><p>This is equivalent to what we did algebraically.</p><p>There is also continuous version of the softmin function.</p><p>For a continuous function $f(y)$:</p>$$
\text{softmin}_{\tau} f = -\tau \log e^{-f(y)/\tau} dy,
$$<p>As $\tau \to 0$ it converges to $\inf_y f(y)$</p><p>(assuming the integral is well-defined). This is the Laplace approximation principle.</p><p>Max version of soft function is</p>$$
\text{softmax}_{\tau}(x) = \tau \log\sum_i^n e^{x_i/\tau}
$$<p>As $\tau\to 0$, $\lim_{\tau \to 0} \text{ softmax}_{\tau}(x)=\max_i\{x_i\}$.</p><h2 id=appendix-b>Appendix B<a hidden class=anchor aria-hidden=true href=#appendix-b>#</a></h2><h3 id=refined-lower-bound-of-the-soft-distance>Refined Lower Bound of the Soft Distance<a hidden class=anchor aria-hidden=true href=#refined-lower-bound-of-the-soft-distance>#</a></h3><p>Recall our soft distance map is defined as</p>$$d_{\text{soft}}(c) = -2\sigma^2\log\sum_y f(y)e^{-\frac{\|c-y\|^2}{2\sigma^2}}$$<p>Assume $y_{\min}$ is with the minimal distance to $c$ and $f(y_{\min})=1$. Then we have</p>$$\begin{aligned}
d_{\text{soft}}(c) &= -2\sigma^2\log\sum_y f(y)e^{-\frac{\|c-y\|^2}{2\sigma^2}}\\
&=\|c-y_{\min}\|^2 - 2\sigma^2\log\sum_{y}e^{-\frac{\|c-y\|^2 - \|c-y_{\min}\|^2}{2\sigma^2}})\\
&\ge \|c-y_{\min}\|^2 - 2\sigma^2\log\sum_{i=-K,\dots, K, j=-K, \dots, K}e^{-\frac{(i\Delta)^2+(j\Delta)^2}{2\sigma^2}}
\end{aligned}
$$<p>where $K$ is the half of the kernel size and $\Delta$ is the resolution of the map.</p><p>Then we need to estimate the summation term. Let us define</p>$$S(K,\Delta, \sigma) = \sum_{i=-K}^K\sum_{j=-K}^K e^{-\frac{(i\Delta)^2+(j\Delta)^2}{2\sigma^2}}$$<p>We can approximate this with a Gaussian integration on condition that $K$ is large and $\frac{\Delta}{\sigma}$ is small. So we have</p>$$
S(K, \Delta, \sigma) \approx \frac{2\pi\sigma^2}{\Delta^2}
$$<p>Thus, the distance is lower bounded by</p>$$
d_{\text{soft}}(c) \ge \|c-y_{\min}\|^2 - 2\sigma^2\log\frac{2\pi\sigma^2}{\Delta^2}
$$<p>If we want a soft distance to be $0.3$, then by solving $2\sigma^2\log\frac{2\pi\sigma^2}{\Delta^2} = 0.3$ and $\Delta=0.4$, we get $\sigma\approx0.32$.</p><h2 id=appendix-c>Appendix C<a hidden class=anchor aria-hidden=true href=#appendix-c>#</a></h2><h3 id=algorithm-for-distance-transform>Algorithm for Distance Transform<a hidden class=anchor aria-hidden=true href=#algorithm-for-distance-transform>#</a></h3><p>Please refer to [1]. The computational complexity is $\mathcal{O}(WH)$.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] P. F. Felzenszwalb and D. P. Huttenlocher, “Distance transforms of sampled functions,” <em>Theory of Computing</em>, vol. 8, no. 1, pp. 415–428, Sep. 2012, doi:10.4086/toc.2012.v008a019</p><p>[2] J. Cheng, Y. Chen, and Q. Chen, “PLUTO: Pushing the Limit of Imitation Learning‑based Planning for Autonomous Driving,” <em>arXiv preprint arXiv:2404.14327</em>, Apr. 22, 2024.</p><p>[3] A. Meijster, J. B. T. M. Roerdink, and W. H. Hesselink, “A general algorithm for computing distance transforms in linear time,” in <em>Mathematical Morphology and its Applications to Image and Signal Processing</em>, J. Goutsias, L. Vincent, and D. S. Bloomberg, Eds. Dordrecht, The Netherlands: Kluwer Academic, 2000, pp. 331–340. <a href=https://pure.rug.nl/ws/files/14632734/c2.pdf>https://pure.rug.nl/ws/files/14632734/c2.pdf</a></p><p>[4] T. Strutz, “The Distance Transform and its Computation — An Introduction,” <em>arXiv</em>, vol. arXiv:2106.03503 v2, Feb. 24, 2023, arXiv Working Paper TECHP/2021/06. <a href=https://arxiv.org/pdf/2106.03503>https://arxiv.org/pdf/2106.03503</a></p><p>[5] “Bilinear interpolation,” Wikipedia, The Free Encyclopedia, last modified August 6, 2025. [Online]. Available: <a href=https://en.wikipedia.org/wiki/Bilinear>https://en.wikipedia.org/wiki/Bilinear</a>_interpolation</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/autonomous-driving/>Autonomous-Driving</a></li><li><a href=https://livey.github.io/tags/collision-loss/>Collision Loss</a></li><li><a href=https://livey.github.io/tags/end-to-end-planning/>End-to-End Planning</a></li><li><a href=https://livey.github.io/tags/probabilistic-maps/>Probabilistic Maps</a></li><li><a href=https://livey.github.io/tags/cost-map/>Cost Map</a></li><li><a href=https://livey.github.io/tags/signed-distance-map/>Signed Distance Map</a></li></ul><nav class=paginav><a class=prev href=https://livey.github.io/posts/2025-08-20-lidar-post-processing/><span class=title>« Prev</span><br><span>Bundle Adjustment for LiDAR SLAM: Mathematical Formulation and Optimization</span>
</a><a class=next href=https://livey.github.io/posts/2025-07-22-track-life/><span class=title>Next »</span><br><span>A State Machine for Object Tracking</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on x" href="https://x.com/intent/tweet/?text=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f&amp;hashtags=AutonomousDriving%2cCollisionLoss%2cEnd-to-EndPlanning%2cProbabilisticMaps%2cCostMap%2cSignedDistanceMap"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f&amp;title=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving&amp;summary=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f&title=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on whatsapp" href="https://api.whatsapp.com/send?text=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on telegram" href="https://telegram.me/share/url?text=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Collision Loss: Bounds and Soft Distance Maps for Autonomous Driving on ycombinator" href="https://news.ycombinator.com/submitlink?t=Probabilistic%20Collision%20Loss%3a%20Bounds%20and%20Soft%20Distance%20Maps%20for%20Autonomous%20Driving&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-08-13-collission-loss%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
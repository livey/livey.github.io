<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Gentle Introduction to Diffusion Models and Flow Matching | Fuwei's Tech Notes</title>
<meta name=keywords content="Diffusion Model,Generative Model"><meta name=description content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><meta name=author content="Fuwei Li"><link rel=canonical href=https://livey.github.io><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://livey.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://livey.github.io/posts/2025-11-22-diffusion/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content="Fuwei Li"><meta name=description content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><meta property="og:type" content="article"><meta property="og:url" content="https://livey.github.io/posts/2025-11-22-diffusion/"><meta property="og:title" content="A Gentle Introduction to Diffusion Models and Flow Matching"><meta property="og:description" content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><meta property="og:image" content="https://livey.github.io/images/site-preview.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="A Gentle Introduction to Diffusion Models and Flow Matching"><meta name=twitter:description content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><meta name=twitter:image content="https://livey.github.io/images/site-preview.jpg"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://livey.github.io/posts/2025-11-22-diffusion/"><meta property="og:site_name" content="Fuwei's Tech Notes"><meta property="og:title" content="A Gentle Introduction to Diffusion Models and Flow Matching"><meta property="og:description" content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-22T00:00:00+00:00"><meta property="article:tag" content="Diffusion Model"><meta property="article:tag" content="Generative Model"><meta property="og:image" content="https://livey.github.io/posts/2025-11-22-diffusion/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://livey.github.io/posts/2025-11-22-diffusion/%3Cimage%20path/url%3E"><meta name=twitter:title content="A Gentle Introduction to Diffusion Models and Flow Matching"><meta name=twitter:description content="A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://livey.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Gentle Introduction to Diffusion Models and Flow Matching","item":"https://livey.github.io/posts/2025-11-22-diffusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Gentle Introduction to Diffusion Models and Flow Matching","name":"A Gentle Introduction to Diffusion Models and Flow Matching","description":"A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models.","keywords":["Diffusion Model","Generative Model"],"articleBody":"Diffusion models have emerged as a powerful and flexible class of generative models, underpinning recent breakthroughs in image, audio, and scientific data generation. This article offers a gentle, beginner-friendly overview of diffusion models and the related flow matching framework, demystifying their mathematical foundations and practical training procedures. We will delve into the core concepts, guiding equations, and step-by-step training algorithms, aiming to provide readers with both an intuition for how these models work and a roadmap for implementing them in practice. This article serves as notes on papers [1] and [2].\nGenerative Models The original motivation behind diffusion models is to generate high-quality, diverse data. To achieve this, we map the observed data distribution to a simple base distribution, sample from the base, and map the samples back to data space. Thus, the goal of a generative model is to learn a mapping between the data and base distributions (in either direction).\nDiffusion Models Flow Matching We mainly follow tutorial [1] to introduce flow matching and diffusion models.\nFlow is a solution to the ODE\n$$ \\psi : \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d, \\quad (x_0, t) \\mapsto \\psi_t(x_0) \\tag{2a} $$$$ \\frac{d}{dt} \\psi_t(x_0) = u_t\\bigl(\\psi_t(x_0)\\bigr) \\tag{2b} $$$$ \\psi_0(x_0) = x_0 \\tag{2c} $$For a given initial condition $X_0 = x_0$, a trajectory of the ODE is recovered via $X_t = \\psi_t(X_0)$. Therefore, vector fields, ODEs, and flows are, intuitively, three descriptions of the same object: vector fields define ODEs whose solutions are flows.\nSolutions are flows. As with every equation, we should ask: does a solution exist, and if so, is it unique? Under mild assumptions on $u_t$, the answer is “yes” to both:\nTheorem 3 (Flow existence and uniqueness)\nIf $u : \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d$ is continuously differentiable with a bounded derivative, then the ODE in (2) has a unique solution given by a flow $\\psi_t$. In this case, $\\psi_t$ is a diffeomorphism for all $t$, i.e. $\\psi_t$ is continuously differentiable with a continuously differentiable inverse $\\psi_t^{-1}$.\nSimulating an ODE. In general, it is not possible to compute the flow $\\psi_t$ explicitly if $u_t$ is not as simple as a linear function. In these cases, one uses numerical methods to simulate ODEs. Fortunately, this is a classical and well researched topic in numerical analysis, and a myriad of powerful methods exist [11]. One of the simplest and most intuitive methods is the Euler method. In the Euler method, we initialize with $X_0 = x_0$ and update via\n$$ X_{t+h} = X_t + h\\,u_t(X_t) \\qquad (t = 0, h, 2h, 3h, \\ldots, 1 - h) \\tag{4} $$where $h = n^{-1} \u003e 0$ is a step size hyperparameter with $n \\in \\mathbb{N}$. For this class, the Euler method will be good enough. To give you a taste of a more complex method, let us consider Heun’s method defined via the update rule\n$$ X'_{t+h} = X_t + h\\,u_t(X_t) \\quad\\text{(initial guess of new state)} $$$$ X_{t+h} = X_t + \\frac{h}{2}\\bigl(u_t(X_t) + u_{t+h}(X'_{t+h})\\bigr) \\quad\\text{(update with average u at current and guessed state)} $$Intuitively, Heun’s method is as follows: it takes a first guess $X'_{t+h}$ of what the next step could be but corrects the direction initially taken via an updated guess.\nFlow models. We can now construct a generative model via an ODE. Remember that our goal was to convert a simple distribution $p_{\\text{init}}$ into a complex distribution $p_{\\text{data}}$. The simulation of an ODE is thus a natural choice for this transformation. A flow model is described by the ODE\n$$ X_0 \\sim p_{\\text{init}} \\qquad \\triangleright\\ \\text{random initialization} $$$$ \\frac{d}{dt} X_t = u_t^\\theta(X_t) \\qquad \\triangleright\\ \\text{ODE} $$where the vector field $u_t^\\theta$ is a neural network $u_t^\\theta$ with parameters $\\theta$. For now, we will speak of $u_t^\\theta$ as being a generic neural network; i.e. a continuous function $u_t^\\theta : \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d$ with parameters $\\theta$. Later, we will discuss particular choices of neural network architectures. Our goal is to make the endpoint $X_1$ of the trajectory have distribution $p_{\\text{data}}$, i.e.\n$$ X_1 \\sim p_{\\text{data}} \\quad \\Leftrightarrow \\quad \\psi_1^\\theta(X_0) \\sim p_{\\text{data}} $$where $\\psi_t^\\theta$ describes the flow induced by $u_t^\\theta$. Note however: although it is called flow model, the neural network parameterizes the vector field, not the flow. In order to compute the flow, we need to simulate the ODE. In algorithm 1, we summarize the procedure how to sample from a flow model.\nDiffusion Models In eq. (1a). Hence, we need to find an equivalent formulation of ODEs that does not use derivatives. where $R_t(h)$ describes a negligible function for small $h$, i.e. such that $\\lim_{h \\to 0} R_t(h) = 0$, and in (i) we simply use the definition of derivatives. The derivation above simply restates what we already know: A trajectory $(X_t)_{0 \\le t \\le 1}$ of an ODE takes, at every timestep, a small step in the direction $u_t(X_t)$. We may now amend the last equation to make it stochastic: A trajectory $(X_t)_{0 \\le t \\le 1}$ of an SDE takes, at every timestep, a small step in the direction $u_t(X_t)$ plus some contribution from a Brownian motion:\n$$ X_{t+h}= X_t + \\underbrace{h\\,u_t(X_t)}_{\\text{deterministic}} + \\underbrace{\\sigma_t\\bigl(W_{t+h} - W_t\\bigr)}_{\\text{stochastic}}+ \\underbrace{h\\,R_t(h)}_{\\text{error term}}\\tag{6} $$where $\\sigma_t \\ge 0$ describes the diffusion coefficient and $R_t(h)$ describes a stochastic error term such that the standard deviation $\\mathbb{E}\\bigl[\\lVert R_t(h)\\rVert^2\\bigr]^{1/2} \\to 0$ goes to zero for $h \\to 0$. The above describes a stochastic differential equation (SDE).\nIt is common to denote a stochastic differential equation (SDE) in the following symbolic notation:\n$$ \\mathrm{d}X_t = u_t(X_t)\\,\\mathrm{d}t + \\sigma_t\\,\\mathrm{d}W_t \\tag{7a} $$$$ X_0 = x_0 \\tag{7b} $$ Theorem 5 (SDE Solution Existence and Uniqueness)\nIf $u : \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d$ is continuously differentiable with a bounded derivative and $\\sigma_t$ is continuous, then the SDE in (7) has a solution given by the unique stochastic process $(X_t)_{0 \\le t \\le 1}$.\nSimulating an SDE. If you struggle with the abstract definition of an SDE so far, then don’t worry about it. A more intuitive way of thinking about SDEs is given by answering the question: How might we simulate an SDE? The simplest such scheme is known as the Euler–Maruyama method, and is essentially to SDEs what the Euler method is to ODEs. Using the Euler–Maruyama method, we initialize $X_0 = x_0$ and update iteratively via\n$$ X_{t+h} = X_t + h u_t(X_t) + \\sqrt{h}\\,\\sigma_t \\epsilon_t, \\qquad \\epsilon_t \\sim \\mathcal{N}(0, I_d) \\tag{9} $$where $h = n^{-1} \u003e 0$ is a step size hyperparameter for $n \\in \\mathbb{N}$. In other words, to simulate using the Euler–Maruyama method, we take a small step in the direction of $u_t(X_t)$ as well as add a little bit of Gaussian noise scaled by $\\sqrt{h}\\,\\sigma_t$. When simulating SDEs in this class (such as in the accompanying labs), we will usually stick to the Euler–Maruyama method.\nSummary 7 (SDE generative model)\nThroughout this document, a diffusion model consists of a neural network $u_t^\\theta$ with parameters $\\theta$ that parameterize a vector field and a fixed diffusion coefficient $\\sigma_t$:\nNeural network: $u^\\theta : \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d,\\ (x,t) \\mapsto u_t^\\theta(x)$ with parameters $\\theta$ Fixed: $\\sigma_t : [0,1] \\to [0,\\infty),\\ t \\mapsto \\sigma_t$ To obtain samples from our SDE model (i.e. generate objects), the procedure is as follows:\nInitialization: $X_0 \\sim p_{\\text{init}}$ ▸ Initialize with simple distribution, e.g. a Gaussian Simulation: $\\mathrm{d}X_t = u_t^\\theta(X_t)\\,\\mathrm{d}t + \\sigma_t\\,\\mathrm{d}W_t$ ▸ Simulate SDE from $0$ to $1$ Goal: $X_1 \\sim p_{\\text{data}}$ ▸ Goal is to make $X_1$ have distribution $p_{\\text{data}}$ A diffusion model with $\\sigma_t = 0$ is a flow model.\n$$p_t(x,z) = p_t(x|z)p_{data}(z)$$ as beblow\nsample $z\\sim p_{data}(z)$, sample $x\\sim p_t(x|z)$ Then, we descard $z$. The resulting $x$ is distributed as the marginal $p_t(x)$.\nIdealy, we would like to learn a marginal vector field $u_t$ that maps the noise distribution $p_{\\text{init}}$ to the data distribution $p_{\\text{data}}$. However, it involves learning the marginal distribution $p_t$, which is intractable. Therefore, we need to use the marginalization trick. For marginal vector field, it does not depend on the distribution of the whole data. So, we can construct some conditional vector fields $u_t^{\\text{target}}(x \\mid z)$ for each data point $z$. By this sepcial construction, we can get closed-form expression of the conditional vector fields. The following theorem shows the relationship between the marginal vector field and the conditional vector field. Indeed we can choose any conditional probability. In practice, we would like to choose the one that is easy to sample and yield trackable targets (vector field).\nTheorem 10 (Marginalization trick)\nFor every data point $z \\in \\mathbb{R}^d$, let $u_t^{\\text{target}}(\\cdot \\mid z)$ denote a conditional vector field, defined so that the corresponding ODE yields the conditional probability path $p_t(\\cdot \\mid z)$, viz.,\n$$ X_0 \\sim p_{\\text{init}}, \\\\\\\\ \\qquad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t \\mid z) \\;\\;\\Rightarrow\\;\\; X_t \\sim p_t(\\cdot \\mid z) \\qquad (0 \\le t \\le 1). \\tag{18} $$Then the marginal vector field $u_t^{\\text{target}}(x)$, defined by\n$$ u_t^{\\text{target}}(x) =\\int u_t^{\\text{target}}(x \\mid z) \\frac{p_t(x \\mid z)\\, p_{\\text{data}}(z)}{p_t(x)} \\, dz, \\tag{19} $$follows the marginal probability path, i.e.\n$$ X_0 \\sim p_{\\text{init}}, \\qquad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) \\;\\;\\Rightarrow\\;\\; X_t \\sim p_t \\qquad (0 \\le t \\le 1). \\tag{20} $$In particular, $X_1 \\sim p_{\\text{data}}$ for this ODE, so that we might say “$u_t^{\\text{target}}$ converts noise $p_{\\text{init}}$ into data $p_{\\text{data}}$”.\nWhy we say $X_1 \\sim p_{\\text{data}}$? It is due to the fact that we implicitly in the conditional probability states that $p_1(\\cdot|z)=\\delta_z$ and we have $p_1(x) = \\int p_1(x|z)p_{\\text{data}}(z)\\mathrm{d}z=\\int \\delta_z\\cdot p_{\\text{data}}(z)\\mathrm{d}z=p_{\\text{data}}(z)$.\nHere I list some notations for clarification.\nConcept Notation Meaning Single deterministic solution $x_t$ state at time (t) for a fixed $x_0$ Explicit dependence on start $x(t;x_0)$ same, but shows dependence on $x_0$ Flow map $\\phi_t(x_0)$ the function mapping $x_0 \\mapsto x_t$ Random state $X_t$ $x_t$ when $x_0$ is random Sampling statement $X_t \\sim p_t$ distribution (law) of $X_t$ is $p_t$ A minimal mental model\nPick initial point $x_0$. The ODE defines a path $t\\to x_t$. (trajectory) Collect all these paths into $x_t=\\phi_t(x_0)$. (flow map) if $x_0$ is random, call it $X_0$ and the path is $X_t$. (random process) The following theorem shows the relationship between the vector field and the distribution. It tells that the change of the distribution over time is equal to the probability move according to the vector field.\n$$ \\partial_t p_t(x) = -\\operatorname{div}\\big(p_t u_t^{\\text{target}}\\big)(x) \\quad \\text{for all } x \\in \\mathbb{R}^d,\\, 0 \\le t \\le 1, \\tag{24} $$ where $\\partial_t p_t(x) = \\frac{d}{dt} p_t(x)$ denotes the time-derivative of $p_t(x)$ (This is true because we fix $x$ here. It is different from the following Langarangian equation where it focuses on the trajectory $x_t$). Equation 24 is known as the continuity equation. (Note that this is partial differential equation (PDE) instead of ordinary partial equaiton )\nwhere the divergence operator $\\operatorname{div}$, is defined as:\n$$ \\operatorname{div}(v_t)(x) = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i} v_t^{\\color{red}{(i)}}(x) \\tag{23} $$In the book, [1], there is a typo that they mistakenly omitted the superscript $(i)$ in the divergence operator.\nThe continuity equation states how the probability distribution changes for fix spatial point using the PDE. We can also evaluate the PDE along the curve, $x=X_t$, to get its Langrangain form.\n$$ \\begin{aligned} \\partial_t p_t(x) \u0026=\\sum_i\\frac{\\partial}{\\partial x_i}(p \\cdot u_i)\\\\ \u0026= -\\nabla\\, p_t(x)^\\top u_t(x) - \\text{div}\\,u_t\\cdot p_t(x) \\end{aligned} $$$\\Rightarrow$\n$$ \\frac{\\partial_t p_t(x)}{p_t(x)} = -\\text{div}\\,u_t(x) - \\frac{\\nabla p_t(x)^\\top}{p_t(x)}u_t(x) $$$\\Rightarrow$\n$$ \\partial_t \\ln p_t(x) = -\\text{div}\\,u_t(x) - \\nabla_{x}\\ln p_t(x)^\\top u_t(x) $$Further, we have\n$$ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d}t}\\ln p_t(x) \u0026= \\frac{\\partial_t p_t(x)}{p_t(x)} + \\frac{\\partial_x p_t(x)\\cdot \\partial_t x}{p_t(x)}\\\\ \u0026=\\partial_t \\ln p_t(x) + \\partial_t x^\\top \\nabla_x \\ln p_t(x)\\\\ \u0026=-\\text{div}\\,u(x) \\end{aligned} $$In summary, we get another nice continuity equation\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}t}\\ln p_t(x) =-\\text{div}\\,u(x) $$Equivalently\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}t} p_t(x) =-p_t(x)\\text{div}\\,u(x) $$We can even compute the probability, $p_t(x)$. By integration both sides of the above equation, we get\n$$ \\ln p_t(x) - \\ln p_0(x)=-\\int_0^t \\text{div}\\,u(x)\\,\\mathrm{d}t $$Take exponential on both sides we have\n$$ p_t(x)=p_0(x_0)\\exp\\left(-\\int_0^t \\text{div}\\,u_s(x_s)\\,\\mathrm{d}s\\right) $$We can use the continuity equation to derive the relationship between the marginal vector field and the conditional vector field.\nWe may also see the following form of the theorem\nTheorem 13 (SDE extension trick)\nDefine the conditional and marginal vector fields $u_t^{\\text{target}}(x \\mid z)$ and $u_t^{\\text{target}}(x)$ as before. Then, for diffusion coefficient $\\sigma_t \\ge 0$, we may construct an SDE which follows the same probability path:\n$$ X_0 \\sim p_{\\text{init}}, \\qquad \\mathrm{d}X_t =\\Big[ u_t^{\\text{target}}(X_t) + \\frac{\\sigma_t^2}{2} \\nabla \\log p_t(X_t) \\Big] \\mathrm{d}t + \\sigma_t \\mathrm{d}W_t \\tag{25} $$$$ \\Rightarrow\\quad X_t \\sim p_t \\qquad (0 \\le t \\le 1) \\tag{26} $$In particular, $X_1 \\sim p_{\\text{data}}$ for this SDE. The same identity holds if we replace the marginal probability $p_t(x)$ and vector field $u_t^{\\text{target}}(x)$ with the conditional probability path $p_t(x \\mid z)$ and vector field $u_t^{\\text{target}}(x \\mid z)$. Here, we require $\\sigma_t \\ge 0$. So, we can inject as much as noise but get the same expected distribution.\nTheorem 15 (Fokker–Planck Equation)\nLet $p_t$ be a probability path and let us consider the SDE\n$$ X_0 \\sim p_{\\text{init}}, \\qquad \\mathrm{d}X_t = u_t(X_t)\\,\\mathrm{d}t + \\sigma_t\\,\\mathrm{d}W_t. $$Then $X_t$ has distribution $p_t$ for all $0 \\le t \\le 1$ if and only if the Fokker–Planck equation holds:\n$$ \\partial_t p_t(x) = -\\operatorname{div}\\big(p_t u_t\\big)(x) + \\frac{\\sigma_t^2}{2}\\,\\Delta p_t(x) \\quad \\text{for all } x \\in \\mathbb{R}^d,\\; 0 \\le t \\le 1. \\tag{30} $$ Summary 17 (Derivation of the Training Target)\nThe flow training target is the marginal vector field $u_t^{\\text{target}}$. To construct it, we choose a conditional probability path $p_t(x\\mid z)$ that fulfils $p_0(\\cdot \\mid z) = p_{\\text{init}},\\ p_1(\\cdot \\mid z) = \\delta_z$. Next, we find a conditional vector field $u_t^{\\text{flow}}(x\\mid z)$ such that its corresponding flow $\\psi_t^{\\text{target}}(x\\mid z)$ fulfills\n$$ X_0 \\sim p_{\\text{init}} \\;\\Rightarrow\\; X_t = \\psi_t^{\\text{target}}(X_0\\mid z) \\sim p_t(\\cdot \\mid z), $$or, equivalently, that $u_t^{\\text{target}}$ satisfies the continuity equation. Then the marginal vector field defined by\n$$ u_t^{\\text{target}}(x) =\\int u_t^{\\text{target}}(x\\mid z) \\frac{p_t(x\\mid z)p_{\\text{data}}(z)}{p_t(x)}\\,\\mathrm{d}z, \\tag{32} $$follows the marginal probability path, i.e.,\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\mathrm{d}X_t = u_t^{\\text{target}}(X_t)\\,\\mathrm{d}t \\;\\Rightarrow\\; X_t \\sim p_t \\quad (0 \\le t \\le 1). \\tag{33} $$In particular, $X_1 \\sim p_{\\text{data}}$ for this ODE, so that $u_t^{\\text{target}}$ “converts noise into data”, as desired.\nExtending to SDEs. For a time-dependent diffusion coefficient $\\sigma_t \\ge 0$, we can extend the above ODE to an SDE with the same marginal probability path:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\mathrm{d}X_t = \\Big[ u_t^{\\text{target}}(X_t) + \\frac{\\sigma_t^2}{2}\\nabla \\log p_t(X_t) \\Big] \\mathrm{d}t + \\sigma_t \\mathrm{d}W_t \\tag{34} $$$$ \\Rightarrow\\; X_t \\sim p_t \\quad (0 \\le t \\le 1), \\tag{35} $$where $\\nabla \\log p_t(x)$ is the marginal score function\n$$ \\nabla \\log p_t(x) =\\int \\nabla \\log p_t(x\\mid z) \\frac{p_t(x\\mid z)p_{\\text{data}}(z)}{p_t(x)}\\,\\mathrm{d}z. \\tag{36} $$Conditional and Marginal Score Functions\nIn particular, for the trajectories $X_t$ of the above SDE, it holds that $X_1 \\sim p_{\\text{data}}$, so that the SDE “converts noise into data”, as desired. An important example is the Gaussian probability path, yielding the formulae:\n$$ p_t(x\\mid z) =\\mathcal{N}(x; \\alpha_t z, \\beta_t^2 I_d) \\tag{37} $$$$ u_t^{\\text{flow}}(x\\mid z) =\\left(\\dot{\\alpha}_t- \\frac{\\dot{\\beta}_t}{\\beta_t}\\alpha_t \\right) z + \\frac{\\dot{\\beta}_t}{\\beta_t} x \\tag{38} $$$$ \\nabla \\log p_t(x\\mid z) =- \\frac{x - \\alpha_t z}{\\beta_t^2}, \\tag{39} $$for noise schedulers $\\alpha_t, \\beta_t \\in \\mathbb{R}$: continuously differentiable, monotonic functions such that $\\alpha_0 = \\beta_1 = 0$, $\\alpha_1 = \\beta_0 = 1$.\nIn general, If you define a conditional flow map (or know the sampling method) as $x=\\phi_t(z, \\epsilon)$. Let us define the random veriable\n$$ X_t := \\phi_t(z, \\epsilon) $$Then we can get $u_t(x|z)$ by taking partial derivative of $X_t$\n$$ u_t(x|z) = \\partial_t X_t = \\partial_t \\phi_t(z, \\varepsilon) $$$u_t$ is function of $x$. So, we need represent $\\varepsilon$ as function of $x$. Thus we get $\\varepsilon = \\phi_t^{-1}(x, z)$. So, we get the conditional velocity field\n$$ u_t(x|z) = \\partial_t\\left(x, \\phi_t^{-1}(x, z)\\right) $$Training the Generative Model We are trying to learn the marginal vector field $u_t^{\\text{target}}(x)$. So, our objective is to parameter a neuron network, $v^{\\theta}_t(x)$ to approximate $u_t^{\\text{target}}(x)$ at every $t$ and $x$. Thus, our cost function is\n$$ \\mathcal{L}(\\theta) = \\mathbb{E}_{t, x} \\left[ \\| v^{\\theta}_t(x) - u_t^{\\text{target}}(x) \\|^2 \\right]. $$Here we do not constrain the distribution of $t$. But, a convenient choice is uniform distribution, $t\\sim \\mathcal{U}(0,1)$. Other choices are possible, e.g., Beta distribution. Since $x = p_t(x) = \\int p(x|z)p_{\\text{data}}(z)\\mathrm{d}z$, we have\n$$ L_{\\text{FM}}(\\theta) = \\mathbb{E}_{t\\sim \\mathcal{U}(0,1), x\\sim p_t(x|z), z\\sim p_{\\text{data}}(z)} \\left[ \\| v^{\\theta}_t(x) - u_t^{\\text{target}}(x) \\|^2 \\right]. $$Theorem 18 The marginal flow matching loss equals the conditional flow matching loss up to a constant. That is\n$$ \\mathcal{L}_{\\text{FM}}(\\theta) = \\mathcal{L}_{\\text{CFM}}(\\theta) + C, $$$$ \\nabla_\\theta \\mathcal{L}_{\\text{FM}}(\\theta) = \\nabla_\\theta \\mathcal{L}_{\\text{CFM}}(\\theta). $$Hence, minimizing $\\mathcal{L}_{\\text{CFM}}(\\theta)$ with e.g., stochastic gradient descent (SGD) is equivalent to minimizing $\\mathcal{L}_{\\text{FM}}(\\theta)$ in the same fashion. In particular, for the minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{CFM}}(\\theta)$, it will hold that $u_t^{\\theta^*} = u_t^{\\text{target}}$ (assuming an infinitely expressive parameterization).\nAlgorithm 3 Flow Matching Training Procedure (here for Gaussian CondOT path $p_t(x\\mid z) = \\mathcal{N}(tz, (1 - t)^2)$)\nRequire: A dataset of samples $z \\sim p_{\\text{data}}$, neural network $u_t^\\theta$\n$$\\mathcal{L}(\\theta) = \\lVert u_t^\\theta(x) - (z - \\epsilon) \\rVert^2$$\n(General case: $= \\lVert u_t^\\theta(x) - u_t^{\\text{target}}(x\\mid z) \\rVert^2$)\n7: Update the model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$.\n8: end for\nAlgorithm 4 Score Matching Training Procedure for Gaussian probability path\nRequire: A dataset of samples $z \\sim p_{\\text{data}}$, score network $s_t^\\theta$ or noise predictor $\\epsilon_t^\\theta$\n1: for each mini-batch of data do\n2: Sample a data example $z$ from the dataset.\n3: Sample a random time $t \\sim \\text{Unif}_{[0,1]}$.\n4: Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$ (General case: $x_t \\sim p_t(\\cdot \\mid z)$)\n5: Set $x_t = \\alpha_t z + \\beta_t \\epsilon$\n6: Compute loss\n$$ \\mathcal{L}(\\theta) = \\lVert s_t^\\theta(x_t) + \\tfrac{\\epsilon}{\\beta_t} \\rVert^2 \\quad \\text{(General case: } = \\lVert s_t^\\theta(x_t) - \\nabla \\log p_t(x_t \\mid z) \\rVert^2 \\text{)} $$Alternatively:\n$$ \\mathcal{L}(\\theta) = \\lVert \\epsilon_t^\\theta(x_t) - \\epsilon \\rVert^2 $$7: Update the model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$.\n8: end for\nBuilding an Image Generator The objective of our network is to learn an mixture of the guided and unguided vector fields.\n$$ \\tilde{u}_t(x|y) = u_t^{\\text{target}}(x) + w b_t \\nabla \\log p_t(y|x) \\\\ = u_t^{\\text{target}}(x) + w b_t (\\nabla \\log p_t(x|y) - \\nabla \\log p_t(x)) \\\\ = u_t^{\\text{target}}(x) - (w a_t x + w b_t \\nabla \\log p_t(x)) + (w a_t x + w b_t \\nabla \\log p_t(x|y)) \\\\ = (1 - w) u_t^{\\text{target}}(x) + w u_t^{\\text{target}}(x|y) $$ Summary 27 (Classifier-Free Guidance for Flow Models)\n$$ \\tilde{u}_t(x|y) = (1-w)u_t^{\\text{target}}(x|\\varnothing) + w u_t^{\\text{target}}(x|y). \\tag{70} $$$$ \\mathcal{L}^{\\text{CFG}}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{\\square}\\big\\lVert u_t^{\\theta}(x|y) - u_t^{\\text{target}}(x|z) \\big\\rVert^2 \\tag{71} $$$$ \\square = (z,y) \\sim p_{\\text{data}}(z,y),\\quad t \\sim \\text{Unif}[0,1],\\quad x \\sim p_t(\\cdot|z),\\ \\text{replace } y = \\varnothing \\text{ with prob. } \\eta. \\tag{72} $$In plain English, $\\mathcal{L}^{\\text{CFG}}_{\\text{CFM}}$ might be approximated by\n$(z,y) \\sim p_{\\text{data}}(z,y)$ — Sample $(z,y)$ from data distribution. $t \\sim \\text{Unif}[0,1]$ — Sample $t$ uniformly on $[0,1]$. $x \\sim p_t(x|z)$ — Sample $x$ from the conditional probability path $p_t(x|z)$. with prob. $\\eta$, $y \\leftarrow \\varnothing$ — Replace $y$ with $\\varnothing$ with probability $\\eta$. $$ \\widehat{\\mathcal{L}}^{\\text{CFG}}_{\\text{CFM}}(\\theta) = \\big\\lVert u_t^{\\theta}(x|y) - u_t^{\\text{target}}(x|z) \\big\\rVert^2. $$$$ u_t^{\\text{target}}(x|z) = u_t^{\\text{target}}(x|z,y). $$ At inference time, for a fixed choice of $y$, we may sample via\nInitialization: $X_0 \\sim p_{\\text{init}}(x)$ — Initialize with simple distribution (such as a Gaussian). Simulation: $\\mathrm{d}X_t = \\tilde{u}_t^{\\theta}(X_t|y)\\,\\mathrm{d}t$ — Simulate ODE from $t=0$ to $t=1$. Samples: $X_1$ — Goal is for $X_1$ to adhere to the guiding variable $y$. Recent Advances in Diffusion Models Back to Basics: Let Denoising Generative Models Denoise This paper [2] is simple yet inspiring. The toy example in Fig. 2 clearly shows why it is preferable to predict the original data rather than the noise. Table 1 also summarizes how different objectives are related.\n$$ \\begin{cases} x_\\theta = \\text{net}_\\theta \\\\ z_t = t x_\\theta + (1-t)\\epsilon_\\theta \\\\ v_\\theta = x_\\theta - \\epsilon_\\theta \\end{cases} $$Theory By writing the data distribution as discrete distribution\n$$ p(y) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{y^i} $$where $\\delta_{y^i}$ is the Dirac delta distribution. Then, we can derive many interesting closed-form expressions [4, 5].\nFirstly, we will compute the marginal distribution $X_t \\sim p_t(x)$. Let’s assume the conditional probability path is given by\n$$ X_t|z \\sim \\mathcal{N}(\\mu_t(z), \\sigma_t^2(z)\\mathbf{I}_d) $$Then, the marginal distribution is\n$$ \\begin{aligned} p_t(x) \u0026= \\int_z p(z) p_t(x|z) \\mathrm{d}z \\\\ \u0026=\\int_z \\frac{1}{n} \\sum_{i=1}^n \\delta_{y^i} \\mathcal{N}(x; \\mu_t(z), \\sigma_t^2(z)\\mathbf{I}_d) \\mathrm{d}z \\\\ \u0026=\\frac{1}{n} \\sum_{i=1}^n \\mathcal{N}(x; \\mu_t(y^i), \\sigma_t^2(y^i)\\mathbf{I}_d) \\end{aligned} $$It is just a mixture of Gaussians!\nSecond, we will compute the marginal vector field, $u_t(x)$. Let us reparamterize the conditional variable $X_t$ as\n$$ X_t = \\alpha_t(z) + \\beta_t(z) \\varepsilon $$So, we have $\\varepsilon = \\frac{X_t - \\alpha_t(z)}{\\beta_t(z)}$ and\n$$ \\partial_t X_t = \\dot{\\alpha}_t(z) + \\dot{\\beta}_t(z) \\varepsilon $$Substitute the expression of $\\varepsilon$, we have\n$$ u_t(x|z) = \\frac{\\dot{\\beta}_t(x)}{\\beta_t(z)}\\left(x - \\alpha_t(z)\\right) + \\dot{\\alpha}_t(z) $$Then, the marginal vector field is\n$$ \\begin{aligned} u_t(x) \u0026= \\int_z u_t(x|z)p_t(z|x) \\mathrm{d}z \\\\ \u0026= \\int_z u_t(x|z) \\frac{p_t(x|z)p(z)}{\\int_z p_t(x|z)p(z)} \\mathrm{d}z \\\\ \u0026= \\frac{1}{n} \\sum_{i=1}^n u_t(x|y^i) \\frac{\\mathcal{N}(x; \\alpha_t(y^i), \\sigma_t^2(y^i)\\mathbf{I}_d)}{\\frac{1}{n} \\mathcal{N}(x; \\alpha_t(y^i), \\sigma_t^2(y^i)\\mathbf{I}_d)}\\\\ \u0026=\\frac{1}{n} \\sum_{i=1}^n u_t(x|y^i)\\frac{\\mathcal{N}\\left((x-\\alpha_t(y^i))/\\sigma_t(y^i);\\mathbf{0}, \\mathbf{I}_d\\right)}{\\frac{1}{n} \\sum_i\\mathcal{N}\\left((x-\\alpha_t(y^i))/\\sigma_t(y^i); \\mathbf{0}, \\mathbf{I}_d\\right)}\\\\ \u0026=\\frac{1}{n} \\sum_{i=1}^n u_t(x|y^i)\\frac{\\exp\\left(-\\|x-\\alpha_t(y^i)\\|^2/(2\\sigma_t^2(y^i))\\right)}{\\frac{1}{n} \\sum_i\\exp\\left(-\\|x-\\alpha_t(y^i)\\|^2)/(2\\sigma_t^2(y^i)\\right)} \\end{aligned} $$This is weighted sum of the conditional vector field, where the weights are defined by the exponential of the negative distance to the data samples. Let us look at the example that $\\alpha_t(z)=tz$ and $\\beta_t(z)=(1-t)$. So that the conditional vector field is\n$$ u_t(x|z) = \\frac{z-x}{1-t} $$Substitute in the marginal vector filed equation we have\n$$ u_t(x) = \\frac{1}{n}\\sum_{i=1}^n\\frac{y^i - x}{1-t}\\cdot\\frac{\\exp\\left(-\\|x-t\\cdot y^i)\\|^2/(2(1-t)^2))\\right)}{\\frac{1}{n} \\sum_i\\exp\\left(-\\|x-t\\cdot y^i\\|^2)/(2(1-t)^2)\\right)} $$References [1] P. Holderrieth and E. Erives, “An Introduction to Flow Matching and Diffusion Models,” July 12, 2025, arXiv: arXiv:2506.02070. doi: 10.48550/arXiv.2506.02070.\n[2] T. Li and K. He, “Back to Basics: Let Denoising Generative Models Denoise,” Nov. 17, 2025, arXiv:2511.13720. doi: 10.48550/arXiv.2511.13720.\n[3] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow Matching for Generative Modeling,” Feb. 08, 2023, arXiv: arXiv:2210.02747. doi: 10.48550/arXiv.2210.02747.\n[4] W. Gao and M. Li, “How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?,” Oct. 31, 2024, arXiv: arXiv:2410.23594. doi: 10.48550/arXiv.2410.23594.\n[5] Q. Bertrand, A. Gagneux, M. Massias, and R. Emonet, “On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity,” Dec. 01, 2025, arXiv:2506.03719. doi: 10.48550/arXiv.2506.03719.\n[6] T. Bonnaire, R. Urfin, G. Biroli, and M. Mézard, “Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training,” Oct. 28, 2025, arXiv: arXiv:2505.17638. doi: 10.48550/arXiv.2505.17638.\n","wordCount":"3617","inLanguage":"en","image":"https://livey.github.io/posts/2025-11-22-diffusion/%3Cimage%20path/url%3E","datePublished":"2025-11-22T00:00:00Z","dateModified":"2025-11-22T00:00:00Z","author":{"@type":"Person","name":"Fuwei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://livey.github.io/posts/2025-11-22-diffusion/"},"publisher":{"@type":"Organization","name":"Fuwei's Tech Notes","logo":{"@type":"ImageObject","url":"https://livey.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://livey.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://livey.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://livey.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://livey.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://livey.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://livey.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://livey.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://livey.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">A Gentle Introduction to Diffusion Models and Flow Matching</h1><div class=post-description>A beginner-friendly overview of diffusion models and flow matching, exploring the mathematical foundations and practical training procedures behind modern generative models.</div><div class=post-meta><span title='2025-11-22 00:00:00 +0000 UTC'>November 22, 2025</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;3617 words&nbsp;·&nbsp;Fuwei Li&nbsp;|&nbsp;<a href=https://github.com/livey/livey.github.io/issues/new rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#generative-models aria-label="Generative Models">Generative Models</a></li><li><a href=#diffusion-models aria-label="Diffusion Models">Diffusion Models</a><ul><li><a href=#flow-matching aria-label="Flow Matching">Flow Matching</a></li><li><a href=#diffusion-models-1 aria-label="Diffusion Models">Diffusion Models</a></li></ul></li><li><a href=#training-the-generative-model aria-label="Training the Generative Model">Training the Generative Model</a></li><li><a href=#building-an-image-generator aria-label="Building an Image Generator">Building an Image Generator</a></li><li><a href=#recent-advances-in-diffusion-models aria-label="Recent Advances in Diffusion Models">Recent Advances in Diffusion Models</a><ul><li><a href=#back-to-basics-let-denoising-generative-models-denoise aria-label="Back to Basics: Let Denoising Generative Models Denoise">Back to Basics: Let Denoising Generative Models Denoise</a></li><li><a href=#theory aria-label=Theory>Theory</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>Diffusion models have emerged as a powerful and flexible class of generative models, underpinning recent breakthroughs in image, audio, and scientific data generation. This article offers a gentle, beginner-friendly overview of diffusion models and the related flow matching framework, demystifying their mathematical foundations and practical training procedures. We will delve into the core concepts, guiding equations, and step-by-step training algorithms, aiming to provide readers with both an intuition for how these models work and a roadmap for implementing them in practice. This article serves as notes on papers [1] and [2].</p><h1 id=generative-models>Generative Models<a hidden class=anchor aria-hidden=true href=#generative-models>#</a></h1><p>The original motivation behind diffusion models is to generate high-quality, diverse data. To achieve this, we map the observed data distribution to a simple base distribution, sample from the base, and map the samples back to data space. Thus, the goal of a generative model is to learn a mapping between the data and base distributions (in either direction).</p><h1 id=diffusion-models>Diffusion Models<a hidden class=anchor aria-hidden=true href=#diffusion-models>#</a></h1><h2 id=flow-matching>Flow Matching<a hidden class=anchor aria-hidden=true href=#flow-matching>#</a></h2><p>We mainly follow tutorial [1] to introduce flow matching and diffusion models.</p><p>Flow is a solution to the ODE</p>$$
\psi : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d, \quad (x_0, t) \mapsto \psi_t(x_0) \tag{2a}
$$$$
\frac{d}{dt} \psi_t(x_0) = u_t\bigl(\psi_t(x_0)\bigr) \tag{2b}
$$$$
\psi_0(x_0) = x_0 \tag{2c}
$$<p>For a given initial condition $X_0 = x_0$, a trajectory of the ODE is recovered via $X_t = \psi_t(X_0)$. Therefore, vector fields, ODEs, and flows are, intuitively, three descriptions of the same object: vector fields define ODEs whose solutions are flows.</p><p>Solutions are flows. As with every equation, we should ask: does a solution exist, and if so, is it unique? Under mild assumptions on $u_t$, the answer is “yes” to both:</p><hr><p><strong>Theorem 3 (Flow existence and uniqueness)</strong><br>If $u : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ is continuously differentiable with a bounded derivative, then the ODE in (2) has a unique solution given by a flow $\psi_t$. In this case, $\psi_t$ is a <strong>diffeomorphism</strong> for all $t$, i.e. $\psi_t$ is continuously differentiable with a continuously differentiable inverse $\psi_t^{-1}$.</p><hr><p><strong>Simulating an ODE.</strong> In general, it is not possible to compute the flow $\psi_t$ explicitly if $u_t$ is not as simple as a linear function. In these cases, one uses <strong>numerical methods</strong> to simulate ODEs. Fortunately, this is a classical and well researched topic in numerical analysis, and a myriad of powerful methods exist [11]. One of the simplest and most intuitive methods is the <strong>Euler method</strong>. In the Euler method, we initialize with $X_0 = x_0$ and update via</p>$$
X_{t+h} = X_t + h\,u_t(X_t)
\qquad
(t = 0, h, 2h, 3h, \ldots, 1 - h)
\tag{4}
$$<p>where $h = n^{-1} > 0$ is a step size hyperparameter with $n \in \mathbb{N}$. For this class, the Euler method will be good enough. To give you a taste of a more complex method, let us consider <strong>Heun&rsquo;s method</strong> defined via the update rule</p>$$
X'_{t+h} = X_t + h\,u_t(X_t)
\quad\text{(initial guess of new state)}
$$$$
X_{t+h} = X_t + \frac{h}{2}\bigl(u_t(X_t) + u_{t+h}(X'_{t+h})\bigr)
\quad\text{(update with average u at current and guessed state)}
$$<p>Intuitively, Heun&rsquo;s method is as follows: it takes a first guess $X'_{t+h}$ of what the next step could be but corrects the direction initially taken via an updated guess.</p><p><strong>Flow models.</strong> We can now construct a generative model via an ODE. Remember that our goal was to convert a simple distribution $p_{\text{init}}$ into a complex distribution $p_{\text{data}}$. The simulation of an ODE is thus a natural choice for this transformation. A <strong>flow model</strong> is described by the ODE</p>$$
X_0 \sim p_{\text{init}} \qquad \triangleright\ \text{random initialization}
$$$$
\frac{d}{dt} X_t = u_t^\theta(X_t) \qquad \triangleright\ \text{ODE}
$$<p>where the vector field $u_t^\theta$ is a neural network $u_t^\theta$ with parameters $\theta$. For now, we will speak of $u_t^\theta$ as being a generic neural network; i.e. a continuous function $u_t^\theta : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ with parameters $\theta$. Later, we will discuss particular choices of neural network architectures. Our goal is to make the endpoint $X_1$ of the trajectory have distribution $p_{\text{data}}$, i.e.</p>$$
X_1 \sim p_{\text{data}} \quad \Leftrightarrow \quad \psi_1^\theta(X_0) \sim p_{\text{data}}
$$<p>where $\psi_t^\theta$ describes the flow induced by $u_t^\theta$. Note however: although it is called <em>flow model</em>, <strong>the neural network parameterizes the vector field, not the flow</strong>. In order to compute the flow, we need to simulate the ODE. In algorithm 1, we summarize the procedure how to sample from a flow model.</p><h2 id=diffusion-models-1>Diffusion Models<a hidden class=anchor aria-hidden=true href=#diffusion-models-1>#</a></h2><p>In eq. (1a). Hence, we need to find an equivalent formulation of ODEs that does not use derivatives.
where $R_t(h)$ describes a negligible function for small $h$, i.e. such that $\lim_{h \to 0} R_t(h) = 0$, and in (i) we simply use the definition of derivatives. The derivation above simply restates what we already know: A trajectory $(X_t)_{0 \le t \le 1}$ of an ODE takes, at every timestep, a small step in the direction $u_t(X_t)$. We may now amend the last equation to make it stochastic: A trajectory $(X_t)_{0 \le t \le 1}$ of an SDE takes, at every timestep, a small step in the direction $u_t(X_t)$ plus some contribution from a Brownian motion:</p>$$
X_{t+h}= X_t + \underbrace{h\,u_t(X_t)}_{\text{deterministic}} + \underbrace{\sigma_t\bigl(W_{t+h} - W_t\bigr)}_{\text{stochastic}}+ \underbrace{h\,R_t(h)}_{\text{error term}}\tag{6}
$$<p>where $\sigma_t \ge 0$ describes the diffusion coefficient and $R_t(h)$ describes a stochastic error term such that the standard deviation $\mathbb{E}\bigl[\lVert R_t(h)\rVert^2\bigr]^{1/2} \to 0$ goes to zero for $h \to 0$. The above describes a stochastic differential equation (SDE).</p><p>It is common to denote a <strong>stochastic differential equation (SDE)</strong> in the following symbolic notation:</p>$$
\mathrm{d}X_t = u_t(X_t)\,\mathrm{d}t + \sigma_t\,\mathrm{d}W_t
\tag{7a}
$$$$
X_0 = x_0
\tag{7b}
$$<hr><p><strong>Theorem 5 (SDE Solution Existence and Uniqueness)</strong><br>If $u : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ is continuously differentiable with a bounded derivative and $\sigma_t$ is continuous, then the SDE in (7) has a solution given by the <strong>unique stochastic process</strong> $(X_t)_{0 \le t \le 1}$.</p><hr><p><strong>Simulating an SDE.</strong> If you struggle with the abstract definition of an SDE so far, then don’t worry about it. A more intuitive way of thinking about SDEs is given by answering the question: How might we simulate an SDE? The simplest such scheme is known as the <strong>Euler–Maruyama method</strong>, and is essentially to SDEs what the Euler method is to ODEs. Using the Euler–Maruyama method, we initialize $X_0 = x_0$ and update iteratively via</p>$$
X_{t+h} = X_t + h u_t(X_t) + \sqrt{h}\,\sigma_t \epsilon_t,
\qquad
\epsilon_t \sim \mathcal{N}(0, I_d)
\tag{9}
$$<p>where $h = n^{-1} > 0$ is a step size hyperparameter for $n \in \mathbb{N}$. In other words, to simulate using the Euler–Maruyama method, we take a small step in the direction of $u_t(X_t)$ as well as add a little bit of Gaussian noise scaled by $\sqrt{h}\,\sigma_t$. When simulating SDEs in this class (such as in the accompanying labs), we will usually stick to the Euler–Maruyama method.</p><hr><p><strong>Summary 7 (SDE generative model)</strong><br>Throughout this document, a <strong>diffusion model</strong> consists of a neural network $u_t^\theta$ with parameters $\theta$ that parameterize a vector field and a fixed diffusion coefficient $\sigma_t$:</p><ul><li><strong>Neural network:</strong> $u^\theta : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d,\ (x,t) \mapsto u_t^\theta(x)$ with parameters $\theta$</li><li><strong>Fixed:</strong> $\sigma_t : [0,1] \to [0,\infty),\ t \mapsto \sigma_t$</li></ul><p>To obtain samples from our SDE model (i.e. generate objects), the procedure is as follows:</p><ul><li><strong>Initialization:</strong> $X_0 \sim p_{\text{init}}$ ▸ Initialize with simple distribution, e.g. a Gaussian</li><li><strong>Simulation:</strong> $\mathrm{d}X_t = u_t^\theta(X_t)\,\mathrm{d}t + \sigma_t\,\mathrm{d}W_t$ ▸ Simulate SDE from $0$ to $1$</li><li><strong>Goal:</strong> $X_1 \sim p_{\text{data}}$ ▸ Goal is to make $X_1$ have distribution $p_{\text{data}}$</li></ul><p>A diffusion model with $\sigma_t = 0$ is a <strong>flow model</strong>.</p><hr>$$p_t(x,z) = p_t(x|z)p_{data}(z)$$<p>as beblow</p><ol><li>sample $z\sim p_{data}(z)$,</li><li>sample $x\sim p_t(x|z)$</li></ol><p>Then, we descard $z$. The resulting $x$ is distributed as the marginal $p_t(x)$.</p><hr><p>Idealy, we would like to learn a marginal vector field $u_t$ that maps the noise distribution $p_{\text{init}}$ to the data distribution $p_{\text{data}}$. However, it involves learning the marginal distribution $p_t$, which is intractable. Therefore, we need to use the marginalization trick. For marginal vector field, it does not depend on the distribution of the whole data. So, we can construct some conditional vector fields $u_t^{\text{target}}(x \mid z)$ for each data point $z$. By this sepcial construction, we can get closed-form expression of the conditional vector fields. The following theorem shows the relationship between the marginal vector field and the conditional vector field. Indeed we can choose any conditional probability. In practice, we would like to choose the one that is easy to sample and yield trackable targets (vector field).</p><hr><p><strong>Theorem 10 (Marginalization trick)</strong><br>For every data point $z \in \mathbb{R}^d$, let $u_t^{\text{target}}(\cdot \mid z)$ denote a <strong>conditional vector field</strong>, defined so that the corresponding ODE yields the conditional probability path $p_t(\cdot \mid z)$, viz.,</p>$$
X_0 \sim p_{\text{init}}, \\\\
\qquad
\frac{d}{dt} X_t = u_t^{\text{target}}(X_t \mid z)
\;\;\Rightarrow\;\;
X_t \sim p_t(\cdot \mid z)
\qquad
(0 \le t \le 1). \tag{18}
$$<p>Then the <strong>marginal vector field</strong> $u_t^{\text{target}}(x)$, <strong>defined</strong> by</p>$$
u_t^{\text{target}}(x)
=\int
u_t^{\text{target}}(x \mid z)
\frac{p_t(x \mid z)\, p_{\text{data}}(z)}{p_t(x)} \, dz,
\tag{19}
$$<p>follows the marginal probability path, i.e.</p>$$
X_0 \sim p_{\text{init}}, \qquad \frac{d}{dt} X_t = u_t^{\text{target}}(X_t) \;\;\Rightarrow\;\; X_t \sim p_t \qquad (0 \le t \le 1).
\tag{20}
$$<p>In particular, $X_1 \sim p_{\text{data}}$ for this ODE, so that we might say “$u_t^{\text{target}}$ converts noise $p_{\text{init}}$ into data $p_{\text{data}}$”.</p><p>Why we say $X_1 \sim p_{\text{data}}$? It is due to the fact that we implicitly in the conditional probability states that $p_1(\cdot|z)=\delta_z$ and we have $p_1(x) = \int p_1(x|z)p_{\text{data}}(z)\mathrm{d}z=\int \delta_z\cdot p_{\text{data}}(z)\mathrm{d}z=p_{\text{data}}(z)$.</p><p>Here I list some notations for clarification.</p><table><thead><tr><th>Concept</th><th>Notation</th><th>Meaning</th></tr></thead><tbody><tr><td>Single deterministic solution</td><td>$x_t$</td><td>state at time (t) for a fixed $x_0$</td></tr><tr><td>Explicit dependence on start</td><td>$x(t;x_0)$</td><td>same, but shows dependence on $x_0$</td></tr><tr><td>Flow map</td><td>$\phi_t(x_0)$</td><td>the function mapping $x_0 \mapsto x_t$</td></tr><tr><td>Random state</td><td>$X_t$</td><td>$x_t$ when $x_0$ is random</td></tr><tr><td>Sampling statement</td><td>$X_t \sim p_t$</td><td>distribution (law) of $X_t$ is $p_t$</td></tr></tbody></table><p>A minimal mental model</p><ol><li>Pick initial point $x_0$.</li><li>The ODE defines a path $t\to x_t$. (trajectory)</li><li>Collect all these paths into $x_t=\phi_t(x_0)$. (flow map)</li><li>if $x_0$ is random, call it $X_0$ and the path is $X_t$. (random process)</li></ol><hr><p>The following theorem shows the relationship between the vector field and the distribution. It tells that the change of the distribution over time is equal to the probability move according to the vector field.</p><hr>$$
\partial_t p_t(x) = -\operatorname{div}\big(p_t u_t^{\text{target}}\big)(x)
\quad \text{for all } x \in \mathbb{R}^d,\, 0 \le t \le 1,
\tag{24}
$$<p>where $\partial_t p_t(x) = \frac{d}{dt} p_t(x)$ denotes the time-derivative of $p_t(x)$ (This is true because we fix $x$ here. It is different from the following Langarangian equation where it focuses on the trajectory $x_t$). Equation 24 is known as the <strong>continuity equation</strong>. (Note that this is partial differential equation (PDE) instead of ordinary partial equaiton )</p><hr><p>where the <strong>divergence</strong> operator $\operatorname{div}$, is defined as:</p>$$
\operatorname{div}(v_t)(x) = \sum_{i=1}^d \frac{\partial}{\partial x_i} v_t^{\color{red}{(i)}}(x)
\tag{23}
$$<p>In the book, [1], there is a typo that they mistakenly omitted the superscript $(i)$ in the divergence operator.</p><p>The continuity equation states how the probability distribution changes for fix spatial point using the PDE. We can also evaluate the PDE along the curve, $x=X_t$, to get its Langrangain form.</p>$$
\begin{aligned}
\partial_t p_t(x)
&=\sum_i\frac{\partial}{\partial x_i}(p \cdot u_i)\\
&= -\nabla\, p_t(x)^\top u_t(x) - \text{div}\,u_t\cdot p_t(x)
\end{aligned}
$$<p>$\Rightarrow$</p>$$
\frac{\partial_t p_t(x)}{p_t(x)} = -\text{div}\,u_t(x) - \frac{\nabla p_t(x)^\top}{p_t(x)}u_t(x)
$$<p>$\Rightarrow$</p>$$
\partial_t \ln p_t(x)
= -\text{div}\,u_t(x) - \nabla_{x}\ln p_t(x)^\top u_t(x)
$$<p>Further, we have</p>$$
\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d}t}\ln p_t(x) &= \frac{\partial_t p_t(x)}{p_t(x)} + \frac{\partial_x p_t(x)\cdot \partial_t x}{p_t(x)}\\
&=\partial_t \ln p_t(x) + \partial_t x^\top \nabla_x \ln p_t(x)\\
&=-\text{div}\,u(x)
\end{aligned}
$$<p>In summary, we get another nice continuity equation</p>$$
\frac{\mathrm{d}}{\mathrm{d}t}\ln p_t(x) =-\text{div}\,u(x)
$$<p>Equivalently</p>$$
\frac{\mathrm{d}}{\mathrm{d}t} p_t(x) =-p_t(x)\text{div}\,u(x)
$$<p>We can even compute the probability, $p_t(x)$. By integration both sides of the above equation, we get</p>$$
\ln p_t(x) - \ln p_0(x)=-\int_0^t \text{div}\,u(x)\,\mathrm{d}t
$$<p>Take exponential on both sides we have</p>$$
p_t(x)=p_0(x_0)\exp\left(-\int_0^t \text{div}\,u_s(x_s)\,\mathrm{d}s\right)
$$<p>We can use the continuity equation to derive the relationship between the marginal vector field and the conditional vector field.</p><p>We may also see the following form of the theorem</p><hr><p><strong>Theorem 13</strong> (SDE extension trick)</p><p>Define the conditional and marginal vector fields $u_t^{\text{target}}(x \mid z)$ and $u_t^{\text{target}}(x)$ as before. Then, for diffusion coefficient $\sigma_t \ge 0$, we may construct an SDE which follows the same probability path:</p>$$
X_0 \sim p_{\text{init}},
\qquad
\mathrm{d}X_t
=\Big[
u_t^{\text{target}}(X_t)
+
\frac{\sigma_t^2}{2} \nabla \log p_t(X_t)
\Big] \mathrm{d}t
+
\sigma_t \mathrm{d}W_t
\tag{25}
$$$$
\Rightarrow\quad
X_t \sim p_t
\qquad
(0 \le t \le 1)
\tag{26}
$$<p>In particular, $X_1 \sim p_{\text{data}}$ for this SDE. The same identity holds if we replace the marginal probability $p_t(x)$ and vector field $u_t^{\text{target}}(x)$ with the conditional probability path $p_t(x \mid z)$ and vector field $u_t^{\text{target}}(x \mid z)$. Here, we require $\sigma_t \ge 0$. So, we can inject as much as noise but get the same expected distribution.</p><hr><p><strong>Theorem 15</strong> (Fokker–Planck Equation)</p><p>Let $p_t$ be a probability path and let us consider the SDE</p>$$
X_0 \sim p_{\text{init}}, \qquad \mathrm{d}X_t = u_t(X_t)\,\mathrm{d}t + \sigma_t\,\mathrm{d}W_t.
$$<p>Then $X_t$ has distribution $p_t$ for all $0 \le t \le 1$ if and only if the <strong>Fokker–Planck equation</strong> holds:</p>$$
\partial_t p_t(x) = -\operatorname{div}\big(p_t u_t\big)(x) + \frac{\sigma_t^2}{2}\,\Delta p_t(x)
\quad \text{for all } x \in \mathbb{R}^d,\; 0 \le t \le 1.
\tag{30}
$$<hr><p><strong>Summary 17</strong> (Derivation of the Training Target)</p><p><strong>The flow training target is the marginal vector field</strong> $u_t^{\text{target}}$. To construct it, we choose a <strong>conditional probability path</strong> $p_t(x\mid z)$ that fulfils $p_0(\cdot \mid z) = p_{\text{init}},\ p_1(\cdot \mid z) = \delta_z$. Next, we find a <strong>conditional vector field</strong> $u_t^{\text{flow}}(x\mid z)$ such that its corresponding flow $\psi_t^{\text{target}}(x\mid z)$ fulfills</p>$$
X_0 \sim p_{\text{init}}
\;\Rightarrow\;
X_t = \psi_t^{\text{target}}(X_0\mid z) \sim p_t(\cdot \mid z),
$$<p>or, equivalently, that $u_t^{\text{target}}$ satisfies the continuity equation. Then the <strong>marginal vector field</strong> defined by</p>$$
u_t^{\text{target}}(x)
=\int
u_t^{\text{target}}(x\mid z)
\frac{p_t(x\mid z)p_{\text{data}}(z)}{p_t(x)}\,\mathrm{d}z,
\tag{32}
$$<p>follows the marginal probability path, i.e.,</p>$$
X_0 \sim p_{\text{init}}, \quad
\mathrm{d}X_t = u_t^{\text{target}}(X_t)\,\mathrm{d}t
\;\Rightarrow\;
X_t \sim p_t
\quad
(0 \le t \le 1).
\tag{33}
$$<p>In particular, $X_1 \sim p_{\text{data}}$ for this ODE, so that $u_t^{\text{target}}$ &ldquo;converts noise into data&rdquo;, as desired.</p><p><strong>Extending to SDEs.</strong> For a time-dependent diffusion coefficient $\sigma_t \ge 0$, we can extend the above ODE to an SDE with the same marginal probability path:</p>$$
X_0 \sim p_{\text{init}}, \quad
\mathrm{d}X_t =
\Big[
u_t^{\text{target}}(X_t)
+
\frac{\sigma_t^2}{2}\nabla \log p_t(X_t)
\Big] \mathrm{d}t
+
\sigma_t \mathrm{d}W_t
\tag{34}
$$$$
\Rightarrow\;
X_t \sim p_t
\quad
(0 \le t \le 1),
\tag{35}
$$<p>where $\nabla \log p_t(x)$ is the <strong>marginal score function</strong></p>$$
\nabla \log p_t(x)
=\int
\nabla \log p_t(x\mid z)
\frac{p_t(x\mid z)p_{\text{data}}(z)}{p_t(x)}\,\mathrm{d}z.
\tag{36}
$$<p>Conditional and Marginal Score Functions</p><p>In particular, for the trajectories $X_t$ of the above SDE, it holds that $X_1 \sim p_{\text{data}}$, so that the SDE &ldquo;converts noise into data&rdquo;, as desired. An important example is the <strong>Gaussian probability path</strong>, yielding the formulae:</p>$$
p_t(x\mid z)
=\mathcal{N}(x; \alpha_t z, \beta_t^2 I_d)
\tag{37}
$$$$
u_t^{\text{flow}}(x\mid z)
=\left(\dot{\alpha}_t-
\frac{\dot{\beta}_t}{\beta_t}\alpha_t
\right) z + \frac{\dot{\beta}_t}{\beta_t} x
\tag{38}
$$$$
\nabla \log p_t(x\mid z)
=-
\frac{x - \alpha_t z}{\beta_t^2},
\tag{39}
$$<p>for <strong>noise schedulers</strong> $\alpha_t, \beta_t \in \mathbb{R}$: continuously differentiable, monotonic functions such that $\alpha_0 = \beta_1 = 0$, $\alpha_1 = \beta_0 = 1$.</p><p>In general, If you define a conditional flow map (or know the sampling method) as $x=\phi_t(z, \epsilon)$. Let us define the random veriable</p>$$
X_t := \phi_t(z, \epsilon)
$$<p>Then we can get $u_t(x|z)$ by taking partial derivative of $X_t$</p>$$
u_t(x|z) = \partial_t X_t = \partial_t \phi_t(z, \varepsilon)
$$<p>$u_t$ is function of $x$. So, we need represent $\varepsilon$ as function of $x$. Thus we get $\varepsilon = \phi_t^{-1}(x, z)$. So, we get the conditional velocity field</p>$$
u_t(x|z) = \partial_t\left(x, \phi_t^{-1}(x, z)\right)
$$<h1 id=training-the-generative-model>Training the Generative Model<a hidden class=anchor aria-hidden=true href=#training-the-generative-model>#</a></h1><p>We are trying to learn the marginal vector field $u_t^{\text{target}}(x)$. So, our objective is to parameter a neuron network, $v^{\theta}_t(x)$ to approximate $u_t^{\text{target}}(x)$ at every $t$ and $x$. Thus, our cost function is</p>$$
\mathcal{L}(\theta) = \mathbb{E}_{t, x} \left[ \| v^{\theta}_t(x) - u_t^{\text{target}}(x) \|^2 \right].
$$<p>Here we do not constrain the distribution of $t$. But, a convenient choice is uniform distribution, $t\sim \mathcal{U}(0,1)$. Other choices are possible, e.g., Beta distribution. Since $x = p_t(x) = \int p(x|z)p_{\text{data}}(z)\mathrm{d}z$, we have</p>$$
L_{\text{FM}}(\theta) = \mathbb{E}_{t\sim \mathcal{U}(0,1), x\sim p_t(x|z), z\sim p_{\text{data}}(z)} \left[ \| v^{\theta}_t(x) - u_t^{\text{target}}(x) \|^2 \right].
$$<p><strong>Theorem 18</strong>
The marginal flow matching loss equals the conditional flow matching loss up to a constant. That is</p>$$
\mathcal{L}_{\text{FM}}(\theta) = \mathcal{L}_{\text{CFM}}(\theta) + C,
$$$$
\nabla_\theta \mathcal{L}_{\text{FM}}(\theta) = \nabla_\theta \mathcal{L}_{\text{CFM}}(\theta).
$$<p>Hence, minimizing $\mathcal{L}_{\text{CFM}}(\theta)$ with e.g., stochastic gradient descent (SGD) is equivalent to minimizing $\mathcal{L}_{\text{FM}}(\theta)$ in the same fashion. In particular, for the minimizer $\theta^*$ of $\mathcal{L}_{\text{CFM}}(\theta)$, it will hold that $u_t^{\theta^*} = u_t^{\text{target}}$ (assuming an infinitely expressive parameterization).</p><hr><p><strong>Algorithm 3</strong> Flow Matching Training Procedure (here for Gaussian CondOT path $p_t(x\mid z) = \mathcal{N}(tz, (1 - t)^2)$)</p><p><strong>Require:</strong> A dataset of samples $z \sim p_{\text{data}}$, neural network $u_t^\theta$</p>$$\mathcal{L}(\theta) = \lVert u_t^\theta(x) - (z - \epsilon) \rVert^2$$<p><br>      (General case: $= \lVert u_t^\theta(x) - u_t^{\text{target}}(x\mid z) \rVert^2$)</p><p>7:    Update the model parameters $\theta$ via gradient descent on $\mathcal{L}(\theta)$.<br>8: <strong>end for</strong></p><hr><p><strong>Algorithm 4</strong> Score Matching Training Procedure for Gaussian probability path</p><p><strong>Require:</strong> A dataset of samples $z \sim p_{\text{data}}$, score network $s_t^\theta$ or noise predictor $\epsilon_t^\theta$</p><p>1: for each mini-batch of data <strong>do</strong><br>2:    Sample a data example $z$ from the dataset.<br>3:    Sample a random time $t \sim \text{Unif}_{[0,1]}$.<br>4:    Sample noise $\epsilon \sim \mathcal{N}(0, I_d)$ (General case: $x_t \sim p_t(\cdot \mid z)$)<br>5:    Set $x_t = \alpha_t z + \beta_t \epsilon$<br>6:    Compute loss</p>$$
\mathcal{L}(\theta) = \lVert s_t^\theta(x_t) + \tfrac{\epsilon}{\beta_t} \rVert^2
\quad
\text{(General case: } = \lVert s_t^\theta(x_t) - \nabla \log p_t(x_t \mid z) \rVert^2 \text{)}
$$<p>Alternatively:</p>$$
\mathcal{L}(\theta) = \lVert \epsilon_t^\theta(x_t) - \epsilon \rVert^2
$$<p>7:    Update the model parameters $\theta$ via gradient descent on $\mathcal{L}(\theta)$.<br>8: <strong>end for</strong></p><h1 id=building-an-image-generator>Building an Image Generator<a hidden class=anchor aria-hidden=true href=#building-an-image-generator>#</a></h1><p>The objective of our network is to learn an mixture of the guided and unguided vector fields.</p>$$
\tilde{u}_t(x|y)
= u_t^{\text{target}}(x) + w b_t \nabla \log p_t(y|x) \\
= u_t^{\text{target}}(x) + w b_t (\nabla \log p_t(x|y) - \nabla \log p_t(x)) \\
= u_t^{\text{target}}(x) - (w a_t x + w b_t \nabla \log p_t(x)) + (w a_t x + w b_t \nabla \log p_t(x|y)) \\
= (1 - w) u_t^{\text{target}}(x) + w u_t^{\text{target}}(x|y)
$$<hr><p><strong>Summary 27 (Classifier-Free Guidance for Flow Models)</strong></p>$$
\tilde{u}_t(x|y) = (1-w)u_t^{\text{target}}(x|\varnothing) + w u_t^{\text{target}}(x|y).
\tag{70}
$$$$
\mathcal{L}^{\text{CFG}}_{\text{CFM}}(\theta)
= \mathbb{E}_{\square}\big\lVert u_t^{\theta}(x|y) - u_t^{\text{target}}(x|z) \big\rVert^2
\tag{71}
$$$$
\square = (z,y) \sim p_{\text{data}}(z,y),\quad
t \sim \text{Unif}[0,1],\quad
x \sim p_t(\cdot|z),\ \text{replace } y = \varnothing \text{ with prob. } \eta.
\tag{72}
$$<p>In plain English, $\mathcal{L}^{\text{CFG}}_{\text{CFM}}$ might be approximated by</p><ul><li>$(z,y) \sim p_{\text{data}}(z,y)$ — Sample $(z,y)$ from data distribution.</li><li>$t \sim \text{Unif}[0,1]$ — Sample $t$ uniformly on $[0,1]$.</li><li>$x \sim p_t(x|z)$ — Sample $x$ from the conditional probability path $p_t(x|z)$.</li><li>with prob. $\eta$, $y \leftarrow \varnothing$ — Replace $y$ with $\varnothing$ with probability $\eta$.</li></ul>$$
\widehat{\mathcal{L}}^{\text{CFG}}_{\text{CFM}}(\theta)
= \big\lVert u_t^{\theta}(x|y) - u_t^{\text{target}}(x|z) \big\rVert^2.
$$$$
u_t^{\text{target}}(x|z) = u_t^{\text{target}}(x|z,y).
$$<p>At inference time, for a fixed choice of $y$, we may sample via</p><ul><li><strong>Initialization:</strong> $X_0 \sim p_{\text{init}}(x)$ — Initialize with simple distribution (such as a Gaussian).</li><li><strong>Simulation:</strong> $\mathrm{d}X_t = \tilde{u}_t^{\theta}(X_t|y)\,\mathrm{d}t$ — Simulate ODE from $t=0$ to $t=1$.</li><li><strong>Samples:</strong> $X_1$ — Goal is for $X_1$ to adhere to the guiding variable $y$.</li></ul><hr><h1 id=recent-advances-in-diffusion-models>Recent Advances in Diffusion Models<a hidden class=anchor aria-hidden=true href=#recent-advances-in-diffusion-models>#</a></h1><h2 id=back-to-basics-let-denoising-generative-models-denoise>Back to Basics: Let Denoising Generative Models Denoise<a hidden class=anchor aria-hidden=true href=#back-to-basics-let-denoising-generative-models-denoise>#</a></h2><p>This paper [2] is simple yet inspiring. The toy example in Fig. 2 clearly shows why it is preferable to predict the original data rather than the noise. Table 1 also summarizes how different objectives are related.</p><p><img alt="Summary of objectives and denoising targets from Back to Basics" loading=lazy src=/posts/2025-11-22-diffusion/table1.png></p>$$
\begin{cases}
x_\theta = \text{net}_\theta \\
z_t = t x_\theta + (1-t)\epsilon_\theta \\
v_\theta = x_\theta - \epsilon_\theta
\end{cases}
$$<h2 id=theory>Theory<a hidden class=anchor aria-hidden=true href=#theory>#</a></h2><p>By writing the data distribution as discrete distribution</p>$$
p(y) = \frac{1}{n} \sum_{i=1}^n \delta_{y^i}
$$<p>where $\delta_{y^i}$ is the Dirac delta distribution. Then, we can derive many interesting closed-form expressions [4, 5].</p><p>Firstly, we will compute the marginal distribution $X_t \sim p_t(x)$. Let&rsquo;s assume the conditional probability path is given by</p>$$
X_t|z \sim \mathcal{N}(\mu_t(z), \sigma_t^2(z)\mathbf{I}_d)
$$<p>Then, the marginal distribution is</p>$$
\begin{aligned}
p_t(x) &= \int_z p(z) p_t(x|z) \mathrm{d}z \\
&=\int_z \frac{1}{n} \sum_{i=1}^n \delta_{y^i} \mathcal{N}(x; \mu_t(z), \sigma_t^2(z)\mathbf{I}_d) \mathrm{d}z \\
&=\frac{1}{n} \sum_{i=1}^n \mathcal{N}(x; \mu_t(y^i), \sigma_t^2(y^i)\mathbf{I}_d)
\end{aligned}
$$<p>It is just a mixture of Gaussians!</p><p>Second, we will compute the marginal vector field, $u_t(x)$. Let us reparamterize the conditional variable $X_t$ as</p>$$
X_t = \alpha_t(z) + \beta_t(z) \varepsilon
$$<p>So, we have $\varepsilon = \frac{X_t - \alpha_t(z)}{\beta_t(z)}$ and</p>$$
\partial_t X_t = \dot{\alpha}_t(z) + \dot{\beta}_t(z) \varepsilon
$$<p>Substitute the expression of $\varepsilon$, we have</p>$$
u_t(x|z) = \frac{\dot{\beta}_t(x)}{\beta_t(z)}\left(x - \alpha_t(z)\right) + \dot{\alpha}_t(z)
$$<p>Then, the marginal vector field is</p>$$
\begin{aligned}
u_t(x) &= \int_z u_t(x|z)p_t(z|x) \mathrm{d}z \\
&= \int_z u_t(x|z) \frac{p_t(x|z)p(z)}{\int_z p_t(x|z)p(z)} \mathrm{d}z \\
&= \frac{1}{n} \sum_{i=1}^n u_t(x|y^i) \frac{\mathcal{N}(x; \alpha_t(y^i), \sigma_t^2(y^i)\mathbf{I}_d)}{\frac{1}{n} \mathcal{N}(x; \alpha_t(y^i), \sigma_t^2(y^i)\mathbf{I}_d)}\\
&=\frac{1}{n} \sum_{i=1}^n u_t(x|y^i)\frac{\mathcal{N}\left((x-\alpha_t(y^i))/\sigma_t(y^i);\mathbf{0}, \mathbf{I}_d\right)}{\frac{1}{n} \sum_i\mathcal{N}\left((x-\alpha_t(y^i))/\sigma_t(y^i); \mathbf{0}, \mathbf{I}_d\right)}\\
&=\frac{1}{n} \sum_{i=1}^n u_t(x|y^i)\frac{\exp\left(-\|x-\alpha_t(y^i)\|^2/(2\sigma_t^2(y^i))\right)}{\frac{1}{n} \sum_i\exp\left(-\|x-\alpha_t(y^i)\|^2)/(2\sigma_t^2(y^i)\right)}
\end{aligned}
$$<p>This is weighted sum of the conditional vector field, where the weights are defined by the exponential of the negative distance to the data samples. Let us look at the example that $\alpha_t(z)=tz$ and $\beta_t(z)=(1-t)$. So that the conditional vector field is</p>$$
u_t(x|z) = \frac{z-x}{1-t}
$$<p>Substitute in the marginal vector filed equation we have</p>$$
u_t(x) = \frac{1}{n}\sum_{i=1}^n\frac{y^i - x}{1-t}\cdot\frac{\exp\left(-\|x-t\cdot y^i)\|^2/(2(1-t)^2))\right)}{\frac{1}{n} \sum_i\exp\left(-\|x-t\cdot y^i\|^2)/(2(1-t)^2)\right)}
$$<h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>[1] P. Holderrieth and E. Erives, “An Introduction to Flow Matching and Diffusion Models,” July 12, 2025, arXiv: arXiv:2506.02070. doi: 10.48550/arXiv.2506.02070.</p><p>[2] T. Li and K. He, “Back to Basics: Let Denoising Generative Models Denoise,” Nov. 17, 2025, arXiv:2511.13720. doi: 10.48550/arXiv.2511.13720.</p><p>[3] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow Matching for Generative Modeling,” Feb. 08, 2023, arXiv: arXiv:2210.02747. doi: 10.48550/arXiv.2210.02747.</p><p>[4] W. Gao and M. Li, “How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?,” Oct. 31, 2024, arXiv: arXiv:2410.23594. doi: 10.48550/arXiv.2410.23594.</p><p>[5] Q. Bertrand, A. Gagneux, M. Massias, and R. Emonet, “On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity,” Dec. 01, 2025, arXiv:2506.03719. doi: 10.48550/arXiv.2506.03719.</p><p>[6] T. Bonnaire, R. Urfin, G. Biroli, and M. Mézard, “Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training,” Oct. 28, 2025, arXiv: arXiv:2505.17638. doi: 10.48550/arXiv.2505.17638.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://livey.github.io/tags/diffusion-model/>Diffusion Model</a></li><li><a href=https://livey.github.io/tags/generative-model/>Generative Model</a></li></ul><nav class=paginav><a class=prev href=https://livey.github.io/posts/2026-01-13-data-dist/><span class=title>« Prev</span><br><span>Rethinking Data Augmentation in End-to-End Learning</span>
</a><a class=next href=https://livey.github.io/posts/2025-08-21-lidar-adjust/><span class=title>Next »</span><br><span>LiDAR Extrinsic Parameter Adjustment for SLAM Recalibration</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on x" href="https://x.com/intent/tweet/?text=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f&amp;hashtags=DiffusionModel%2cGenerativeModel"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f&amp;title=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching&amp;summary=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching&amp;source=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f&title=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on whatsapp" href="https://api.whatsapp.com/send?text=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching%20-%20https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on telegram" href="https://telegram.me/share/url?text=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching&amp;url=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Gentle Introduction to Diffusion Models and Flow Matching on ycombinator" href="https://news.ycombinator.com/submitlink?t=A%20Gentle%20Introduction%20to%20Diffusion%20Models%20and%20Flow%20Matching&u=https%3a%2f%2flivey.github.io%2fposts%2f2025-11-22-diffusion%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://livey.github.io/>Fuwei's Tech Notes</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>